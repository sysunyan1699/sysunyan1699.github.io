
<!DOCTYPE html><html lang="zh-CN">

<head>
  <meta charset="utf-8">
  <meta name="hexo-theme" content="https://github.com/xaoxuu/hexo-theme-stellar/tree/1.28.1" theme-name="Stellar" theme-version="1.28.1">
  
  <meta name="generator" content="Hexo 7.2.0">
  <meta http-equiv='x-dns-prefetch-control' content='on' />
  
  <meta name="renderer" content="webkit">
  <meta name="force-rendering" content="webkit">
  <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
  <meta name="HandheldFriendly" content="True" >
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="theme-color" media="(prefers-color-scheme: light)" content="#f9fafb">
  <meta name="theme-color" media="(prefers-color-scheme: dark)" content="#000">
  
  <title>The Unreasonable Effectiveness of Recurrent Neural Networks - Sun Yan</title>

  
    <meta name="description" content="Andrej Karpathy blog:# The Unreasonable Effectiveness of Recurrent Neural Networks Thereâ€™s something magical about Recurrent Neural Networks (RNNs). I still remember when I trained my first recurrent">
<meta property="og:type" content="article">
<meta property="og:title" content="The Unreasonable Effectiveness of Recurrent Neural Networks">
<meta property="og:url" content="http://example.com/2472be8a/index.html">
<meta property="og:site_name" content="Sun Yan">
<meta property="og:description" content="Andrej Karpathy blog:# The Unreasonable Effectiveness of Recurrent Neural Networks Thereâ€™s something magical about Recurrent Neural Networks (RNNs). I still remember when I trained my first recurrent">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/2472be8a/1.png">
<meta property="og:image" content="https://karpathy.github.io/assets/rnn/house_read.gif">
<meta property="og:image" content="https://karpathy.github.io/assets/rnn/house_generate.gif">
<meta property="og:image" content="http://example.com/2472be8a/2.png">
<meta property="og:image" content="http://example.com/2472be8a/3.png">
<meta property="og:image" content="http://example.com/2472be8a/4.png">
<meta property="og:image" content="http://example.com/2472be8a/5.png">
<meta property="og:image" content="http://example.com/2472be8a/6.png">
<meta property="og:image" content="http://example.com/2472be8a/7.png">
<meta property="og:image" content="http://example.com/2472be8a/8.png">
<meta property="og:image" content="http://example.com/2472be8a/9.png">
<meta property="og:image" content="http://example.com/2472be8a/10.png">
<meta property="article:published_time" content="2024-06-11T09:59:43.000Z">
<meta property="article:modified_time" content="2024-06-16T14:23:22.626Z">
<meta property="article:author" content="Sun Yan">
<meta property="article:tag" content="AI">
<meta property="article:tag" content="ç¥ç»ç½‘ç»œ">
<meta property="article:tag" content="Ilya sutskeverâ€˜s 30  papers">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2472be8a/1.png">
  
  
  
  <meta name="keywords" content="AI,ç¥ç»ç½‘ç»œ,Ilya sutskeverâ€˜s 30  papers">

  <!-- feed -->
  
    <link rel="alternate" href="/atom.xml" title="Sun Yan" type="application/atom+xml">
  

  <link rel="stylesheet" href="/css/main.css?v=1.28.1">

  

  

  
</head>
<body>

<div class="l_body s:aa content tech" id="start" layout="post" ><aside class="l_left"><div class="leftbar-container">


<header class="header"><div class="logo-wrap"><a class="avatar" href="/"><div class="bg" style="opacity:0;background-image:url(https://gcore.jsdelivr.net/gh/cdn-x/placeholder@1.0.12/avatar/round/rainbow64@3x.webp);"></div><img no-lazy class="avatar" src="/images/avatar.jpg" onerror="javascript:this.classList.add('error');this.src='https://gcore.jsdelivr.net/gh/cdn-x/placeholder@1.0.12/image/2659360.svg';"></a><a class="title" href="/"><div class="main" ff="title">Sun Yan</div><div class="sub cap">Backend Developer</div></a></div></header>

<div class="nav-area">
<div class="search-wrapper" id="search-wrapper"><form class="search-form"><a class="search-button" onclick="document.getElementById(&quot;search-input&quot;).focus();"><svg t="1705074644177" viewBox="0 0 1025 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="1560" width="200" height="200"><path d="M1008.839137 935.96571L792.364903 719.491476a56.783488 56.783488 0 0 0-80.152866 0 358.53545 358.53545 0 1 1 100.857314-335.166073 362.840335 362.840335 0 0 1-3.689902 170.145468 51.248635 51.248635 0 1 0 99.217358 26.444296 462.057693 462.057693 0 1 0-158.255785 242.303546l185.930047 185.725053a51.248635 51.248635 0 0 0 72.568068 0 51.248635 51.248635 0 0 0 0-72.978056z" p-id="1561"></path><path d="M616.479587 615.969233a50.428657 50.428657 0 0 0-61.498362-5.534852 174.655348 174.655348 0 0 1-177.525271 3.484907 49.403684 49.403684 0 0 0-58.833433 6.76482l-3.074918 2.869923a49.403684 49.403684 0 0 0 8.609771 78.10292 277.767601 277.767601 0 0 0 286.992355-5.739847 49.403684 49.403684 0 0 0 8.404776-76.667958z" p-id="1562"></path></svg></a><input type="text" class="search-input" id="search-input" placeholder="ç«™å†…æœç´¢"></form><div id="search-result"></div><div class="search-no-result">æ²¡æœ‰æ‰¾åˆ°å†…å®¹ï¼</div></div>


<nav class="menu dis-select"></nav>
</div>
<div class="widgets">


<widget class="widget-wrapper post-list"><div class="widget-header dis-select"><span class="name">æœ€è¿‘æ›´æ–°</span></div><div class="widget-body fs14"><a class="item title" href="/d9903cb/"><span class="title">åˆ†æ‘Šç®—æ³•</span></a><a class="item title" href="/6cb5dc64/"><span class="title">InnoDBäº‹åŠ¡-æŒä¹…æ€§çš„å®ç°,binglog & redo log</span></a><a class="item title" href="/5b064db6/"><span class="title">Intro to äº‹åŠ¡</span></a><a class="item title" href="/d8e12886/"><span class="title">SafeInventory- å¦‚ä½•é«˜æ•ˆæ‰£å‡åº“å­˜</span></a><a class="item title" href="/ca1e509/"><span class="title">SafeInventory-æœ¬åœ°äº‹åŠ¡ä¸‹ï¼Œå¦‚ä½•å®‰å…¨æ“ä½œåº“å­˜</span></a><a class="item title" href="/b36b0ce9/"><span class="title">InnoDBäº‹åŠ¡-åŸå­æ€§çš„å®ç°,undo log</span></a><a class="item title" href="/4bd46d7d/"><span class="title">SpringBoot è‡ªåŠ¨é…ç½®å®ç°åŸç†</span></a><a class="item title" href="/a5120d74/"><span class="title">SafeInventory-åˆ†å¸ƒå¼äº‹åŠ¡ä¸‹ï¼Œå¦‚ä½•å®‰å…¨æ“ä½œåº“å­˜</span></a><a class="item title" href="/c3915fbe/"><span class="title">å¦‚ä½•å¤„ç†é‡å¤è¯·æ±‚ä¿è¯å¹‚ç­‰</span></a><a class="item title" href="/2a483461/"><span class="title">åœ¨Springäº‹åŠ¡ç®¡ç†ä¸‹ï¼Œä½¿ç”¨Synchronized   ä¸ºä»€ä¹ˆä¼šå‡ºç°å¹¶å‘é—®é¢˜</span></a></div></widget>
</div>
<footer class="footer dis-select"><div class="social-wrap"><a class="social" href="https://github.com/sysunyan1699" target="_blank" rel="external nofollow noopener noreferrer"><img no-lazy src="/images/github.png" / onerror="javascript:this.classList.add('error');this.src='https://gcore.jsdelivr.net/gh/cdn-x/placeholder@1.0.12/image/2659360.svg';"></a><a class="social" href="https://www.linkedin.com/in/yansun1699/" target="_blank" rel="external nofollow noopener noreferrer"><img no-lazy src="/images/LinkedIn.png" / onerror="javascript:this.classList.add('error');this.src='https://gcore.jsdelivr.net/gh/cdn-x/placeholder@1.0.12/image/2659360.svg';"></a><a class="social" href="/sysunyan1699@gmail.com" rel="noopener noreferrer"><img no-lazy src="/images/gmail.png" / onerror="javascript:this.classList.add('error');this.src='https://gcore.jsdelivr.net/gh/cdn-x/placeholder@1.0.12/image/2659360.svg';"></a></div></footer>
</div></aside><div class="l_main" id="main">





<div class="article banner top">
  <div class="content">
    <div class="top bread-nav footnote"><div class="left"><div class="flex-row" id="breadcrumb"><a class="cap breadcrumb" href="/">ä¸»é¡µ</a>
<span class="sep"></span><a class="cap breadcrumb" href="/">æ–‡ç« </a></div>
<div class="flex-row" id="post-meta"><span class="text created">å‘å¸ƒäºï¼š<time datetime="2024-06-11T09:59:43.000Z">2024-06-11</time></span><span class="sep updated"></span><span class="text updated">æ›´æ–°äºï¼š<time datetime="2024-06-16T14:23:22.626Z">2024-06-16</time></span></div></div></div>
    
    <div class="bottom only-title">
      
      <div class="text-area">
        <h1 class="text title"><span>The Unreasonable Effectiveness of Recurrent Neural Networks</span></h1>
        
      </div>
    </div>
    
  </div>
  </div><article class="md-text content"><p><a target="_blank" rel="noopener" href="https://karpathy.github.io/2015/05/21/rnn-effectiveness/">Andrej Karpathy blog:# The Unreasonable Effectiveness of Recurrent Neural Networks</a></p>
<p>Thereâ€™s something magical about Recurrent Neural Networks (RNNs). I still remember when I trained my first recurrent network for <a target="_blank" rel="noopener" href="http://cs.stanford.edu/people/karpathy/deepimagesent/">Image Captioning</a>. Within a few dozen minutes of training my first baby model (with rather arbitrarily-chosen hyperparameters) started to generate very nice looking descriptions of images that were on the edge of making sense. Sometimes the ratio of how simple your model is to the quality of the results you get out of it blows past your expectations, and this was one of those times. What made this result so shocking at the time was that the common wisdom was that RNNs were supposed to be difficult to train (with more experience Iâ€™ve in fact reached the opposite conclusion). Fast forward about a year: Iâ€™m training RNNs all the time and Iâ€™ve witnessed their power and robustness many times, and yet their magical outputs still find ways of amusing me. This post is about sharing some of that magic with you.<br>å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰æœ‰å…¶ç‹¬ç‰¹çš„é­…åŠ›ã€‚æˆ‘è¿˜è®°å¾—ç¬¬ä¸€æ¬¡è®­ç»ƒç”¨äºå›¾åƒæè¿°ç”Ÿæˆçš„å¾ªç¯ç¥ç»ç½‘ç»œæ—¶çš„æƒ…æ™¯ã€‚åªç”¨äº†çŸ­çŸ­å‡ ååˆ†é’Ÿï¼Œå³ä½¿æ˜¯éšæ„é€‰æ‹©çš„è¶…å‚æ•°ï¼Œè¿™ä¸ªåˆæ­¥æ¨¡å‹å·²ç»å¼€å§‹ç”Ÿæˆçœ‹èµ·æ¥éå¸¸ä¸é”™çš„å›¾åƒæè¿°ï¼Œå°½ç®¡è¿™äº›æè¿°æœ‰æ—¶åªæ˜¯å‹‰å¼ºåˆç†ã€‚æœ‰æ—¶ï¼Œæ¨¡å‹çš„ç®€å•ç¨‹åº¦ä¸å…¶è¾“å‡ºç»“æœçš„è´¨é‡ä¹‹é—´çš„æ¯”ä¾‹ä¼šè¿œè¿œè¶…å‡ºé¢„æœŸï¼Œè¿™æ¬¡å°±æ˜¯ä¸€ä¸ªå…¸å‹çš„ä¾‹å­ã€‚è¿™æ¬¡ç»“æœå¦‚æ­¤ä»¤äººéœ‡æƒŠçš„åŸå› åœ¨äºï¼Œå½“æ—¶çš„æ™®éè®¤çŸ¥æ˜¯ï¼ŒRNNå¾ˆéš¾è®­ç»ƒï¼ˆéšç€ç»éªŒçš„å¢åŠ ï¼Œæˆ‘å®é™…ä¸Šå¾—å‡ºäº†ç›¸åçš„ç»“è®ºï¼‰ã€‚æ—¶é—´å¿«è¿›å¤§çº¦ä¸€å¹´ï¼šæˆ‘ä¸€ç›´åœ¨è®­ç»ƒRNNï¼Œç›®ç¹äº†å®ƒä»¬çš„å¼ºå¤§å’Œç¨³å¥ï¼Œå°½ç®¡å¦‚æ­¤ï¼Œå®ƒä»¬ç¥å¥‡çš„è¾“å‡ºä¾ç„¶èƒ½ä¸æ–­å¸¦ç»™æˆ‘æƒŠå–œã€‚è¿™ç¯‡æ–‡ç« æ—¨åœ¨ä¸å¤§å®¶åˆ†äº«è¿™ç§é­”åŠ›ã€‚</p>
<p>Weâ€™ll train RNNs to generate text character by character and ponder the question â€œhow is that even possible?â€<br>æˆ‘ä»¬å°†è®­ç»ƒå¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰é€å­—ç¬¦åœ°ç”Ÿæˆæ–‡æœ¬ï¼Œå¹¶æ€è€ƒè¿™ä¸ªé—®é¢˜ï¼šâ€œè¿™åˆ°åº•æ˜¯æ€ä¹ˆåšåˆ°çš„ï¼Ÿâ€</p>
<p>By the way, together with this post I am also releasing <a target="_blank" rel="noopener" href="https://github.com/karpathy/char-rnn">code on Github</a> that allows you to train character-level language models based on multi-layer LSTMs. You give it a large chunk of text and it will learn to generate text like it one character at a time. You can also use it to reproduce my experiments below. But weâ€™re getting ahead of ourselves; What are RNNs anyway?<br>é¡ºä¾¿æä¸€ä¸‹ï¼Œä¸è¿™ç¯‡æ–‡ç« ä¸€èµ·ï¼Œæˆ‘è¿˜åœ¨Githubä¸Šå‘å¸ƒäº†ä»£ç ï¼Œè¿™äº›ä»£ç å¯ä»¥ç”¨æ¥è®­ç»ƒåŸºäºå¤šå±‚LSTMçš„å­—ç¬¦çº§è¯­è¨€æ¨¡å‹ã€‚ä½ åªéœ€æä¾›ä¸€å¤§æ®µæ–‡æœ¬ï¼Œå®ƒå°±ä¼šé€å­—ç¬¦åœ°å­¦ä¹ ç”Ÿæˆç±»ä¼¼çš„æ–‡æœ¬ã€‚ä½ è¿˜å¯ä»¥ä½¿ç”¨å®ƒæ¥é‡ç°æˆ‘ä¸‹é¢çš„å®éªŒã€‚ä½†åœ¨æ­¤ä¹‹å‰ï¼Œæˆ‘ä»¬è¿˜æ˜¯å…ˆå›åˆ°æ­£é¢˜ä¸Šæ¥ï¼šæˆ‘ä»¬å…ˆæ¥äº†è§£ä¸€ä¸‹RNNåˆ°åº•æ˜¯ä»€ä¹ˆï¼Ÿ</p>
<h1 id="recurrent-neural-networks-é€’å½’ç¥ç»ç½‘ç»œ"><a href="#Recurrent-Neural-Networks-é€’å½’ç¥ç»ç½‘ç»œ" class="headerlink" title="Recurrent Neural Networks  é€’å½’ç¥ç»ç½‘ç»œ"></a>Recurrent Neural Networks  é€’å½’ç¥ç»ç½‘ç»œ</h1><h2 id="sequences"><a href="#Sequences" class="headerlink" title="Sequences"></a>Sequences</h2><p><strong>Sequences</strong>. Depending on your background you might be wondering: _What makes Recurrent Networks so special_? A glaring limitation of Vanilla Neural Networks (and also Convolutional Networks) is that their API is too constrained: they accept a fixed-sized vector as input (e.g. an image) and produce a fixed-sized vector as output (e.g. probabilities of different classes). Not only that: These models perform this mapping using a fixed amount of computational steps (e.g. the number of layers in the model). The core reason that recurrent nets are more exciting is that they allow us to operate over _sequences_ of vectors: Sequences in the input, the output, or in the most general case both. A few examples may make this more concrete:<br>åºåˆ—ã€‚æ ¹æ®ä½ çš„èƒŒæ™¯ï¼Œä½ å¯èƒ½ä¼šé—®ï¼šå¾ªç¯ç¥ç»ç½‘ç»œæœ‰ä»€ä¹ˆç‰¹åˆ«ä¹‹å¤„ï¼Ÿä¸€ä¸ªæ˜¾è€Œæ˜“è§çš„é™åˆ¶æ˜¯Vanilla ç¥ç»ç½‘ç»œï¼ˆä»¥åŠå·ç§¯ç¥ç»ç½‘ç»œï¼‰çš„APIè¿‡äºå—é™ï¼šå®ƒä»¬æ¥å—å›ºå®šå¤§å°çš„å‘é‡ä½œä¸ºè¾“å…¥ï¼ˆä¾‹å¦‚ï¼Œä¸€å¼ å›¾ç‰‡ï¼‰ï¼Œå¹¶äº§ç”Ÿå›ºå®šå¤§å°çš„å‘é‡ä½œä¸ºè¾“å‡ºï¼ˆä¾‹å¦‚ï¼Œä¸åŒç±»åˆ«çš„æ¦‚ç‡ï¼‰ã€‚ä¸ä»…å¦‚æ­¤ï¼Œè¿™äº›æ¨¡å‹ä½¿ç”¨å›ºå®šæ•°é‡çš„è®¡ç®—æ­¥éª¤æ¥å®Œæˆè¿™ä¸ªæ˜ å°„ï¼ˆä¾‹å¦‚ï¼Œæ¨¡å‹ä¸­çš„å±‚æ•°ï¼‰ã€‚å¾ªç¯ç¥ç»ç½‘ç»œæ›´ä»¤äººå…´å¥‹çš„æ ¸å¿ƒåŸå› åœ¨äºå®ƒä»¬å…è®¸æˆ‘ä»¬å¯¹å‘é‡åºåˆ—è¿›è¡Œæ“ä½œï¼šè¾“å…¥ä¸­çš„åºåˆ—ï¼Œè¾“å‡ºä¸­çš„åºåˆ—ï¼Œæˆ–è€…åœ¨æœ€ä¸€èˆ¬çš„æƒ…å†µä¸‹ï¼Œä¸¤è€…éƒ½æ˜¯åºåˆ—ã€‚å‡ ä¸ªä¾‹å­å¯ä»¥è®©è¿™ä¸€ç‚¹æ›´åŠ å…·ä½“ï¼š</p>
<img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="/2472be8a/1.png" class>
<p>Each rectangle is a vector and arrows represent functions (e.g. matrix multiply). Input vectors are in red, output vectors are in blue and green vectors hold the RNNâ€™s state (more on this soon). From left to right: <strong>(1)</strong> Vanilla mode of processing without RNN, from fixed-sized input to fixed-sized output (e.g. image classification). <strong>(2)</strong> Sequence output (e.g. image captioning takes an image and outputs a sentence of words). <strong>(3)</strong> Sequence input (e.g. sentiment analysis where a given sentence is classified as expressing positive or negative sentiment). <strong>(4)</strong> Sequence input and sequence output (e.g. Machine Translation: an RNN reads a sentence in English and then outputs a sentence in French). <strong>(5)</strong> Synced sequence input and output (e.g. video classification where we wish to label each frame of the video). Notice that in every case are no pre-specified constraints on the lengths sequences because the recurrent transformation (green) is fixed and can be applied as many times as we like.<br>æ¯ä¸ªçŸ©å½¢ä»£è¡¨ä¸€ä¸ªå‘é‡ï¼Œç®­å¤´ä»£è¡¨å‡½æ•°ï¼ˆä¾‹å¦‚çŸ©é˜µä¹˜æ³•ï¼‰ã€‚è¾“å…¥å‘é‡ç”¨çº¢è‰²è¡¨ç¤ºï¼Œè¾“å‡ºå‘é‡ç”¨è“è‰²è¡¨ç¤ºï¼Œç»¿è‰²å‘é‡è¡¨ç¤ºRNNçš„çŠ¶æ€ï¼ˆç¨åä¼šè¯¦ç»†è¯´æ˜ï¼‰ã€‚ä»å·¦åˆ°å³ä¾æ¬¡æ˜¯ï¼š</p>
<p><strong>(1)</strong> æ™®é€šæ¨¡å¼çš„å¤„ç†ï¼Œæ²¡æœ‰ä½¿ç”¨RNNï¼Œä»å›ºå®šå¤§å°çš„è¾“å…¥åˆ°å›ºå®šå¤§å°çš„è¾“å‡ºï¼ˆä¾‹å¦‚å›¾åƒåˆ†ç±»ï¼‰ã€‚</p>
<p><strong>(2)</strong> åºåˆ—è¾“å‡ºï¼ˆä¾‹å¦‚å›¾åƒæè¿°ç”Ÿæˆï¼Œè¾“å…¥ä¸€å¼ å›¾ç‰‡ï¼Œè¾“å‡ºä¸€ä¸ªå•è¯å¥å­ï¼‰ã€‚</p>
<p><strong>(3)</strong> åºåˆ—è¾“å…¥ï¼ˆä¾‹å¦‚æƒ…æ„Ÿåˆ†æï¼Œå°†ç»™å®šçš„å¥å­åˆ†ç±»ä¸ºè¡¨è¾¾æ­£é¢æˆ–è´Ÿé¢æƒ…æ„Ÿï¼‰ã€‚</p>
<p><strong>(4)</strong> åºåˆ—è¾“å…¥å’Œåºåˆ—è¾“å‡ºï¼ˆä¾‹å¦‚æœºå™¨ç¿»è¯‘ï¼šRNNè¯»å–ä¸€æ®µè‹±æ–‡å¥å­ï¼Œç„¶åè¾“å‡ºä¸€æ®µæ³•æ–‡å¥å­ï¼‰ã€‚</p>
<p><strong>(5)</strong> åŒæ­¥çš„åºåˆ—è¾“å…¥å’Œè¾“å‡ºï¼ˆä¾‹å¦‚è§†é¢‘åˆ†ç±»ï¼Œæˆ‘ä»¬å¸Œæœ›å¯¹è§†é¢‘çš„æ¯ä¸€å¸§è¿›è¡Œæ ‡ç­¾ï¼‰ã€‚</p>
<p>æ³¨æ„ï¼Œåœ¨æ¯ç§æƒ…å†µä¸‹ï¼Œåºåˆ—é•¿åº¦éƒ½æ²¡æœ‰é¢„å…ˆæŒ‡å®šçš„é™åˆ¶ï¼Œå› ä¸ºå¾ªç¯å˜æ¢ï¼ˆç»¿è‰²ï¼‰æ˜¯å›ºå®šçš„ï¼Œå¯ä»¥æ ¹æ®éœ€è¦åº”ç”¨å¤šæ¬¡ã€‚</p>
<p>As you might expect, the sequence regime of operation is much more powerful compared to fixed networks that are doomed from the get-go by a fixed number of computational steps, and hence also much more appealing for those of us who aspire to build more intelligent systems. Moreover, as weâ€™ll see in a bit, RNNs combine the input vector with their state vector with a fixed (but learned) function to produce a new state vector. This can in programming terms be interpreted as running a fixed program with certain inputs and some internal variables. Viewed this way, RNNs essentially describe programs. In fact, it is known that <a target="_blank" rel="noopener" href="http://binds.cs.umass.edu/papers/1995_Siegelmann_Science.pdf">RNNs are Turing-Complete</a> in the sense that they can to simulate arbitrary programs (with proper weights). But similar to universal approximation theorems for neural nets you shouldnâ€™t read too much into this. In fact, forget I said anything.<br>æ­£å¦‚ä½ æ‰€é¢„æ–™çš„é‚£æ ·ï¼Œç›¸è¾ƒäºå—é™äºå›ºå®šè®¡ç®—æ­¥éª¤çš„å›ºå®šç½‘ç»œï¼Œåºåˆ—æ“ä½œæ¨¡å¼è¦å¼ºå¤§å¾—å¤šï¼Œå› æ­¤å¯¹äºé‚£äº›å¸Œæœ›æ„å»ºæ›´æ™ºèƒ½ç³»ç»Ÿçš„äººæ¥è¯´ä¹Ÿæ›´å…·å¸å¼•åŠ›ã€‚æ­¤å¤–ï¼Œæ­£å¦‚æˆ‘ä»¬ç¨åä¼šçœ‹åˆ°çš„ï¼ŒRNNé€šè¿‡å›ºå®šï¼ˆä½†å¯å­¦ä¹ ï¼‰çš„å‡½æ•°å°†è¾“å…¥å‘é‡ä¸å…¶çŠ¶æ€å‘é‡ç»“åˆï¼Œç”Ÿæˆä¸€ä¸ªæ–°çš„çŠ¶æ€å‘é‡ã€‚è¿™åœ¨ç¼–ç¨‹æœ¯è¯­ä¸­å¯ä»¥ç†è§£ä¸ºè¿è¡Œä¸€ä¸ªå…·æœ‰ç‰¹å®šè¾“å…¥å’Œä¸€äº›å†…éƒ¨å˜é‡çš„å›ºå®šç¨‹åºã€‚ä»è¿™ä¸ªè§’åº¦æ¥çœ‹ï¼ŒRNNæœ¬è´¨ä¸Šæ˜¯åœ¨æè¿°ç¨‹åºã€‚äº‹å®ä¸Šï¼ŒRNNè¢«è®¤ä¸ºæ˜¯å›¾çµå®Œå¤‡çš„ï¼Œè¿™æ„å‘³ç€å®ƒä»¬å¯ä»¥æ¨¡æ‹Ÿä»»æ„ç¨‹åºï¼ˆåœ¨é€‚å½“çš„æƒé‡ä¸‹ï¼‰ã€‚ä½†æ˜¯ï¼Œä¸ç¥ç»ç½‘ç»œçš„é€šç”¨è¿‘ä¼¼å®šç†ç±»ä¼¼ï¼Œä½ ä¸åº”è¯¥å¯¹æ­¤è¿‡äºè§£è¯»ã€‚å®é™…ä¸Šï¼Œå¿˜æ‰æˆ‘åˆšæ‰è¯´çš„è¯å§ã€‚</p>
<blockquote>
<p>If training vanilla neural nets is optimization over functions, training recurrent nets is optimization over programs.<br>å¦‚æœè¯´è®­ç»ƒæ™®é€šç¥ç»ç½‘ç»œæ˜¯å¯¹å‡½æ•°çš„ä¼˜åŒ–ï¼Œé‚£ä¹ˆè®­ç»ƒå¾ªç¯ç½‘ç»œå°±æ˜¯å¯¹ç¨‹åºçš„ä¼˜åŒ–ã€‚</p>
</blockquote>
<p><strong>Sequential processing in absence of sequences</strong>. You might be thinking that having sequences as inputs or outputs could be relatively rare, but an important point to realize is that even if your inputs/outputs are fixed vectors, it is still possible to use this powerful formalism to _process_ them in a sequential manner. For instance, the figure below shows results from two very nice papers from <a target="_blank" rel="noopener" href="http://deepmind.com/">DeepMind</a>. On the left, an algorithm learns a recurrent network policy that steers its attention around an image; In particular, it learns to read out house numbers from left to right (<a target="_blank" rel="noopener" href="http://arxiv.org/abs/1412.7755">Ba et al.</a>). On the right, a recurrent network _generates_ images of digits by learning to sequentially add color to a canvas (<a target="_blank" rel="noopener" href="http://arxiv.org/abs/1502.04623">Gregor et al.</a>):<br>åœ¨æ²¡æœ‰åºåˆ—çš„æƒ…å†µä¸‹è¿›è¡Œé¡ºåºå¤„ç†ã€‚æ‚¨å¯èƒ½è®¤ä¸ºå°†åºåˆ—ä½œä¸ºè¾“å…¥æˆ–è¾“å‡ºå¯èƒ½ç›¸å¯¹ç½•è§ï¼Œä½†éœ€è¦æ„è¯†åˆ°çš„é‡è¦ä¸€ç‚¹æ˜¯ï¼Œå³ä½¿ä½ çš„è¾“å…¥/è¾“å‡ºæ˜¯å›ºå®šå‘é‡ï¼Œä»ç„¶å¯ä»¥ä½¿ç”¨è¿™ç§å¼ºå¤§çš„å½¢å¼ä¸»ä¹‰ä»¥é¡ºåºæ–¹å¼å¤„ç†å®ƒä»¬ã€‚ä¾‹å¦‚ï¼Œä¸‹å›¾æ˜¾ç¤ºäº† DeepMind çš„ä¸¤ç¯‡éå¸¸å¥½çš„è®ºæ–‡çš„ç»“æœã€‚åœ¨å·¦è¾¹ï¼Œç®—æ³•å­¦ä¹ ä¸€ä¸ªå¾ªç¯ç½‘ç»œç­–ç•¥ï¼Œå°†å…¶æ³¨æ„åŠ›å¼•å¯¼åˆ°å›¾åƒå‘¨å›´;ç‰¹åˆ«æ˜¯ï¼Œå®ƒå­¦ä¼šäº†ä»å·¦åˆ°å³è¯»å‡ºé—¨ç‰Œå·ï¼ˆBaç­‰äººï¼‰ã€‚åœ¨å³è¾¹ï¼Œä¸€ä¸ªå¾ªç¯ç½‘ç»œé€šè¿‡å­¦ä¹ ä¾æ¬¡å‘ç”»å¸ƒæ·»åŠ é¢œè‰²æ¥ç”Ÿæˆæ•°å­—å›¾åƒï¼š<br><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://karpathy.github.io/assets/rnn/house_read.gif" alt> <img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://karpathy.github.io/assets/rnn/house_generate.gif" alt></p>
<p>The takeaway is that even if your data is not in form of sequences, you can still formulate and train powerful models that learn to process it sequentially. Youâ€™re learning stateful programs that process your fixed-sized data.<br>è¦ç‚¹æ˜¯ï¼Œå³ä½¿ä½ çš„æ•°æ®ä¸æ˜¯ä»¥åºåˆ—å½¢å¼å­˜åœ¨ï¼Œä½ ä»ç„¶å¯ä»¥è®¾è®¡å’Œè®­ç»ƒå¼ºå¤§çš„æ¨¡å‹ï¼Œä½¿å…¶å­¦ä¼šä»¥é¡ºåºæ–¹å¼å¤„ç†è¿™äº›æ•°æ®ã€‚ä½ æ­£åœ¨å­¦ä¹ çš„æ˜¯å¤„ç†å›ºå®šå¤§å°æ•°æ®çš„æœ‰çŠ¶æ€ç¨‹åºã€‚</p>
<h2 id="rnn-computation"><a href="#RNN-computation" class="headerlink" title="RNN computation"></a>RNN computation</h2><p><strong>RNN computation.</strong> So how do these things work? At the core, RNNs have a deceptively simple API: They accept an input vector <code>x</code> and give you an output vector <code>y</code>. However, crucially this output vectorâ€™s contents are influenced not only by the input you just fed in, but also on the entire history of inputs youâ€™ve fed in in the past. Written as a class, the RNNâ€™s API consists of a single <code>step</code> function:<br>RNN è®¡ç®—ã€‚é‚£ä¹ˆè¿™äº›ä¸œè¥¿æ˜¯å¦‚ä½•å·¥ä½œçš„å‘¢ï¼Ÿåœ¨æ ¸å¿ƒä¸Šï¼ŒRNN æœ‰ä¸€ä¸ªçœ‹ä¼¼ç®€å•çš„ APIï¼šå®ƒä»¬æ¥å—ä¸€ä¸ªè¾“å…¥å‘é‡ <code>x</code> å¹¶ç»™ä½ ä¸€ä¸ªè¾“å‡ºå‘é‡ <code>y</code> ã€‚ç„¶è€Œï¼Œè‡³å…³é‡è¦çš„æ˜¯ï¼Œè¿™ä¸ªè¾“å‡ºå‘é‡çš„å†…å®¹ä¸ä»…å—åˆ°ä½ åˆšåˆšè¾“å…¥çš„è¾“å…¥çš„å½±å“ï¼Œè¿˜å—åˆ°ä½ è¿‡å»è¾“å…¥çš„æ•´ä¸ªè¾“å…¥å†å²çš„å½±å“ã€‚RNN çš„ API ç¼–å†™ä¸ºä¸€ä¸ªç±»ï¼Œç”±ä¸€ä¸ª <code>step</code> å‡½æ•°ç»„æˆï¼š<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rnn = RNN()</span><br><span class="line">y = rnn.step(x) # x is an input vector, y is the RNN&#x27;s output vector</span><br></pre></td></tr></table></figure><br>The RNN class has some internal state that it gets to update every time <code>step</code> is called. In the simplest case this state consists of a single _hidden_ vector <code>h</code>. Here is an implementation of the step function in a Vanilla RNN:<br>RNN ç±»å…·æœ‰ä¸€äº›å†…éƒ¨çŠ¶æ€ï¼Œæ¯æ¬¡è°ƒç”¨æ—¶ <code>step</code> éƒ½ä¼šæ›´æ–°ã€‚åœ¨æœ€ç®€å•çš„æƒ…å†µä¸‹ï¼Œæ­¤çŠ¶æ€ç”±å•ä¸ªéšè—å‘é‡ç»„æˆ <code>h</code> ã€‚ä»¥ä¸‹æ˜¯ Vanilla RNN ä¸­ step å‡½æ•°çš„å®ç°ï¼š</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">class RNN:</span><br><span class="line">  # ...</span><br><span class="line">  def step(self, x):</span><br><span class="line">    # update the hidden state</span><br><span class="line">    self.h = np.tanh(np.dot(self.W_hh, self.h) + np.dot(self.W_xh, x))</span><br><span class="line">    # compute the output vector</span><br><span class="line">    y = np.dot(self.W_hy, self.h)</span><br><span class="line">    return y</span><br></pre></td></tr></table></figure>
<p>The above specifies the forward pass of a vanilla RNN. This RNNâ€™s parameters are the three matrices <code>W_hh, W_xh, W_hy</code>. The hidden state <code>self.h</code> is initialized with the zero vector. The <code>np.tanh</code> function implements a non-linearity that squashes the activations to the range <code>[-1, 1]</code>. Notice briefly how this works: There are two terms inside of the tanh: one is based on the previous hidden state and one is based on the current input. In numpy <code>np.dot</code> is matrix multiplication. The two intermediates interact with addition, and then get squashed by the tanh into the new state vector. If youâ€™re more comfortable with math notation, we can also write the hidden state update as $â„_ğ‘¡=tanhâ¡(ğ‘Š_{â„â„}â„_{ğ‘¡âˆ’1}+ğ‘Š_{ğ‘¥â„}ğ‘¥_ğ‘¡)$, where tanh is applied elementwise.<br>ä¸Šè¿°å†…å®¹æè¿°äº†ä¸€ä¸ªåŸºç¡€RNNçš„å‰å‘ä¼ æ’­è¿‡ç¨‹ã€‚è¿™ä¸ªRNNçš„å‚æ•°æ˜¯ä¸‰ä¸ªçŸ©é˜µW_hhã€W_xhå’ŒW_hyã€‚éšè—çŠ¶æ€self.håˆå§‹åŒ–ä¸ºé›¶å‘é‡ã€‚np.tanhå‡½æ•°å®ç°äº†ä¸€ç§éçº¿æ€§æ¿€æ´»å‡½æ•°ï¼Œå°†æ¿€æ´»å€¼å‹ç¼©åˆ°[-1, 1]èŒƒå›´å†…ã€‚ç®€è¦è¯´æ˜å…¶å·¥ä½œåŸç†ï¼štanhå†…éƒ¨æœ‰ä¸¤ä¸ªé¡¹ï¼Œä¸€ä¸ªåŸºäºå‰ä¸€ä¸ªæ—¶é—´æ­¥éšè—çŠ¶æ€ï¼Œå¦ä¸€ä¸ªåŸºäºå½“å‰æ—¶é—´æ­¥è¾“å…¥ã€‚åœ¨numpyä¸­ï¼Œnp.dotè¡¨ç¤ºçŸ©é˜µä¹˜æ³•ã€‚è¿™ä¸¤ä¸ªä¸­é—´ç»“æœé€šè¿‡åŠ æ³•ç›¸äº’ä½œç”¨ï¼Œç„¶åé€šè¿‡tanhå‡½æ•°å‹ç¼©ä¸ºæ–°çš„çŠ¶æ€å‘é‡ã€‚å¦‚æœä½ å¯¹æ•°å­¦è¡¨ç¤ºæ³•æ›´ç†Ÿæ‚‰ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥å°†éšè—çŠ¶æ€çš„æ›´æ–°å†™æˆ $â„_ğ‘¡=tanhâ¡(ğ‘Š_{â„â„}â„_{ğ‘¡âˆ’1}+ğ‘Š_{ğ‘¥â„}ğ‘¥_ğ‘¡)$ï¼Œå…¶ä¸­tanhé€å…ƒç´ åº”ç”¨ã€‚</p>
<p>âš ï¸ï¼šnumpyæ˜¯Pythonä¸­ä¸€ä¸ªéå¸¸æµè¡Œçš„æ•°å€¼è®¡ç®—åº“ã€‚np.tanhå‡½æ•°å’Œnp.dotå‡½æ•°éƒ½æ˜¯numpyåº“ä¸­çš„å‡½æ•°ã€‚np.tanhå‡½æ•°ç”¨äºè®¡ç®—å…ƒç´ çº§çš„åŒæ›²æ­£åˆ‡ï¼Œè€Œnp.dotå‡½æ•°ç”¨äºæ‰§è¡ŒçŸ©é˜µä¹˜æ³•ã€‚</p>
<p>We initialize the matrices of the RNN with random numbers and the bulk of work during training goes into finding the matrices that give rise to desirable behavior, as measured with some loss function that expresses your preference to what kinds of outputs $y$ youâ€™d like to see in response to your input sequences $x$.<br>æˆ‘ä»¬ç”¨éšæœºæ•°åˆå§‹åŒ–RNNçš„çŸ©é˜µï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå¤§éƒ¨åˆ†å·¥ä½œæ˜¯æ‰¾åˆ°èƒ½å¤Ÿäº§ç”Ÿç†æƒ³è¡Œä¸ºçš„çŸ©é˜µï¼Œè¿™é€šè¿‡æŸç§æŸå¤±å‡½æ•°æ¥è¡¡é‡ï¼Œè¯¥æŸå¤±å‡½æ•°è¡¨è¾¾äº†ä½ å¯¹è¾“å…¥åºåˆ—$x$å¯¹åº”è¾“å‡º$y$çš„æœŸæœ›ã€‚</p>
<h2 id="going-deep"><a href="#Going-deep" class="headerlink" title="Going deep"></a>Going deep</h2><p><strong>Going deep</strong>. RNNs are neural networks and everything works monotonically better (if done right) if you put on your deep learning hat and start stacking models up like pancakes. For instance, we can form a 2-layer recurrent network as follows:<br>æ·±å…¥ç ”ç©¶ã€‚RNNæ˜¯ç¥ç»ç½‘ç»œçš„ä¸€ç§ï¼Œå¦‚æœæ–¹æ³•å¾—å½“ï¼Œé‡‡ç”¨æ·±åº¦å­¦ä¹ çš„æ–¹æ³•å¹¶åƒå ç…é¥¼ä¸€æ ·å°†æ¨¡å‹å †å èµ·æ¥ï¼Œä¸€åˆ‡éƒ½ä¼šå•è°ƒåœ°å˜å¾—æ›´å¥½ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥å¦‚ä¸‹æ„å»ºä¸€ä¸ªä¸¤å±‚çš„å¾ªç¯ç¥ç»ç½‘ç»œï¼š<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y1 = rnn1.step(x)</span><br><span class="line">y = rnn2.step(y1)</span><br></pre></td></tr></table></figure></p>
<p>In other words we have two separate RNNs: One RNN is receiving the input vectors and the second RNN is receiving the output of the first RNN as its input. Except neither of these RNNs know or care - itâ€™s all just vectors coming in and going out, and some gradients flowing through each module during backpropagation.<br>æ¢å¥è¯è¯´ï¼Œæˆ‘ä»¬æœ‰ä¸¤ä¸ªç‹¬ç«‹çš„ RNNï¼šä¸€ä¸ª RNN æ¥æ”¶è¾“å…¥å‘é‡ï¼Œç¬¬äºŒä¸ª RNN æ¥æ”¶ç¬¬ä¸€ä¸ª RNN çš„è¾“å‡ºä½œä¸ºå…¶è¾“å…¥ã€‚é™¤äº†è¿™äº› RNN éƒ½ä¸çŸ¥é“æˆ–ä¸å…³å¿ƒä¹‹å¤–â€”â€”å®ƒä»¬éƒ½åªæ˜¯è¿›å‡ºçš„å‘é‡ï¼Œä»¥åŠåœ¨åå‘ä¼ æ’­è¿‡ç¨‹ä¸­æµè¿‡æ¯ä¸ªæ¨¡å—çš„ä¸€äº›æ¢¯åº¦ã€‚</p>
<h2 id="getting-fancy"><a href="#Getting-fancy" class="headerlink" title="Getting fancy"></a>Getting fancy</h2><p><strong>Getting fancy</strong>. Iâ€™d like to briefly mention that in practice most of us use a slightly different formulation than what I presented above called a _Long Short-Term Memory_ (LSTM) network. The LSTM is a particular type of recurrent network that works slightly better in practice, owing to its more powerful update equation and some appealing backpropagation dynamics. I wonâ€™t go into details, but everything Iâ€™ve said about RNNs stays exactly the same, except the mathematical form for computing the update (the line <code>self.h = ...</code> ) gets a little more complicated. From here on I will use the terms â€œRNN/LSTMâ€ interchangeably but all experiments in this post use an LSTM.<br>æ›´å¤æ‚çš„æ¨¡å‹ã€‚åœ¨å®è·µä¸­ï¼Œæˆ‘ä»¬å¤§å¤šæ•°äººä½¿ç”¨çš„å…¬å¼ä¸æˆ‘ä¸Šé¢æåˆ°çš„ç¨æœ‰ä¸åŒï¼Œè¢«ç§°ä¸ºé•¿çŸ­æœŸè®°å¿†ç½‘ç»œï¼ˆLSTMï¼‰ã€‚LSTMæ˜¯ä¸€ç§ç‰¹å®šç±»å‹çš„å¾ªç¯ç¥ç»ç½‘ç»œï¼Œå®é™…ä¸Šæ•ˆæœæ›´å¥½ï¼Œå› ä¸ºå®ƒå…·æœ‰æ›´å¼ºå¤§çš„æ›´æ–°æ–¹ç¨‹å’Œä¸€äº›æ›´å…·å¸å¼•åŠ›çš„åå‘ä¼ æ’­åŠ¨æ€ã€‚æˆ‘ä¸ä¼šæ·±å…¥è®¨è®ºç»†èŠ‚ï¼Œä½†æˆ‘æ‰€è¯´çš„å…³äºRNNçš„ä¸€åˆ‡éƒ½å®Œå…¨ç›¸åŒï¼Œé™¤äº†è®¡ç®—æ›´æ–°çš„æ•°å­¦å½¢å¼ï¼ˆå³self.h = â€¦è¿™ä¸€è¡Œï¼‰å˜å¾—ç¨å¾®å¤æ‚äº†ä¸€äº›ã€‚ä»ç°åœ¨å¼€å§‹ï¼Œæˆ‘ä¼šäº¤æ›¿ä½¿ç”¨â€œRNN/LSTMâ€è¿™ä¸¤ä¸ªæœ¯è¯­ï¼Œä½†æœ¬æ–‡ä¸­çš„æ‰€æœ‰å®éªŒéƒ½ä½¿ç”¨LSTMã€‚</p>
<h1 id="character-level-language-models"><a href="#Character-Level-Language-Models" class="headerlink" title="Character-Level Language Models"></a>Character-Level Language Models</h1><p>å­—ç¬¦çº§è¯­è¨€æ¨¡å‹</p>
<p>Okay, so we have an idea about what RNNs are, why they are super exciting, and how they work. Weâ€™ll now ground this in a fun application: Weâ€™ll train RNN character-level language models. That is, weâ€™ll give the RNN a huge chunk of text and ask it to model the probability distribution of the next character in the sequence given a sequence of previous characters. This will then allow us to generate new text one character at a time.<br>å¥½çš„ï¼Œæ‰€ä»¥æˆ‘ä»¬å·²ç»å¯¹RNNæ˜¯ä»€ä¹ˆã€ä¸ºä»€ä¹ˆå®ƒä»¬éå¸¸ä»¤äººå…´å¥‹ä»¥åŠå®ƒä»¬å¦‚ä½•å·¥ä½œæœ‰äº†ä¸€å®šçš„äº†è§£ã€‚ç°åœ¨ï¼Œæˆ‘ä»¬å°†æŠŠè¿™äº›çŸ¥è¯†åº”ç”¨åˆ°ä¸€ä¸ªæœ‰è¶£çš„å®é™…åº”ç”¨ä¸­ï¼šæˆ‘ä»¬å°†è®­ç»ƒRNNå­—ç¬¦çº§åˆ«çš„è¯­è¨€æ¨¡å‹ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œæˆ‘ä»¬ä¼šç»™RNNæä¾›ä¸€å¤§æ®µæ–‡æœ¬ï¼Œå¹¶è®©å®ƒæ ¹æ®å‰é¢å­—ç¬¦çš„åºåˆ—æ¥å»ºæ¨¡ä¸‹ä¸€ä¸ªå­—ç¬¦çš„æ¦‚ç‡åˆ†å¸ƒã€‚è¿™æ ·ä¸€æ¥ï¼Œæˆ‘ä»¬å°±å¯ä»¥ä¸€æ¬¡ç”Ÿæˆä¸€ä¸ªå­—ç¬¦çš„æ–°æ–‡æœ¬ã€‚</p>
<p>As a working example, suppose we only had a vocabulary of four possible letters â€œheloâ€, and wanted to train an RNN on the training sequence â€œhelloâ€. This training sequence is in fact a source of 4 separate training examples: 1. The probability of â€œeâ€ should be likely given the context of â€œhâ€, 2. â€œlâ€ should be likely in the context of â€œheâ€, 3. â€œlâ€ should also be likely given the context of â€œhelâ€, and finally 4. â€œoâ€ should be likely given the context of â€œhellâ€.<br>ä½œä¸ºä¸€ä¸ªå®é™…ä¾‹å­ï¼Œå‡è®¾æˆ‘ä»¬åªæœ‰å››ä¸ªå¯èƒ½çš„å­—æ¯â€œheloâ€çš„è¯æ±‡è¡¨ï¼Œå¹¶ä¸”æƒ³è¦åœ¨è®­ç»ƒåºåˆ—â€œhelloâ€ä¸Šè®­ç»ƒä¸€ä¸ªRNNã€‚è¿™ä¸ªè®­ç»ƒåºåˆ—å®é™…ä¸Šæ˜¯å››ä¸ªç‹¬ç«‹çš„è®­ç»ƒç¤ºä¾‹çš„æ¥æºï¼š</p>
<ol>
<li>åœ¨â€œhâ€çš„ä¸Šä¸‹æ–‡ä¸­ï¼Œâ€œeâ€çš„æ¦‚ç‡åº”è¯¥å¾ˆå¤§ã€‚</li>
<li>åœ¨â€œheâ€çš„ä¸Šä¸‹æ–‡ä¸­ï¼Œâ€œlâ€çš„æ¦‚ç‡åº”è¯¥å¾ˆå¤§ã€‚</li>
<li>åœ¨â€œhelâ€çš„ä¸Šä¸‹æ–‡ä¸­ï¼Œâ€œlâ€çš„æ¦‚ç‡ä¹Ÿåº”è¯¥å¾ˆå¤§ã€‚</li>
<li>æœ€åï¼Œåœ¨â€œhellâ€çš„ä¸Šä¸‹æ–‡ä¸­ï¼Œâ€œoâ€çš„æ¦‚ç‡åº”è¯¥å¾ˆå¤§ã€‚</li>
</ol>
<p>Concretely, we will encode each character into a vector using 1-of-k encoding (i.e. all zero except for a single one at the index of the character in the vocabulary), and feed them into the RNN one at a time with the <code>step</code> function. We will then observe a sequence of 4-dimensional output vectors (one dimension per character), which we interpret as the confidence the RNN currently assigns to each character coming next in the sequence. Hereâ€™s a diagram:<br>å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨1-of-kç¼–ç å°†æ¯ä¸ªå­—ç¬¦ç¼–ç æˆä¸€ä¸ªå‘é‡ï¼ˆå³ï¼Œé™¤äº†åœ¨è¯æ±‡è¡¨ä¸­å­—ç¬¦ç´¢å¼•å¤„ä¸º1ï¼Œå…¶ä½™å…¨ä¸º0ï¼‰ï¼Œç„¶åç”¨stepå‡½æ•°å°†å®ƒä»¬é€ä¸€è¾“å…¥RNNã€‚éšåï¼Œæˆ‘ä»¬ä¼šå¾—åˆ°ä¸€ç³»åˆ—4ç»´è¾“å‡ºå‘é‡ï¼ˆæ¯ä¸ªå­—ç¬¦ä¸€ä¸ªç»´åº¦ï¼‰ï¼Œæˆ‘ä»¬å°†è¿™äº›è¾“å‡ºå‘é‡è§£é‡Šä¸ºRNNå½“å‰å¯¹åºåˆ—ä¸­ä¸‹ä¸€ä¸ªå­—ç¬¦çš„ç½®ä¿¡åº¦ã€‚ä»¥ä¸‹æ˜¯ä¸€ä¸ªç¤ºæ„å›¾ï¼š</p>
<img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="/2472be8a/2.png" class>
<p>An example RNN with 4-dimensional input and output layers, and a hidden layer of 3 units (neurons). This diagram shows the activations in the forward pass when the RNN is fed the characters â€œhellâ€ as input. The output layer contains confidences the RNN assigns for the next character (vocabulary is â€œh,e,l,oâ€); We want the green numbers to be high and red numbers to be low.<br>å…·æœ‰ 4 ç»´è¾“å…¥å’Œè¾“å‡ºå±‚çš„ç¤ºä¾‹ RNNï¼Œä»¥åŠ 3 ä¸ªå•å…ƒï¼ˆç¥ç»å…ƒï¼‰çš„éšè—å±‚ã€‚æ­¤å›¾æ˜¾ç¤ºäº†å°†å­—ç¬¦â€œhellâ€ä½œä¸ºè¾“å…¥é¦ˆé€ RNN æ—¶å‰å‘ä¼ é€’ä¸­çš„æ¿€æ´»ã€‚è¾“å‡ºå±‚åŒ…å« RNN ä¸ºä¸‹ä¸€ä¸ªå­—ç¬¦åˆ†é…çš„ç½®ä¿¡åº¦ï¼ˆè¯æ±‡ä¸ºâ€œhï¼Œeï¼Œlï¼Œoâ€ï¼‰;æˆ‘ä»¬å¸Œæœ›ç»¿è‰²æ•°å­—é«˜ï¼Œçº¢è‰²æ•°å­—ä½ã€‚</p>
<p>âš ï¸ï¼š W_xh æŒ‡è¾“å…¥å±‚å’Œéšè—å±‚ä¹‹é—´çš„æƒé‡çŸ©é˜µï¼Œ W_hh æŒ‡éšè—å±‚ä¹‹é—´çš„æƒé‡çŸ©é˜µï¼Œ W_hy æŒ‡éšè—å±‚å’Œè¾“å‡ºå±‚ä¹‹é—´çš„æƒé‡çŸ©é˜µ</p>
<p>For example, we see that in the first time step when the RNN saw the character â€œhâ€ it assigned confidence of 1.0 to the next letter being â€œhâ€, 2.2 to letter â€œeâ€, -3.0 to â€œlâ€, and 4.1 to â€œoâ€. Since in our training data (the string â€œhelloâ€) the next correct character is â€œeâ€, we would like to increase its confidence (green) and decrease the confidence of all other letters (red). Similarly, we have a desired target character at every one of the 4 time steps that weâ€™d like the network to assign a greater confidence to. Since the RNN consists entirely of differentiable operations we can run the backpropagation algorithm (this is just a recursive application of the chain rule from calculus) to figure out in what direction we should adjust every one of its weights to increase the scores of the correct targets (green bold numbers). We can then perform a _parameter update_, which nudges every weight a tiny amount in this gradient direction. If we were to feed the same inputs to the RNN after the parameter update we would find that the scores of the correct characters (e.g. â€œeâ€ in the first time step) would be slightly higher (e.g. 2.3 instead of 2.2), and the scores of incorrect characters would be slightly lower. We then repeat this process over and over many times until the network converges and its predictions are eventually consistent with the training data in that correct characters are always predicted next.<br>ä¾‹å¦‚ï¼Œæˆ‘ä»¬çœ‹åˆ°åœ¨ç¬¬ä¸€ä¸ªæ—¶é—´æ­¥ä¸­ï¼Œå½“RNNçœ‹åˆ°å­—ç¬¦â€œhâ€æ—¶ï¼Œå®ƒå¯¹ä¸‹ä¸€ä¸ªå­—ç¬¦çš„ç½®ä¿¡åº¦åˆ†é…ä¸ºï¼šå­—ç¬¦â€œhâ€æ˜¯1.0ï¼Œå­—ç¬¦â€œeâ€æ˜¯2.2ï¼Œå­—ç¬¦â€œlâ€æ˜¯-3.0ï¼Œå­—ç¬¦â€œoâ€æ˜¯4.1ã€‚ç”±äºåœ¨æˆ‘ä»¬çš„è®­ç»ƒæ•°æ®ï¼ˆå­—ç¬¦ä¸²â€œhelloâ€ï¼‰ä¸­ï¼Œä¸‹ä¸€ä¸ªæ­£ç¡®å­—ç¬¦æ˜¯â€œeâ€ï¼Œæˆ‘ä»¬å¸Œæœ›å¢åŠ â€œeâ€çš„ç½®ä¿¡åº¦ï¼ˆç”¨ç»¿è‰²è¡¨ç¤ºï¼‰ï¼Œå¹¶é™ä½æ‰€æœ‰å…¶ä»–å­—ç¬¦çš„ç½®ä¿¡åº¦ï¼ˆç”¨çº¢è‰²è¡¨ç¤ºï¼‰ã€‚ç±»ä¼¼åœ°ï¼Œåœ¨æ¯ä¸€ä¸ªæ—¶é—´æ­¥ä¸Šï¼Œæˆ‘ä»¬éƒ½æœ‰ä¸€ä¸ªæœŸæœ›çš„ç›®æ ‡å­—ç¬¦ï¼Œå¸Œæœ›ç½‘ç»œèƒ½å¯¹å…¶åˆ†é…æ›´é«˜çš„ç½®ä¿¡åº¦ã€‚ç”±äºRNNå®Œå…¨ç”±å¯å¾®æ“ä½œç»„æˆï¼Œæˆ‘ä»¬å¯ä»¥è¿è¡Œåå‘ä¼ æ’­ç®—æ³•ï¼ˆè¿™åªæ˜¯å¾®ç§¯åˆ†ä¸­é“¾å¼æ³•åˆ™çš„é€’å½’åº”ç”¨ï¼‰æ¥ç¡®å®šåº”è°ƒæ•´æ¯ä¸ªæƒé‡çš„æ–¹å‘ï¼Œä»¥æé«˜æ­£ç¡®ç›®æ ‡çš„å¾—åˆ†ï¼ˆç»¿è‰²åŠ ç²—æ•°å­—ï¼‰ã€‚ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥æ‰§è¡Œå‚æ•°æ›´æ–°ï¼Œå°†æ¯ä¸ªæƒé‡åœ¨è¯¥æ¢¯åº¦æ–¹å‘ä¸Šå¾®è°ƒä¸€ä¸ªå°é‡ã€‚å¦‚æœåœ¨å‚æ•°æ›´æ–°åå†æ¬¡å°†ç›¸åŒçš„è¾“å…¥æä¾›ç»™RNNï¼Œæˆ‘ä»¬ä¼šå‘ç°æ­£ç¡®å­—ç¬¦çš„å¾—åˆ†ï¼ˆä¾‹å¦‚ï¼Œç¬¬ä¸€ä¸ªæ—¶é—´æ­¥ä¸­çš„â€œeâ€ï¼‰ä¼šç•¥æœ‰æé«˜ï¼ˆä¾‹å¦‚ï¼Œä»2.2æé«˜åˆ°2.3ï¼‰ï¼Œè€Œé”™è¯¯å­—ç¬¦çš„å¾—åˆ†ä¼šç•¥æœ‰é™ä½ã€‚ç„¶åï¼Œæˆ‘ä»¬åå¤è¿›è¡Œè¿™ä¸ªè¿‡ç¨‹å¤šæ¬¡ï¼Œç›´åˆ°ç½‘ç»œæ”¶æ•›ï¼Œå…¶é¢„æµ‹æœ€ç»ˆä¸è®­ç»ƒæ•°æ®ä¸€è‡´ï¼Œå³æ€»æ˜¯é¢„æµ‹å‡ºæ­£ç¡®çš„ä¸‹ä¸€ä¸ªå­—ç¬¦ã€‚</p>
<p>âš ï¸ï¼šå›¾ä¸­output å±‚ æ•°å­—å¹¶ä¸æ˜¯ç½®ä¿¡åº¦ï¼Œè€Œæ˜¯logits, è¿™äº›logitså¹¶ä¸ç›´æ¥è¡¨ç¤ºæ¦‚ç‡/ç½®ä¿¡åº¦,è¦å°†è¿™äº›logitsè½¬åŒ–ä¸ºæ¦‚ç‡ï¼ˆç½®ä¿¡åº¦ï¼‰ï¼Œæˆ‘ä»¬é€šå¸¸ä½¿ç”¨Softmaxå‡½æ•°ã€‚</p>
<p>A more technical explanation is that we use the standard Softmax classifier (also commonly referred to as the cross-entropy loss) on every output vector simultaneously. The RNN is trained with mini-batch Stochastic Gradient Descent and I like to use <a target="_blank" rel="noopener" href="http://arxiv.org/abs/1502.04390">RMSProp</a> or Adam (per-parameter adaptive learning rate methods) to stablilize the updates.<br>æ›´æŠ€æœ¯æ€§çš„è§£é‡Šæ˜¯ï¼Œæˆ‘ä»¬åœ¨æ¯ä¸ªè¾“å‡ºå‘é‡ä¸Š åŒæ—¶ä½¿ç”¨æ ‡å‡†çš„Softmaxåˆ†ç±»å™¨ï¼ˆä¹Ÿå¸¸è¢«ç§°ä¸ºäº¤å‰ç†µæŸå¤±ï¼‰ã€‚RNNä½¿ç”¨å°æ‰¹é‡éšæœºæ¢¯åº¦ä¸‹é™æ³•è¿›è¡Œè®­ç»ƒï¼Œæˆ‘å–œæ¬¢ä½¿ç”¨RMSPropæˆ–Adamï¼ˆæ¯ä¸ªå‚æ•°çš„è‡ªé€‚åº”å­¦ä¹ ç‡æ–¹æ³•ï¼‰æ¥ç¨³å®šæ›´æ–°ã€‚</p>
<p>Notice also that the first time the character â€œlâ€ is input, the target is â€œlâ€, but the second time the target is â€œoâ€. The RNN therefore cannot rely on the input alone and must use its recurrent connection to keep track of the context to achieve this task.<br>å¦è¯·æ³¨æ„ï¼Œç¬¬ä¸€æ¬¡è¾“å…¥å­—ç¬¦â€œlâ€æ—¶ï¼Œç›®æ ‡æ˜¯â€œlâ€ï¼Œä½†ç¬¬äºŒæ¬¡ç›®æ ‡æ˜¯â€œoâ€ã€‚å› æ­¤ï¼ŒRNN ä¸èƒ½å•ç‹¬ä¾èµ–è¾“å…¥ï¼Œå¿…é¡»ä½¿ç”¨å…¶å¾ªç¯è¿æ¥æ¥è·Ÿè¸ªä¸Šä¸‹æ–‡ä»¥å®ç°æ­¤ä»»åŠ¡ã€‚</p>
<p>At <strong>test time</strong>, we feed a character into the RNN and get a distribution over what characters are likely to come next. We sample from this distribution, and feed it right back in to get the next letter. Repeat this process and youâ€™re sampling text! Lets now train an RNN on different datasets and see what happens.<br>åœ¨æµ‹è¯•æ—¶ï¼Œæˆ‘ä»¬å°†ä¸€ä¸ªå­—ç¬¦è¾“å…¥RNNï¼Œå¹¶è·å¾—ä¸‹ä¸€ä¸ªå­—ç¬¦å¯èƒ½å‡ºç°çš„æ¦‚ç‡åˆ†å¸ƒã€‚æˆ‘ä»¬ä»è¿™ä¸ªåˆ†å¸ƒä¸­é‡‡æ ·ï¼Œå¹¶å°†é‡‡æ ·å¾—åˆ°çš„å­—ç¬¦å†æ¬¡è¾“å…¥RNNä»¥è·å–ä¸‹ä¸€ä¸ªå­—ç¬¦ã€‚é‡å¤è¿™ä¸ªè¿‡ç¨‹ï¼Œå°±å¯ä»¥ç”Ÿæˆæ–‡æœ¬äº†ï¼ç°åœ¨ï¼Œè®©æˆ‘ä»¬åœ¨ä¸åŒçš„æ•°æ®é›†ä¸Šè®­ç»ƒä¸€ä¸ªRNNï¼Œçœ‹çœ‹ä¼šå‘ç”Ÿä»€ä¹ˆã€‚</p>
<p>To further clarify, for educational purposes I also wrote a <a target="_blank" rel="noopener" href="https://gist.github.com/karpathy/d4dee566867f8291f086">minimal character-level RNN language model in Python/numpy</a>. It is only about 100 lines long and hopefully it gives a concise, concrete and useful summary of the above if youâ€™re better at reading code than text. Weâ€™ll now dive into example results, produced with the much more efficient Lua/Torch codebase.<br>ä¸ºäº†è¿›ä¸€æ­¥è¯´æ˜ï¼Œæˆ‘è¿˜ç”¨Pythonå’Œnumpyç¼–å†™äº†ä¸€ä¸ªæœ€å°çš„å­—ç¬¦çº§RNNè¯­è¨€æ¨¡å‹ã€‚å®ƒåªæœ‰å¤§çº¦100è¡Œä»£ç ï¼Œå¸Œæœ›èƒ½ä¸ºä½ æä¾›ä¸€ä¸ªç®€æ´ã€å…·ä½“ä¸”æœ‰ç”¨çš„æ€»ç»“ï¼Œå¦‚æœä½ æ›´æ“…é•¿é˜…è¯»ä»£ç è€Œä¸æ˜¯æ–‡å­—ã€‚ç°åœ¨ï¼Œæˆ‘ä»¬å°†æ·±å…¥æ¢è®¨ä½¿ç”¨æ›´åŠ é«˜æ•ˆçš„Lua/Torchä»£ç åº“ç”Ÿæˆçš„ç¤ºä¾‹ç»“æœã€‚</p>
<h1 id="fun-with-rnns"><a href="#Fun-with-RNNs" class="headerlink" title="Fun with RNNs"></a>Fun with RNNs</h1><p>All 5 example character models below were trained with the <a target="_blank" rel="noopener" href="https://github.com/karpathy/char-rnn">code</a> Iâ€™m releasing on Github. The input in each case is a single file with some text, and weâ€™re training an RNN to predict the next character in the sequence.<br>ä¸‹é¢çš„æ‰€æœ‰ 5 ä¸ªç¤ºä¾‹å­—ç¬¦æ¨¡å‹éƒ½æ˜¯ä½¿ç”¨æˆ‘åœ¨ Github ä¸Šå‘å¸ƒçš„ä»£ç è®­ç»ƒçš„ã€‚æ¯ç§æƒ…å†µä¸‹çš„è¾“å…¥éƒ½æ˜¯ä¸€ä¸ªåŒ…å«ä¸€äº›æ–‡æœ¬çš„å•ä¸ªæ–‡ä»¶ï¼Œæˆ‘ä»¬æ­£åœ¨è®­ç»ƒä¸€ä¸ª RNN æ¥é¢„æµ‹åºåˆ—ä¸­çš„ä¸‹ä¸€ä¸ªå­—ç¬¦ã€‚</p>
<h3 id="paul-graham-generator-ä¿ç½—æ ¼é›·å„å§†å‘ç”µæœº"><a href="#Paul-Graham-generator-ä¿ç½—Â·æ ¼é›·å„å§†å‘ç”µæœº" class="headerlink" title="Paul Graham generator ä¿ç½—Â·æ ¼é›·å„å§†å‘ç”µæœº"></a>Paul Graham generator ä¿ç½—Â·æ ¼é›·å„å§†å‘ç”µæœº</h3><p>Lets first try a small dataset of English as a sanity check. My favorite fun dataset is the concatenation of <a target="_blank" rel="noopener" href="http://www.paulgraham.com/articles.html">Paul Grahamâ€™s essays</a>. The basic idea is that thereâ€™s a lot of wisdom in these essays, but unfortunately Paul Graham is a relatively slow generator. Wouldnâ€™t it be great if we could sample startup wisdom on demand? Thatâ€™s where an RNN comes in.<br>è®©æˆ‘ä»¬é¦–å…ˆå°è¯•ä¸€ä¸ªå°å‹çš„è‹±æ–‡æ•°æ®é›†æ¥è¿›è¡ŒåŸºæœ¬æ£€æŸ¥ã€‚æˆ‘æœ€å–œæ¬¢çš„æœ‰è¶£æ•°æ®é›†æ˜¯ä¿ç½—Â·æ ¼é›·å„å§†ï¼ˆPaul Grahamï¼‰çš„æ–‡ç« åˆé›†ã€‚åŸºæœ¬æƒ³æ³•æ˜¯ï¼Œè¿™äº›æ–‡ç« ä¸­æœ‰å¾ˆå¤šæ™ºæ…§ï¼Œä½†é—æ†¾çš„æ˜¯ï¼Œä¿ç½—Â·æ ¼é›·å„å§†çš„å†™ä½œé€Ÿåº¦ç›¸å¯¹è¾ƒæ…¢ã€‚å¦‚æœæˆ‘ä»¬èƒ½æŒ‰éœ€é‡‡æ ·åˆ›ä¸šæ™ºæ…§ï¼Œé‚£ä¸æ˜¯å¾ˆæ£’å—ï¼Ÿè¿™æ­£æ˜¯RNNçš„ç”¨æ­¦ä¹‹åœ°ã€‚</p>
<p>Concatenating all pg essays over the last ~5 years we get approximately 1MB text file, or about 1 million characters (this is considered a very small dataset by the way). _Technical:_ Lets train a 2-layer LSTM with 512 hidden nodes (approx. 3.5 million parameters), and with dropout of 0.5 after each layer. Weâ€™ll train with batches of 100 examples and truncated backpropagation through time of length 100 characters. With these settings one batch on a TITAN Z GPU takes about 0.46 seconds (this can be cut in half with 50 character BPTT at negligible cost in performance). Without further ado, lets see a sample from the RNN:<br>å°†è¿‡å»å¤§çº¦5å¹´é—´çš„æ‰€æœ‰ä¿ç½—Â·æ ¼é›·å„å§†çš„æ–‡ç« åˆå¹¶èµ·æ¥ï¼Œæˆ‘ä»¬å¾—åˆ°äº†ä¸€ä¸ªå¤§çº¦1MBçš„æ–‡æœ¬æ–‡ä»¶ï¼Œçº¦100ä¸‡ä¸ªå­—ç¬¦ï¼ˆé¡ºä¾¿è¯´ä¸€ä¸‹ï¼Œè¿™è¢«è®¤ä¸ºæ˜¯ä¸€ä¸ªéå¸¸å°çš„æ•°æ®é›†ï¼‰ã€‚æŠ€æœ¯ç»†èŠ‚ï¼šè®©æˆ‘ä»¬è®­ç»ƒä¸€ä¸ªå…·æœ‰2å±‚ã€æ¯å±‚512ä¸ªéšè—èŠ‚ç‚¹çš„LSTMï¼ˆå¤§çº¦350ä¸‡ä¸ªå‚æ•°ï¼‰ï¼Œå¹¶åœ¨æ¯å±‚ä¹‹åä½¿ç”¨0.5çš„dropoutã€‚æˆ‘ä»¬å°†ä½¿ç”¨100ä¸ªæ ·æœ¬çš„æ‰¹æ¬¡å’Œé•¿åº¦ä¸º100å­—ç¬¦çš„æˆªæ–­æ—¶é—´åå‘ä¼ æ’­è¿›è¡Œè®­ç»ƒã€‚åœ¨è¿™äº›è®¾ç½®ä¸‹ï¼Œåœ¨TITAN Z GPUä¸Šå¤„ç†ä¸€ä¸ªæ‰¹æ¬¡å¤§çº¦éœ€è¦0.46ç§’ï¼ˆä½¿ç”¨50å­—ç¬¦çš„æˆªæ–­æ—¶é—´åå‘ä¼ æ’­ï¼Œå‡ ä¹ä¸ä¼šå½±å“æ€§èƒ½ï¼Œå¯ä»¥å°†æ—¶é—´å‡åŠï¼‰ã€‚äº‹ä¸å®œè¿Ÿï¼Œè®©æˆ‘ä»¬çœ‹çœ‹RNNç”Ÿæˆçš„ä¸€ä¸ªæ ·æœ¬ï¼š</p>
<p>_â€œThe surprised in investors werenâ€™t going to raise money. Iâ€™m not the company with the time there are all interesting quickly, donâ€™t have to get off the same programmers. Thereâ€™s a super-angel round fundraising, why do you can do. If you have a different physical investment are become in people who reduced in a startup with the way to argument the acquirer could see them just that youâ€™re also the founders will part of usersâ€™ affords that and an alternation to the idea. [2] Donâ€™t work at first member to see the way kids will seem in advance of a bad successful startup. And if you have to act the big company too.â€_<br>_â€œæŠ•èµ„è€…çš„æƒŠè®¶æ˜¯ï¼Œä»–ä»¬å¹¶ä¸æ‰“ç®—ç­¹é›†èµ„é‡‘ã€‚æˆ‘ä¸æ˜¯é‚£ä¸ªæœ‰æ—¶é—´çš„å…¬å¸ï¼Œæœ‰è¶£çš„äº‹æƒ…å¾ˆå¿«å°±ä¼šå‡ºç°ï¼Œä¸éœ€è¦è®©ç›¸åŒçš„ç¨‹åºå‘˜ç¦»å¼€ã€‚æœ‰ä¸€ä¸ªè¶…çº§å¤©ä½¿è½®çš„èèµ„ï¼Œä½ ä¸ºä»€ä¹ˆè¦è¿™æ ·åšã€‚å¦‚æœä½ æœ‰ä¸åŒçš„å®ä½“æŠ•èµ„ï¼Œä¼šæˆä¸ºé‚£äº›åœ¨åˆåˆ›å…¬å¸é‡Œå‡å°‘çš„äººä¸­äº‰è®ºçš„æ–¹å¼ï¼Œæ”¶è´­è€…å¯èƒ½ä¼šçœ‹åˆ°ä»–ä»¬åªæ˜¯åˆ›å§‹äººå°†æˆä¸ºç”¨æˆ·åŠªåŠ›çš„ä¸€éƒ¨åˆ†ï¼Œè¿™æ˜¯å¯¹æƒ³æ³•çš„ä¸€ç§æ›¿ä»£ã€‚[2] ä¸€å¼€å§‹ä¸è¦åœ¨æˆå‘˜èº«ä¸Šå·¥ä½œï¼Œçœ‹å­©å­ä»¬å°†å¦‚ä½•æå‰åœ¨ä¸€ä¸ªå¤±è´¥çš„æˆåŠŸåˆåˆ›å…¬å¸ä¸­è¡¨ç°å‡ºæ¥ã€‚è€Œä¸”ï¼Œå¦‚æœä½ å¿…é¡»è¡ŒåŠ¨ï¼Œé‚£ä¹ˆå¤§å…¬å¸ä¹Ÿä¸€æ ·ã€‚â€_</p>
<p>Okay, clearly the above is unfortunately not going to replace Paul Graham anytime soon, but remember that the RNN had to learn English completely from scratch and with a small dataset (including where you put commas, apostrophes and spaces). I also like that it learns to support its own arguments (e.g. [2], above). Sometimes it says something that offers a glimmer of insight, such as _â€œa company is a meeting to think to investorsâ€_. <a target="_blank" rel="noopener" href="http://cs.stanford.edu/people/karpathy/char-rnn/pg.txt">Hereâ€™s</a> a link to 50K character sample if youâ€™d like to see more.<br>å¥½çš„ï¼Œå¾ˆæ˜æ˜¾ï¼Œä»¥ä¸Šå†…å®¹æš‚æ—¶è¿˜æ— æ³•æ›¿ä»£ä¿ç½—Â·æ ¼é›·å„å§†ï¼Œä½†è¯·è®°ä½ï¼ŒRNNå¿…é¡»ä»é›¶å¼€å§‹å­¦ä¹ è‹±è¯­ï¼Œè€Œä¸”æ˜¯ç”¨ä¸€ä¸ªå°æ•°æ®é›†ï¼ˆåŒ…æ‹¬é€—å·ã€æ’‡å·å’Œç©ºæ ¼çš„ä½ç½®ï¼‰ã€‚æˆ‘ä¹Ÿå–œæ¬¢å®ƒå­¦ä¼šäº†æ”¯æŒè‡ªå·±çš„è®ºç‚¹ï¼ˆä¾‹å¦‚ï¼Œä¸Šæ–‡ä¸­çš„[2]ï¼‰ã€‚æœ‰æ—¶ï¼Œå®ƒä¼šè¯´å‡ºä¸€äº›ç•¥å¸¦å¯å‘æ€§çš„è¯ï¼Œæ¯”å¦‚â€œa company is a meeting to think to investorsâ€ï¼ˆå…¬å¸æ˜¯ä¸æŠ•èµ„è€…æ€è€ƒçš„ä¼šè®®ï¼‰ã€‚å¦‚æœä½ æƒ³æŸ¥çœ‹æ›´å¤šï¼Œè¿™é‡Œæœ‰ä¸€ä¸ª50Kå­—ç¬¦çš„æ ·æœ¬é“¾æ¥ã€‚</p>
<p><strong>Temperature.</strong> We can also play with the temperature of the Softmax during sampling. Decreasing the temperature from 1 to some lower number (e.g. 0.5) makes the RNN more confident, but also more conservative in its samples. Conversely, higher temperatures will give more diversity but at cost of more mistakes (e.g. spelling mistakes, etc). In particular, setting temperature very near zero will give the most likely thing that Paul Graham might say:<br>æ¸©åº¦ã€‚æˆ‘ä»¬ä¹Ÿå¯ä»¥åœ¨é‡‡æ ·è¿‡ç¨‹ä¸­è°ƒæ•´Softmaxçš„æ¸©åº¦ã€‚å°†æ¸©åº¦ä»1é™ä½åˆ°æŸä¸ªè¾ƒä½çš„æ•°å€¼ï¼ˆä¾‹å¦‚0.5ï¼‰ï¼Œä¼šä½¿RNNæ›´æœ‰ä¿¡å¿ƒï¼Œä½†ä¹Ÿæ›´ä¿å®ˆäºå…¶é‡‡æ ·ç»“æœã€‚ç›¸åï¼Œè¾ƒé«˜çš„æ¸©åº¦ä¼šå¸¦æ¥æ›´å¤šçš„å¤šæ ·æ€§ï¼Œä½†ä»£ä»·æ˜¯ä¼šæœ‰æ›´å¤šçš„é”™è¯¯ï¼ˆä¾‹å¦‚æ‹¼å†™é”™è¯¯ç­‰ï¼‰ã€‚ç‰¹åˆ«æ˜¯ï¼Œå°†æ¸©åº¦è®¾ç½®å¾—éå¸¸æ¥è¿‘é›¶ï¼Œä¼šäº§ç”Ÿæœ€æœ‰å¯èƒ½æ˜¯ä¿ç½—Â·æ ¼é›·å„å§†ä¼šè¯´çš„è¯ï¼š</p>
<p>_â€œis that they were all the same thing that was a startup is that they were all the same thing that was a startup is that they were all the same thing that was a startup is that they were all the sameâ€<br>â€œä»–ä»¬éƒ½æ˜¯ä¸€æ ·çš„ï¼Œåˆ›ä¸šå…¬å¸æ˜¯ï¼Œä»–ä»¬éƒ½æ˜¯åˆ›ä¸šå…¬å¸ï¼Œä»–ä»¬éƒ½æ˜¯ä¸€æ ·çš„åˆ›ä¸šå…¬å¸ï¼Œä»–ä»¬éƒ½æ˜¯ä¸€æ ·çš„ï¼Œåˆ›ä¸šå…¬å¸æ˜¯ä¸€æ ·çš„â€_</p>
<p>looks like weâ€™ve reached an infinite loop about startups.<br>çœ‹èµ·æ¥æˆ‘ä»¬å·²ç»è¾¾åˆ°äº†ä¸€ä¸ªå…³äºåˆåˆ›å…¬å¸çš„æ— é™å¾ªç¯ã€‚</p>
<h3 id="shakespeare-èå£«æ¯”äºš"><a href="#Shakespeare-èå£«æ¯”äºš" class="headerlink" title="Shakespeare èå£«æ¯”äºš"></a>Shakespeare èå£«æ¯”äºš</h3><p>It looks like we can learn to spell English words. But how about if there is more structure and style in the data? To examine this I downloaded all the works of Shakespeare and concatenated them into a single (4.4MB) file. We can now afford to train a larger network, in this case lets try a 3-layer RNN with 512 hidden nodes on each layer. After we train the network for a few hours we obtain samples such as:<br>çœ‹èµ·æ¥æˆ‘ä»¬å¯ä»¥å­¦ä¼šæ‹¼å†™è‹±è¯­å•è¯ã€‚ä½†å¦‚æœæ•°æ®ä¸­æœ‰æ›´å¤šçš„ç»“æ„å’Œé£æ ¼å‘¢ï¼Ÿä¸ºæ­¤ï¼Œæˆ‘ä¸‹è½½äº†èå£«æ¯”äºšçš„æ‰€æœ‰ä½œå“ï¼Œå¹¶å°†å®ƒä»¬åˆå¹¶æˆä¸€ä¸ªå•ä¸€çš„ï¼ˆ4.4MBï¼‰æ–‡ä»¶ã€‚æˆ‘ä»¬ç°åœ¨æœ‰èƒ½åŠ›è®­ç»ƒä¸€ä¸ªæ›´å¤§çš„ç½‘ç»œï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè®©æˆ‘ä»¬å°è¯•ä¸€ä¸ª3å±‚çš„RNNï¼Œæ¯å±‚æœ‰512ä¸ªéšè—èŠ‚ç‚¹ã€‚ç»è¿‡å‡ ä¸ªå°æ—¶çš„è®­ç»ƒï¼Œæˆ‘ä»¬å¾—åˆ°å¦‚ä¸‹æ ·æœ¬ï¼š<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">PANDARUS:</span><br><span class="line">Alas, I think he shall be come approached and the day</span><br><span class="line">When little srain would be attain&#x27;d into being never fed,</span><br><span class="line">And who is but a chain and subjects of his death,</span><br><span class="line">I should not sleep.</span><br><span class="line"></span><br><span class="line">Second Senator:</span><br><span class="line">They are away this miseries, produced upon my soul,</span><br><span class="line">Breaking and strongly should be buried, when I perish</span><br><span class="line">The earth and thoughts of many states.</span><br><span class="line"></span><br><span class="line">DUKE VINCENTIO:</span><br><span class="line">Well, your wit is in the care of side and that.</span><br><span class="line"></span><br><span class="line">Second Lord:</span><br><span class="line">They would be ruled after this chamber, and</span><br><span class="line">my fair nues begun out of the fact, to be conveyed,</span><br><span class="line">Whose noble souls I&#x27;ll have the heart of the wars.</span><br><span class="line"></span><br><span class="line">Clown:</span><br><span class="line">Come, sir, I will make did behold your worship.</span><br><span class="line"></span><br><span class="line">VIOLA:</span><br><span class="line">I&#x27;ll drink it.</span><br></pre></td></tr></table></figure></p>
<p>Remember, all the RNN knows are characters, so in particular it samples both speakerâ€™s names and the contents. Sometimes we also get relatively extented monologue passages, such as:<br>è¯·è®°ä½ï¼ŒRNNæ‰€çŸ¥é“çš„åªæœ‰å­—ç¬¦ï¼Œå› æ­¤å®ƒæ—¢é‡‡æ ·è¯´è¯è€…çš„åå­—ï¼Œä¹Ÿé‡‡æ ·å†…å®¹ã€‚æœ‰æ—¶æˆ‘ä»¬è¿˜ä¼šå¾—åˆ°ç›¸å¯¹è¾ƒé•¿çš„ç‹¬ç™½æ®µè½ï¼Œä¾‹å¦‚ï¼š<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">VIOLA:</span><br><span class="line">Why, Salisbury must find his flesh and thought</span><br><span class="line">That which I am not aps, not a man and in fire,</span><br><span class="line">To show the reining of the raven and the wars</span><br><span class="line">To grace my hand reproach within, and not a fair are hand,</span><br><span class="line">That Caesar and my goodly father&#x27;s world;</span><br><span class="line">When I was heaven of presence and our fleets,</span><br><span class="line">We spare with hours, but cut thy council I am great,</span><br><span class="line">Murdered and by thy master&#x27;s ready there</span><br><span class="line">My power to give thee but so much as hell:</span><br><span class="line">Some service in the noble bondman here,</span><br><span class="line">Would show him to her wine.</span><br><span class="line"></span><br><span class="line">KING LEAR:</span><br><span class="line">O, if you were a feeble sight, the courtesy of your law,</span><br><span class="line">Your sight and several breath, will wear the gods</span><br><span class="line">With his heads, and my hands are wonder&#x27;d at the deeds,</span><br><span class="line">So drop upon your lordship&#x27;s head, and your opinion</span><br><span class="line">Shall be against your honour.</span><br></pre></td></tr></table></figure></p>
<p>I can barely recognize these samples from actual Shakespeare :) If you like Shakespeare, you might appreciate this 100,000 character sample. Of course, you can also generate an infinite amount of your own samples at different temperatures with the provided code.<br>æˆ‘å‡ ä¹æ— æ³•è¾¨è®¤è¿™äº›æ ·æœ¬æ˜¯å¦çœŸçš„æ˜¯èå£«æ¯”äºšçš„ä½œå“ :) å¦‚æœä½ å–œæ¬¢èå£«æ¯”äºšï¼Œä½ å¯èƒ½ä¼šå–œæ¬¢è¿™ä¸ª10ä¸‡å­—ç¬¦çš„æ ·æœ¬ã€‚å½“ç„¶ï¼Œä½ ä¹Ÿå¯ä»¥ä½¿ç”¨æä¾›çš„ä»£ç åœ¨ä¸åŒæ¸©åº¦ä¸‹ç”Ÿæˆæ— é™é‡çš„æ ·æœ¬ã€‚</p>
<h3 id="wikipedia-ç»´åŸºç™¾ç§‘"><a href="#Wikipedia-ç»´åŸºç™¾ç§‘" class="headerlink" title="Wikipedia ç»´åŸºç™¾ç§‘"></a>Wikipedia ç»´åŸºç™¾ç§‘</h3><p>We saw that the LSTM can learn to spell words and copy general syntactic structures. Lets further increase the difficulty and train on structured markdown. In particular, lets take the <a target="_blank" rel="noopener" href="http://prize.hutter1.net/">Hutter Prize</a> 100MB dataset of raw Wikipedia and train an LSTM. Following <a target="_blank" rel="noopener" href="http://arxiv.org/abs/1308.0850">Graves et al.</a>, I used the first 96MB for training, the rest for validation and ran a few models overnight. We can now sample Wikipedia articles! Below are a few fun excerpts. First, some basic markdown output:<br>æˆ‘ä»¬å·²ç»çœ‹åˆ°LSTMå¯ä»¥å­¦ä¼šæ‹¼å†™å•è¯å’Œå¤åˆ¶ä¸€èˆ¬çš„å¥æ³•ç»“æ„ã€‚è®©æˆ‘ä»¬è¿›ä¸€æ­¥å¢åŠ éš¾åº¦ï¼Œè®­ç»ƒåœ¨ç»“æ„åŒ–çš„Markdownä¸Šã€‚ç‰¹åˆ«æ˜¯ï¼Œè®©æˆ‘ä»¬ä½¿ç”¨Hutter Prizeçš„100MBåŸå§‹ç»´åŸºç™¾ç§‘æ•°æ®é›†æ¥è®­ç»ƒä¸€ä¸ªLSTMã€‚æŒ‰ç…§Gravesç­‰äººçš„æ–¹æ³•ï¼Œæˆ‘ä½¿ç”¨å‰96MBè¿›è¡Œè®­ç»ƒï¼Œå‰©ä½™çš„ç”¨äºéªŒè¯ï¼Œå¹¶åœ¨ä¸€å¤œä¹‹é—´è¿è¡Œäº†å‡ ä¸ªæ¨¡å‹ã€‚ç°åœ¨æˆ‘ä»¬å¯ä»¥é‡‡æ ·ç”Ÿæˆç»´åŸºç™¾ç§‘æ–‡ç« äº†ï¼ä»¥ä¸‹æ˜¯ä¸€äº›æœ‰è¶£çš„æ‘˜å½•ã€‚é¦–å…ˆï¼Œæ˜¯ä¸€äº›åŸºæœ¬çš„Markdownè¾“å‡ºï¼š<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">Naturalism and decision for the majority of Arab countries&#x27; capitalide was grounded</span><br><span class="line">by the Irish language by [[John Clair]], [[An Imperial Japanese Revolt]], associated </span><br><span class="line">with Guangzham&#x27;s sovereignty. His generals were the powerful ruler of the Portugal </span><br><span class="line">in the [[Protestant Immineners]], which could be said to be directly in Cantonese </span><br><span class="line">Communication, which followed a ceremony and set inspired prison, training. The </span><br><span class="line">emperor travelled back to [[Antioch, Perth, October 25|21]] to note, the Kingdom </span><br><span class="line">of Costa Rica, unsuccessful fashioned the [[Thrales]], [[Cynth&#x27;s Dajoard]], known </span><br><span class="line">in western [[Scotland]], near Italy to the conquest of India with the conflict. </span><br><span class="line">Copyright was the succession of independence in the slop of Syrian influence that </span><br><span class="line">was a famous German movement based on a more popular servicious, non-doctrinal </span><br><span class="line">and sexual power post. Many governments recognize the military housing of the </span><br><span class="line">[[Civil Liberalization and Infantry Resolution 265 National Party in Hungary]], </span><br><span class="line">that is sympathetic to be to the [[Punjab Resolution]]</span><br><span class="line">(PJS)[http://www.humah.yahoo.com/guardian.</span><br><span class="line">cfm/7754800786d17551963s89.htm Official economics Adjoint for the Nazism, Montgomery </span><br><span class="line">was swear to advance to the resources for those Socialism&#x27;s rule, </span><br><span class="line">was starting to signing a major tripad of aid exile.]]</span><br></pre></td></tr></table></figure></p>
<p>In case you were wondering, the yahoo url above doesnâ€™t actually exist, the model just hallucinated it. Also, note that the model learns to open and close the parenthesis correctly. Thereâ€™s also quite a lot of structured markdown that the model learns, for example sometimes it creates headings, lists, etc.:<br>å¦‚æœä½ åœ¨å¥½å¥‡ï¼Œä¸Šè¿°çš„Yahooç½‘å€å®é™…ä¸Šå¹¶ä¸å­˜åœ¨ï¼Œè¿™æ˜¯æ¨¡å‹è‡†æƒ³å‡ºæ¥çš„ã€‚æ­¤å¤–ï¼Œè¯·æ³¨æ„ï¼Œæ¨¡å‹å­¦ä¼šäº†æ­£ç¡®åœ°æ‰“å¼€å’Œå…³é—­æ‹¬å·ã€‚æ¨¡å‹è¿˜å­¦ä¹ äº†å¤§é‡çš„ç»“æ„åŒ–Markdownï¼Œä¾‹å¦‚ï¼Œæœ‰æ—¶å®ƒä¼šåˆ›å»ºæ ‡é¢˜ã€åˆ—è¡¨ç­‰å†…å®¹ï¼š<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">&#123; &#123; cite journal | id=Cerling Nonforest Department|format=Newlymeslated|none &#125; &#125;</span><br><span class="line">&#x27;&#x27;www.e-complete&#x27;&#x27;.</span><br><span class="line"></span><br><span class="line">&#x27;&#x27;&#x27;See also&#x27;&#x27;&#x27;: [[List of ethical consent processing]]</span><br><span class="line"></span><br><span class="line">== See also ==</span><br><span class="line">*[[Iender dome of the ED]]</span><br><span class="line">*[[Anti-autism]]</span><br><span class="line"></span><br><span class="line">===[[Religion|Religion]]===</span><br><span class="line">*[[French Writings]]</span><br><span class="line">*[[Maria]]</span><br><span class="line">*[[Revelation]]</span><br><span class="line">*[[Mount Agamul]]</span><br><span class="line"></span><br><span class="line">== External links==</span><br><span class="line">* [http://www.biblegateway.nih.gov/entrepre/ Website of the World Festival. The labour of India-county defeats at the Ripper of California Road.]</span><br><span class="line"></span><br><span class="line">==External links==</span><br><span class="line">* [http://www.romanology.com/ Constitution of the Netherlands and Hispanic Competition for Bilabial and Commonwealth Industry (Republican Constitution of the Extent of the Netherlands)]</span><br></pre></td></tr></table></figure><br>Sometimes the model snaps into a mode of generating random but valid XML:<br>æœ‰æ—¶ï¼Œæ¨¡å‹ä¼šè¿›å…¥ç”Ÿæˆéšæœºä½†æœ‰æ•ˆçš„ XML çš„æ¨¡å¼ï¼š<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">&lt;page&gt;</span><br><span class="line">  &lt;title&gt;Antichrist&lt;/title&gt;</span><br><span class="line">  &lt;id&gt;865&lt;/id&gt;</span><br><span class="line">  &lt;revision&gt;</span><br><span class="line">    &lt;id&gt;15900676&lt;/id&gt;</span><br><span class="line">    &lt;timestamp&gt;2002-08-03T18:14:12Z&lt;/timestamp&gt;</span><br><span class="line">    &lt;contributor&gt;</span><br><span class="line">      &lt;username&gt;Paris&lt;/username&gt;</span><br><span class="line">      &lt;id&gt;23&lt;/id&gt;</span><br><span class="line">    &lt;/contributor&gt;</span><br><span class="line">    &lt;minor /&gt;</span><br><span class="line">    &lt;comment&gt;Automated conversion&lt;/comment&gt;</span><br><span class="line">    &lt;text xml:space=&quot;preserve&quot;&gt;#REDIRECT [[Christianity]]&lt;/text&gt;</span><br><span class="line">  &lt;/revision&gt;</span><br><span class="line">&lt;/page&gt;</span><br></pre></td></tr></table></figure></p>
<p>The model completely makes up the timestamp, id, and so on. Also, note that it closes the correct tags appropriately and in the correct nested order. Here are <a target="_blank" rel="noopener" href="http://cs.stanford.edu/people/karpathy/char-rnn/wiki.txt">100,000 characters of sampled wikipedia</a> if youâ€™re interested to see more.<br>è¯¥æ¨¡å‹å®Œå…¨ç”±æ—¶é—´æˆ³ã€id ç­‰ç»„æˆã€‚å¦å¤–ï¼Œè¯·æ³¨æ„ï¼Œå®ƒæ­£ç¡®åœ°å…³é—­äº†æ ‡ç­¾ï¼Œå¹¶ä¸”æŒ‰ç…§æ­£ç¡®çš„åµŒå¥—é¡ºåºã€‚è¿™é‡Œæœ‰ 100,000 ä¸ªå­—ç¬¦çš„æ ·æœ¬ç»´åŸºç™¾ç§‘ï¼Œå¦‚æœæ‚¨æœ‰å…´è¶£æŸ¥çœ‹æ›´å¤šã€‚</p>
<h3 id="algebraic-geometry-latex-ä»£æ•°å‡ ä½•latex"><a href="#Algebraic-Geometry-Latex-ä»£æ•°å‡ ä½•ï¼ˆLatexï¼‰" class="headerlink" title="Algebraic Geometry (Latex)  ä»£æ•°å‡ ä½•ï¼ˆLatexï¼‰"></a>Algebraic Geometry (Latex)  ä»£æ•°å‡ ä½•ï¼ˆLatexï¼‰</h3><p>The results above suggest that the model is actually quite good at learning complex syntactic structures. Impressed by these results, my labmate (<a target="_blank" rel="noopener" href="http://cs.stanford.edu/people/jcjohns/">Justin Johnson</a>) and I decided to push even further into structured territories and got a hold of <a target="_blank" rel="noopener" href="http://stacks.math.columbia.edu/">this book</a> on algebraic stacks/geometry. We downloaded the raw Latex source file (a 16MB file) and trained a multilayer LSTM. Amazingly, the resulting sampled Latex _almost_ compiles. We had to step in and fix a few issues manually but then you get plausible looking math, itâ€™s quite astonishing:<br>ä¸Šè¿°ç»“æœè¡¨æ˜ï¼Œæ¨¡å‹åœ¨å­¦ä¹ å¤æ‚å¥æ³•ç»“æ„æ–¹é¢å®é™…ä¸Šç›¸å½“ä¸é”™ã€‚è¿™äº›ç»“æœç»™æˆ‘ç•™ä¸‹äº†æ·±åˆ»çš„å°è±¡ï¼Œæˆ‘çš„å®éªŒå®¤åŒäº‹Justin Johnsonå’Œæˆ‘å†³å®šè¿›ä¸€æ­¥æ¢ç´¢ç»“æ„åŒ–é¢†åŸŸï¼Œå¹¶æ‰¾åˆ°äº†ä¸€æœ¬å…³äºä»£æ•°å /å‡ ä½•çš„ä¹¦ã€‚æˆ‘ä»¬ä¸‹è½½äº†åŸå§‹çš„Latexæºæ–‡ä»¶ï¼ˆä¸€ä¸ª16MBçš„æ–‡ä»¶ï¼‰å¹¶è®­ç»ƒäº†ä¸€ä¸ªå¤šå±‚LSTMã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œç”Ÿæˆçš„Latexå‡ ä¹å¯ä»¥ç¼–è¯‘ã€‚æˆ‘ä»¬ä¸å¾—ä¸æ‰‹åŠ¨ä¿®å¤ä¸€äº›é—®é¢˜ï¼Œä½†æœ€ç»ˆå¾—åˆ°äº†çœ‹èµ·æ¥å¾ˆåˆç†çš„æ•°å­¦è¡¨è¾¾ï¼Œè¿™çœŸæ˜¯ä»¤äººæƒŠå¹ï¼š<br><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="/2472be8a/3.png" class><br>Sampled (fake) algebraic geometry. <a target="_blank" rel="noopener" href="http://cs.stanford.edu/people/jcjohns/fake-math/4.pdf">Hereâ€™s the actual pdf.</a><br>é‡‡æ ·ï¼ˆå‡ï¼‰ä»£æ•°å‡ ä½•ã€‚è¿™æ˜¯å®é™…çš„pdfã€‚</p>
<p>Hereâ€™s another sample: ä¸‹é¢æ˜¯å¦ä¸€ä¸ªç¤ºä¾‹ï¼š<br><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="/2472be8a/4.png" class><br>More hallucinated algebraic geometry. Nice try on the diagram (right).<br>æ›´å¤šå¹»è§‰çš„ä»£æ•°å‡ ä½•ã€‚ä¸é”™çš„å°è¯•å›¾ï¼ˆå³ï¼‰ã€‚</p>
<p>As you can see above, sometimes the model tries to generate latex diagrams, but clearly it hasnâ€™t really figured them out. I also like the part where it chooses to skip a proof (_â€œProof omitted.â€_, top left). Of course, keep in mind that latex has a relatively difficult structured syntactic format that I havenâ€™t even fully mastered myself. For instance, here is a raw sample from the model (unedited):<br>æ­£å¦‚ä½ åœ¨ä¸Šé¢çœ‹åˆ°çš„ï¼Œæœ‰æ—¶æ¨¡å‹å°è¯•ç”ŸæˆLatexå›¾è¡¨ï¼Œä½†æ˜¾ç„¶å®ƒè¿˜æ²¡æœ‰çœŸæ­£æŒæ¡è¿™ç§æŠ€èƒ½ã€‚æˆ‘ä¹Ÿå–œæ¬¢å®ƒé€‰æ‹©è·³è¿‡è¯æ˜çš„éƒ¨åˆ†ï¼ˆâ€œProof omitted.â€ï¼Œå·¦ä¸Šè§’ï¼‰ã€‚å½“ç„¶ï¼Œè¯·è®°ä½ï¼ŒLatexæœ‰ä¸€ä¸ªç›¸å¯¹å¤æ‚çš„ç»“æ„åŒ–å¥æ³•æ ¼å¼ï¼Œæˆ‘è‡ªå·±éƒ½è¿˜æ²¡æœ‰å®Œå…¨æŒæ¡ã€‚ä¾‹å¦‚ï¼Œä¸‹é¢æ˜¯æ¨¡å‹ç”Ÿæˆçš„ä¸€ä¸ªåŸå§‹æ ·æœ¬ï¼ˆæœªç¼–è¾‘ï¼‰ï¼š<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">\begin&#123;proof&#125;</span><br><span class="line">We may assume that $\mathcal&#123;I&#125;$ is an abelian sheaf on $\mathcal&#123;C&#125;$.</span><br><span class="line">\item Given a morphism $\Delta : \mathcal&#123;F&#125; \to \mathcal&#123;I&#125;$</span><br><span class="line">is an injective and let $\mathfrak q$ be an abelian sheaf on $X$.</span><br><span class="line">Let $\mathcal&#123;F&#125;$ be a fibered complex. Let $\mathcal&#123;F&#125;$ be a category.</span><br><span class="line">\begin&#123;enumerate&#125;</span><br><span class="line">\item \hyperref[setain-construction-phantom]&#123;Lemma&#125;</span><br><span class="line">\label&#123;lemma-characterize-quasi-finite&#125;</span><br><span class="line">Let $\mathcal&#123;F&#125;$ be an abelian quasi-coherent sheaf on $\mathcal&#123;C&#125;$.</span><br><span class="line">Let $\mathcal&#123;F&#125;$ be a coherent $\mathcal&#123;O&#125;_X$-module. Then</span><br><span class="line">$\mathcal&#123;F&#125;$ is an abelian catenary over $\mathcal&#123;C&#125;$.</span><br><span class="line">\item The following are equivalent</span><br><span class="line">\begin&#123;enumerate&#125;</span><br><span class="line">\item $\mathcal&#123;F&#125;$ is an $\mathcal&#123;O&#125;_X$-module.</span><br><span class="line">\end&#123;lemma&#125;</span><br></pre></td></tr></table></figure></p>
<p>This sample from a relatively decent model illustrates a few common mistakes. For example, the model opens a <code>\begin&#123;proof&#125;</code> environment but then ends it with a <code>\end&#123;lemma&#125;</code>. This is an example of a problem weâ€™d have to fix manually, and is likely due to the fact that the dependency is too long-term: By the time the model is done with the proof it has forgotten whether it was doing a proof or a lemma. Similarly, it opens an <code>\begin&#123;enumerate&#125;</code> but then forgets to close it. We observed that these became less common with larger/better models, but nonetheless, these are the kinds of mistakes that come up.<br>è¿™ä¸ªæ¥è‡ªç›¸å¯¹ä¸é”™æ¨¡å‹çš„æ ·æœ¬å±•ç¤ºäº†ä¸€äº›å¸¸è§çš„é”™è¯¯ã€‚ä¾‹å¦‚ï¼Œæ¨¡å‹æ‰“å¼€äº†ä¸€ä¸ª \begin{proof} ç¯å¢ƒï¼Œä½†éšåå´ä»¥ \end{lemma} ç»“æŸã€‚è¿™æ˜¯ä¸€ä¸ªæˆ‘ä»¬å¿…é¡»æ‰‹åŠ¨ä¿®å¤çš„é—®é¢˜çš„ä¾‹å­ï¼Œå¯èƒ½æ˜¯ç”±äºä¾èµ–å…³ç³»è¿‡äºé•¿æœŸï¼šåˆ°æ¨¡å‹å®Œæˆè¯æ˜æ—¶ï¼Œå®ƒå·²ç»å¿˜è®°äº†è‡ªå·±æ˜¯åœ¨è¿›è¡Œè¯æ˜è¿˜æ˜¯å¼•ç†ã€‚ç±»ä¼¼åœ°ï¼Œå®ƒæ‰“å¼€äº†ä¸€ä¸ª \begin{enumerate}ï¼Œä½†éšåå¿˜è®°äº†å…³é—­å®ƒã€‚æˆ‘ä»¬æ³¨æ„åˆ°ï¼Œè¿™äº›é”™è¯¯åœ¨è¾ƒå¤§/æ›´å¥½çš„æ¨¡å‹ä¸­å˜å¾—ä¸é‚£ä¹ˆå¸¸è§ï¼Œä½†è¿™äº›éƒ½æ˜¯å¯èƒ½å‡ºç°çš„é”™è¯¯ç±»å‹ã€‚</p>
<h3 id="linux-source-code-linux-æºä»£ç "><a href="#Linux-Source-Code-Linux-æºä»£ç " class="headerlink" title="Linux Source Code Linux æºä»£ç "></a>Linux Source Code Linux æºä»£ç </h3><p>I wanted to push structured data to its limit, so for the final challenge I decided to use code. In particular, I took all the source and header files found in the <a target="_blank" rel="noopener" href="https://github.com/torvalds/linux">Linux repo on Github</a>, concatenated all of them in a single giant file (474MB of C code) (I was originally going to train only on the kernel but that by itself is only ~16MB). Then I trained several as-large-as-fits-on-my-GPU 3-layer LSTMs over a period of a few days. These models have about 10 million parameters, which is still on the lower end for RNN models. The results are superfun:<br>æˆ‘æƒ³å°†ç»“æ„åŒ–æ•°æ®æ¨å‘æé™ï¼Œå› æ­¤åœ¨æœ€åçš„æŒ‘æˆ˜ä¸­æˆ‘å†³å®šä½¿ç”¨ä»£ç ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘æ”¶é›†äº†Githubä¸ŠLinuxä»“åº“ä¸­çš„æ‰€æœ‰æºæ–‡ä»¶å’Œå¤´æ–‡ä»¶ï¼Œå°†å®ƒä»¬åˆå¹¶æˆä¸€ä¸ªå·¨å¤§çš„æ–‡ä»¶ï¼ˆ474MBçš„Cä»£ç ï¼‰ï¼ˆæˆ‘æœ¬æ¥æ‰“ç®—åªè®­ç»ƒå†…æ ¸ä»£ç ï¼Œä½†å®ƒæœ¬èº«åªæœ‰å¤§çº¦16MBï¼‰ã€‚ç„¶åï¼Œæˆ‘åœ¨å‡ å¤©å†…è®­ç»ƒäº†å¤šä¸ªå°½å¯èƒ½é€‚åº”GPUå®¹é‡çš„ä¸‰å±‚LSTMæ¨¡å‹ã€‚è¿™äº›æ¨¡å‹å¤§çº¦æœ‰1000ä¸‡ä¸ªå‚æ•°ï¼Œè¿™åœ¨RNNæ¨¡å‹ä¸­ä»ç„¶ç®—æ˜¯è¾ƒå°‘çš„ã€‚ç»“æœéå¸¸æœ‰è¶£ï¼š<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">/*</span><br><span class="line"> * Increment the size file of the new incorrect UI_FILTER group information</span><br><span class="line"> * of the size generatively.</span><br><span class="line"> */</span><br><span class="line">static int indicate_policy(void)</span><br><span class="line">&#123;</span><br><span class="line">  int error;</span><br><span class="line">  if (fd == MARN_EPT) &#123;</span><br><span class="line">    /*</span><br><span class="line">     * The kernel blank will coeld it to userspace.</span><br><span class="line">     */</span><br><span class="line">    if (ss-&gt;segment &lt; mem_total)</span><br><span class="line">      unblock_graph_and_set_blocked();</span><br><span class="line">    else</span><br><span class="line">      ret = 1;</span><br><span class="line">    goto bail;</span><br><span class="line">  &#125;</span><br><span class="line">  segaddr = in_SB(in.addr);</span><br><span class="line">  selector = seg / 16;</span><br><span class="line">  setup_works = true;</span><br><span class="line">  for (i = 0; i &lt; blocks; i++) &#123;</span><br><span class="line">    seq = buf[i++];</span><br><span class="line">    bpf = bd-&gt;bd.next + i * search;</span><br><span class="line">    if (fd) &#123;</span><br><span class="line">      current = blocked;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  rw-&gt;name = &quot;Getjbbregs&quot;;</span><br><span class="line">  bprm_self_clearl(&amp;iv-&gt;version);</span><br><span class="line">  regs-&gt;new = blocks[(BPF_STATS &lt;&lt; info-&gt;historidac)] | PFMR_CLOBATHINC_SECONDS &lt;&lt; 12;</span><br><span class="line">  return segtable;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>The code looks really quite great overall. Of course, I donâ€™t think it compiles but when you scroll through the generate code it feels very much like a giant C code base. Notice that the RNN peppers its code with comments here and there at random. It is also very good at making very few syntactic errors. For example, it uses strings properly, pointer notation, etc. It also opens and closes brackets {   correctly and learns to indent its code very well. A common error is that it canâ€™t keep track of variable names: It often uses undefined variables (e.g. rw above), declares variables it never uses (e.g. int error), or returns non-existing variables. Lets see a few more examples. Hereâ€™s another snippet that shows a wider array of operations that the RNN learns:</p>
<p>æ€»ä½“æ¥è¯´ï¼Œè¿™äº›ä»£ç çœ‹èµ·æ¥éå¸¸ä¸é”™ã€‚å½“ç„¶ï¼Œæˆ‘ä¸è®¤ä¸ºå®ƒèƒ½ç¼–è¯‘ï¼Œä½†å½“ä½ æ»šåŠ¨æŸ¥çœ‹ç”Ÿæˆçš„ä»£ç æ—¶ï¼Œå®ƒç¡®å®ç»™äººä¸€ç§å·¨å¤§çš„Cä»£ç åº“çš„æ„Ÿè§‰ã€‚æ³¨æ„ï¼ŒRNNä¼šéšæœºåœ¨ä»£ç ä¸­æ’å…¥æ³¨é‡Šï¼Œå¹¶ä¸”åœ¨è¯­æ³•é”™è¯¯æ–¹é¢è¡¨ç°å¾—éå¸¸å‡ºè‰²ã€‚ä¾‹å¦‚ï¼Œå®ƒèƒ½æ­£ç¡®ä½¿ç”¨å­—ç¬¦ä¸²ã€æŒ‡é’ˆç¬¦å·ç­‰ï¼Œè¿˜èƒ½æ­£ç¡®åœ°æ‰“å¼€å’Œå…³é—­å¤§æ‹¬å·{}ï¼Œå¹¶å¾ˆå¥½åœ°å­¦ä¹ ä»£ç ç¼©è¿›ã€‚<br>ä¸€ä¸ªå¸¸è§çš„é”™è¯¯æ˜¯å®ƒæ— æ³•è·Ÿè¸ªå˜é‡åï¼šå®ƒç»å¸¸ä½¿ç”¨æœªå®šä¹‰çš„å˜é‡ï¼ˆä¾‹å¦‚ä¸Šé¢çš„rwï¼‰ï¼Œå£°æ˜ä»æœªä½¿ç”¨çš„å˜é‡ï¼ˆä¾‹å¦‚int errorï¼‰ï¼Œæˆ–è€…è¿”å›ä¸å­˜åœ¨çš„å˜é‡ã€‚è®©æˆ‘ä»¬å†çœ‹ä¸€äº›ä¾‹å­ã€‚ä¸‹é¢æ˜¯å¦ä¸€ä¸ªä»£ç ç‰‡æ®µï¼Œå±•ç¤ºäº†RNNå­¦åˆ°çš„æ›´å¹¿æ³›çš„æ“ä½œï¼š</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">/*</span><br><span class="line"> * If this error is set, we will need anything right after that BSD.</span><br><span class="line"> */</span><br><span class="line">static void action_new_function(struct s_stat_info *wb)</span><br><span class="line">&#123;</span><br><span class="line">  unsigned long flags;</span><br><span class="line">  int lel_idx_bit = e-&gt;edd, *sys &amp; ~((unsigned long) *FIRST_COMPAT);</span><br><span class="line">  buf[0] = 0xFFFFFFFF &amp; (bit &lt;&lt; 4);</span><br><span class="line">  min(inc, slist-&gt;bytes);</span><br><span class="line">  printk(KERN_WARNING &quot;Memory allocated %02x/%02x, &quot;</span><br><span class="line">    &quot;original MLL instead\n&quot;),</span><br><span class="line">    min(min(multi_run - s-&gt;len, max) * num_data_in),</span><br><span class="line">    frame_pos, sz + first_seg);</span><br><span class="line">  div_u64_w(val, inb_p);</span><br><span class="line">  spin_unlock(&amp;disk-&gt;queue_lock);</span><br><span class="line">  mutex_unlock(&amp;s-&gt;sock-&gt;mutex);</span><br><span class="line">  mutex_unlock(&amp;func-&gt;mutex);</span><br><span class="line">  return disassemble(info-&gt;pending_bh);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">static void num_serial_settings(struct tty_struct *tty)</span><br><span class="line">&#123;</span><br><span class="line">  if (tty == tty)</span><br><span class="line">    disable_single_st_p(dev);</span><br><span class="line">  pci_disable_spool(port);</span><br><span class="line">  return 0;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">static void do_command(struct seq_file *m, void *v)</span><br><span class="line">&#123;</span><br><span class="line">  int column = 32 &lt;&lt; (cmd[2] &amp; 0x80);</span><br><span class="line">  if (state)</span><br><span class="line">    cmd = (int)(int_state ^ (in_8(&amp;ch-&gt;ch_flags) &amp; Cmd) ? 2 : 1);</span><br><span class="line">  else</span><br><span class="line">    seq = 1;</span><br><span class="line">  for (i = 0; i &lt; 16; i++) &#123;</span><br><span class="line">    if (k &amp; (1 &lt;&lt; 1))</span><br><span class="line">      pipe = (in_use &amp; UMXTHREAD_UNCCA) +</span><br><span class="line">        ((count &amp; 0x00000000fffffff8) &amp; 0x000000f) &lt;&lt; 8;</span><br><span class="line">    if (count == 0)</span><br><span class="line">      sub(pid, ppc_md.kexec_handle, 0x20000000);</span><br><span class="line">    pipe_set_bytes(i, 0);</span><br><span class="line">  &#125;</span><br><span class="line">  /* Free our user pages pointer to place camera if all dash */</span><br><span class="line">  subsystem_info = &amp;of_changes[PAGE_SIZE];</span><br><span class="line">  rek_controls(offset, idx, &amp;soffset);</span><br><span class="line">  /* Now we want to deliberately put it to device */</span><br><span class="line">  control_check_polarity(&amp;context, val, 0);</span><br><span class="line">  for (i = 0; i &lt; COUNTER; i++)</span><br><span class="line">    seq_puts(s, &quot;policy &quot;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Notice that in the second function the model compares tty == tty, which is vacuously true. On the other hand, at least the variable tty exists in the scope this time! In the last function, notice that the code does not return anything, which happens to be correct since the function signature is void. However, the first two functions were also declared void and did return values. This is again a form of a common mistake due to long-term interactions.<br>æ³¨æ„åœ¨ç¬¬äºŒä¸ªå‡½æ•°ä¸­ï¼Œæ¨¡å‹æ¯”è¾ƒäº†tty == ttyï¼Œè¿™æ˜¯æ˜¾ç„¶ä¸ºçœŸçš„ã€‚å¦ä¸€æ–¹é¢ï¼Œè‡³å°‘è¿™æ¬¡å˜é‡ttyå­˜åœ¨äºä½œç”¨åŸŸä¸­ï¼åœ¨æœ€åä¸€ä¸ªå‡½æ•°ä¸­ï¼Œæ³¨æ„ä»£ç æ²¡æœ‰è¿”å›ä»»ä½•ä¸œè¥¿ï¼Œè¿™æ­£å¥½æ˜¯æ­£ç¡®çš„ï¼Œå› ä¸ºå‡½æ•°ç­¾åæ˜¯voidã€‚ç„¶è€Œï¼Œå‰ä¸¤ä¸ªå‡½æ•°ä¹Ÿå£°æ˜ä¸ºvoidï¼Œä½†å´è¿”å›äº†å€¼ã€‚è¿™å†æ¬¡æ˜¯ä¸€ç§ç”±äºé•¿æ—¶é—´äº¤äº’è€Œå¯¼è‡´çš„å¸¸è§é”™è¯¯å½¢å¼ã€‚</p>
<p>Sometimes the model decides that itâ€™s time to sample a new file. This is usually a very amusing part: The model first recites the GNU license character by character, samples a few includes, generates some macros and then dives into the code:<br>æœ‰æ—¶ï¼Œæ¨¡å‹ä¼šå†³å®šæ˜¯æ—¶å€™é‡‡æ ·ä¸€ä¸ªæ–°æ–‡ä»¶äº†ã€‚è¿™é€šå¸¸æ˜¯éå¸¸æœ‰è¶£çš„éƒ¨åˆ†ï¼šæ¨¡å‹é¦–å…ˆé€å­—ç¬¦åœ°èƒŒè¯µGNUè®¸å¯è¯ï¼Œé‡‡æ ·å‡ ä¸ªincludeè¯­å¥ï¼Œç”Ÿæˆä¸€äº›å®ï¼Œç„¶åæ·±å…¥åˆ°ä»£ç ä¸­ï¼š<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">/*</span><br><span class="line"> *  Copyright (c) 2006-2010, Intel Mobile Communications.  All rights reserved.</span><br><span class="line"> *</span><br><span class="line"> *   This program is free software; you can redistribute it and/or modify it</span><br><span class="line"> * under the terms of the GNU General Public License version 2 as published by</span><br><span class="line"> * the Free Software Foundation.</span><br><span class="line"> *</span><br><span class="line"> *        This program is distributed in the hope that it will be useful,</span><br><span class="line"> * but WITHOUT ANY WARRANTY; without even the implied warranty of</span><br><span class="line"> *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span><br><span class="line"> *</span><br><span class="line"> *  GNU General Public License for more details.</span><br><span class="line"> *</span><br><span class="line"> *   You should have received a copy of the GNU General Public License</span><br><span class="line"> *    along with this program; if not, write to the Free Software Foundation,</span><br><span class="line"> *  Inc., 675 Mass Ave, Cambridge, MA 02139, USA.</span><br><span class="line"> */</span><br><span class="line"></span><br><span class="line">#include &lt;linux/kexec.h&gt;</span><br><span class="line">#include &lt;linux/errno.h&gt;</span><br><span class="line">#include &lt;linux/io.h&gt;</span><br><span class="line">#include &lt;linux/platform_device.h&gt;</span><br><span class="line">#include &lt;linux/multi.h&gt;</span><br><span class="line">#include &lt;linux/ckevent.h&gt;</span><br><span class="line"></span><br><span class="line">#include &lt;asm/io.h&gt;</span><br><span class="line">#include &lt;asm/prom.h&gt;</span><br><span class="line">#include &lt;asm/e820.h&gt;</span><br><span class="line">#include &lt;asm/system_info.h&gt;</span><br><span class="line">#include &lt;asm/setew.h&gt;</span><br><span class="line">#include &lt;asm/pgproto.h&gt;</span><br><span class="line"></span><br><span class="line">#define REG_PG    vesa_slot_addr_pack</span><br><span class="line">#define PFM_NOCOMP  AFSR(0, load)</span><br><span class="line">#define STACK_DDR(type)     (func)</span><br><span class="line"></span><br><span class="line">#define SWAP_ALLOCATE(nr)     (e)</span><br><span class="line">#define emulate_sigs()  arch_get_unaligned_child()</span><br><span class="line">#define access_rw(TST)  asm volatile(&quot;movd %%esp, %0, %3&quot; : : &quot;r&quot; (0));   \</span><br><span class="line">  if (__type &amp; DO_READ)</span><br><span class="line"></span><br><span class="line">static void stat_PC_SEC __read_mostly offsetof(struct seq_argsqueue, \</span><br><span class="line">          pC&gt;[1]);</span><br><span class="line"></span><br><span class="line">static void</span><br><span class="line">os_prefix(unsigned long sys)</span><br><span class="line">&#123;</span><br><span class="line">#ifdef CONFIG_PREEMPT</span><br><span class="line">  PUT_PARAM_RAID(2, sel) = get_state_state();</span><br><span class="line">  set_pid_sum((unsigned long)state, current_state_str(),</span><br><span class="line">           (unsigned long)-1-&gt;lr_full; low;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>There are too many fun parts to cover- I could probably write an entire blog post on just this part. Iâ€™ll cut it short for now, but here is <a target="_blank" rel="noopener" href="http://cs.stanford.edu/people/karpathy/char-rnn/linux.txt">1MB of sampled Linux code</a> for your viewing pleasure.<br>æœ‰å¤ªå¤šæœ‰è¶£çš„éƒ¨åˆ†è¦æ¶µç›– - æˆ‘å¯èƒ½ä¼šå†™ä¸€æ•´ç¯‡å…³äºè¿™éƒ¨åˆ†çš„åšå®¢æ–‡ç« ã€‚æˆ‘ç°åœ¨ä¼šç¼©çŸ­å®ƒï¼Œä½†è¿™é‡Œæœ‰ 1MB çš„ Linux ä»£ç æ ·æœ¬ä¾›æ‚¨æŸ¥çœ‹ã€‚</p>
<h3 id="generating-baby-names-ç”Ÿæˆå©´å„¿åå­—"><a href="#Generating-Baby-Names-ç”Ÿæˆå©´å„¿åå­—" class="headerlink" title="Generating Baby Names ç”Ÿæˆå©´å„¿åå­—"></a>Generating Baby Names ç”Ÿæˆå©´å„¿åå­—</h3><p>Lets try one more for fun. Lets feed the RNN a large text file that contains 8000 baby names listed out, one per line (names obtained from <a target="_blank" rel="noopener" href="http://www.cs.cmu.edu/afs/cs/project/ai-repository/ai/areas/nlp/corpora/names/">here</a>). We can feed this to the RNN and then generate new names! Here are some example names, only showing the ones that do not occur in the training data (90% donâ€™t):<br>å†æ¥è¯•ä¸€ä¸ªæœ‰è¶£çš„å®éªŒã€‚æˆ‘ä»¬å‘RNNè¾“å…¥ä¸€ä¸ªåŒ…å«8000ä¸ªå©´å„¿åå­—çš„å¤§æ–‡æœ¬æ–‡ä»¶ï¼Œæ¯è¡Œä¸€ä¸ªåå­—ï¼ˆåå­—ä»è¿™é‡Œè·å¾—ï¼‰ã€‚æˆ‘ä»¬å¯ä»¥å°†å…¶è¾“å…¥RNNï¼Œç„¶åç”Ÿæˆæ–°çš„åå­—ï¼ä»¥ä¸‹æ˜¯ä¸€äº›ç¤ºä¾‹åå­—ï¼Œåªå±•ç¤ºé‚£äº›æ²¡æœ‰å‡ºç°åœ¨è®­ç»ƒæ•°æ®ä¸­çš„åå­—ï¼ˆ90%éƒ½ä¸åœ¨è®­ç»ƒæ•°æ®ä¸­ï¼‰ï¼š<br>_Rudi Levette Berice Lussa Hany Mareanne Chrestina Carissy Marylen Hammine Janye Marlise Jacacrie Hendred Romand Charienna Nenotto Ette Dorane Wallen Marly Darine Salina Elvyn Ersia Maralena Minoria Ellia Charmin Antley Nerille Chelon Walmor Evena Jeryly Stachon Charisa Allisa Anatha Cathanie Geetra Alexie Jerin Cassen Herbett Cossie Velen Daurenge Robester Shermond Terisa Licia Roselen Ferine Jayn Lusine Charyanne Sales Sanny Resa Wallon Martine Merus Jelen Candica Wallin Tel Rachene Tarine Ozila Ketia Shanne Arnande Karella Roselina Alessia Chasty Deland Berther Geamar Jackein Mellisand Sagdy Nenc Lessie Rasemy Guen Gavi Milea Anneda Margoris Janin Rodelin Zeanna Elyne Janah Ferzina Susta Pey Castina_</p>
<p>You can see many more <a target="_blank" rel="noopener" href="http://cs.stanford.edu/people/karpathy/namesGenUnique.txt">here</a>. Some of my favorites include â€œBabyâ€ (haha), â€œKillieâ€, â€œCharâ€, â€œRâ€, â€œMoreâ€, â€œMarsâ€, â€œHiâ€, â€œSaddieâ€, â€œWithâ€ and â€œAhbortâ€. Well that was fun.ï»¿ Of course, you can imagine this being quite useful inspiration when writing a novel, or naming a new startup :)<br>ä½ å¯ä»¥åœ¨è¿™é‡Œçœ‹åˆ°æ›´å¤šã€‚æˆ‘æœ€å–œæ¬¢çš„ä¸€äº›åŒ…æ‹¬â€œå®è´â€ï¼ˆhahaï¼‰ã€â€œKillieâ€ã€â€œCharâ€ã€â€œRâ€ã€â€œMoreâ€ã€â€œMarsâ€ã€â€œHiâ€ã€â€œSaddieâ€ã€â€œWithâ€å’Œâ€œAhbortâ€ã€‚å—¯ï¼Œè¿™å¾ˆæœ‰è¶£ã€‚å½“ç„¶ï¼Œä½ å¯ä»¥æƒ³è±¡è¿™åœ¨å†™å°è¯´æˆ–å‘½åæ–°çš„åˆ›ä¸šå…¬å¸æ—¶æ˜¯éå¸¸æœ‰ç”¨çš„çµæ„Ÿ:)</p>
<h1 id="understanding-whats-going-on"><a href="#Understanding-whatâ€™s-going-on" class="headerlink" title="Understanding whatâ€™s going on"></a>Understanding whatâ€™s going on</h1><p>We saw that the results at the end of training can be impressive, but how does any of this work? Lets run two quick experiments to briefly peek under the hood.<br>æˆ‘ä»¬çœ‹åˆ°è®­ç»ƒç»“æŸæ—¶çš„ç»“æœä»¤äººå°è±¡æ·±åˆ»ï¼Œä½†è¿™äº›æ˜¯å¦‚ä½•å®ç°çš„å‘¢ï¼Ÿè®©æˆ‘ä»¬è¿›è¡Œä¸¤ä¸ªå¿«é€Ÿå®éªŒï¼Œç®€è¦æ¢ç©¶ä¸€ä¸‹å…¶å†…éƒ¨å·¥ä½œåŸç†ã€‚</p>
<h2 id="the-evolution-of-samples-while-training"><a href="#The-evolution-of-samples-while-training" class="headerlink" title="The evolution of samples while training"></a>The evolution of samples while training</h2><p>è®­ç»ƒæ—¶æ ·æœ¬çš„æ¼”å˜<br>First, itâ€™s fun to look at how the sampled text evolves while the model trains. For example, I trained an LSTM of Leo Tolstoyâ€™s War and Peace and then generated samples every 100 iterations of training. At iteration 100 the model samples random jumbles:<br>é¦–å…ˆï¼Œè§‚å¯Ÿæ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­ç”Ÿæˆæ–‡æœ¬çš„æ¼”å˜æ˜¯å¾ˆæœ‰è¶£çš„ã€‚ä¾‹å¦‚ï¼Œæˆ‘è®­ç»ƒäº†ä¸€ä¸ªåŸºäºåˆ—å¤«Â·æ‰˜å°”æ–¯æ³°çš„ã€Šæˆ˜äº‰ä¸å’Œå¹³ã€‹çš„LSTMï¼Œå¹¶åœ¨æ¯100æ¬¡è¿­ä»£è®­ç»ƒåç”Ÿæˆæ ·æœ¬ã€‚åœ¨ç¬¬100æ¬¡è¿­ä»£æ—¶ï¼Œæ¨¡å‹ç”Ÿæˆçš„æ ·æœ¬æ˜¯éšæœºæ··æ‚çš„å­—ç¬¦ï¼š</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tyntd-iafhatawiaoihrdemot  lytdws  e ,tfti, astai f ogoh eoase rrranbyne &#x27;nhthnee e </span><br><span class="line">plia tklrgd t o idoe ns,smtt   h ne etie h,hregtrs nigtike,aoaenns lng</span><br></pre></td></tr></table></figure>
<p>However, notice that at least it is starting to get an idea about words separated by spaces. Except sometimes it inserts two spaces. It also doesnâ€™t know that comma is amost always followed by a space. At 300 iterations we see that the model starts to get an idea about quotes and periods:<br>ç„¶è€Œï¼Œè¯·æ³¨æ„è‡³å°‘æ¨¡å‹å¼€å§‹æœ‰äº†å…³äºå•è¯ç”±ç©ºæ ¼åˆ†éš”çš„æ¦‚å¿µã€‚å°½ç®¡æœ‰æ—¶å®ƒä¼šæ’å…¥ä¸¤ä¸ªç©ºæ ¼ã€‚å®ƒè¿˜ä¸çŸ¥é“é€—å·åå‡ ä¹æ€»æ˜¯è·Ÿç€ä¸€ä¸ªç©ºæ ¼ã€‚åœ¨è®­ç»ƒ300æ¬¡è¿­ä»£æ—¶ï¼Œæˆ‘ä»¬çœ‹åˆ°æ¨¡å‹å¼€å§‹äº†è§£å¼•å·å’Œå¥å·çš„ç”¨æ³•ï¼š<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&quot;Tmont thithey&quot; fomesscerliund</span><br><span class="line">Keushey. Thom here</span><br><span class="line">sheulke, anmerenith ol sivh I lalterthend Bleipile shuwy fil on aseterlome</span><br><span class="line">coaniogennc Phe lism thond hon at. MeiDimorotion in ther thize.&quot;</span><br></pre></td></tr></table></figure><br>The words are now also separated with spaces and the model starts to get the idea about periods at the end of a sentence. At iteration 500:<br>ç°åœ¨ï¼Œå•è¯ä¹‹é—´ä¹Ÿç”¨ç©ºæ ¼åˆ†éš”ï¼Œæ¨¡å‹å¼€å§‹ç†è§£å¥æœ«çš„å¥å·ã€‚åœ¨ç¬¬500æ¬¡è¿­ä»£æ—¶ï¼š<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">we counter. He stutn co des. His stanted out one ofler that concossions and was </span><br><span class="line">to gearang reay Jotrets and with fre colt otf paitt thin wall. Which das stimn </span><br></pre></td></tr></table></figure><br>the model has now learned to spell the shortest and most common words such as â€œweâ€, â€œHeâ€, â€œHisâ€, â€œWhichâ€, â€œandâ€, etc. At iteration 700 weâ€™re starting to see more and more English-like text emerge:<br>æ¨¡å‹ç°åœ¨å·²ç»å­¦ä¼šäº†æ‹¼å†™æœ€çŸ­å’Œæœ€å¸¸è§çš„å•è¯ï¼Œæ¯”å¦‚â€œweâ€ã€â€œHeâ€ã€â€œHisâ€ã€â€œWhichâ€ã€â€œandâ€ç­‰ã€‚åœ¨ç¬¬700æ¬¡è¿­ä»£æ—¶ï¼Œæˆ‘ä»¬å¼€å§‹çœ‹åˆ°è¶Šæ¥è¶Šå¤šç±»ä¼¼è‹±è¯­çš„æ–‡æœ¬å‡ºç°ï¼š<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Aftair fall unsuch that the hall for Prince Velzonski&#x27;s that me of</span><br><span class="line">her hearly, and behs to so arwage fiving were to it beloge, pavu say falling misfort </span><br><span class="line">how, and Gogition is so overelical and ofter.</span><br></pre></td></tr></table></figure></p>
<p>At iteration 1200 weâ€™re now seeing use of quotations and question/exclamation marks. Longer words have now been learned as well:<br>åœ¨ç¬¬1200æ¬¡è¿­ä»£æ—¶ï¼Œæˆ‘ä»¬ç°åœ¨çœ‹åˆ°äº†å¼•å·å’Œé—®å·/æ„Ÿå¹å·çš„ä½¿ç”¨ã€‚æ›´é•¿çš„å•è¯ä¹Ÿå·²ç»è¢«å­¦ä¼šäº†ï¼š<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&quot;Kite vouch!&quot; he repeated by her</span><br><span class="line">door. &quot;But I would be done and quarts, feeling, then, son is people....&quot;</span><br></pre></td></tr></table></figure><br>Until at last we start to get properly spelled words, quotations, names, and so on by about iteration 2000:<br>ç›´åˆ°æœ€åï¼Œåœ¨å¤§çº¦ç¬¬2000æ¬¡è¿­ä»£æ—¶ï¼Œæˆ‘ä»¬å¼€å§‹å¾—åˆ°æ­£ç¡®æ‹¼å†™çš„å•è¯ã€å¼•å·ã€åå­—ç­‰ï¼š<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&quot;Why do what that day,&quot; replied Natasha, and wishing to himself the fact the</span><br><span class="line">princess, Princess Mary was easier, fed in had oftened him.</span><br><span class="line">Pierre aking his soul came to the packs and drove up his father-in-law women.</span><br></pre></td></tr></table></figure></p>
<p>The picture that emerges is that the model first discovers the general word-space structure and then rapidly starts to learn the words; First starting with the short words and then eventually the longer ones. Topics and themes that span multiple words (and in general longer-term dependencies) start to emerge only much later.<br>å‡ºç°çš„æƒ…å†µæ˜¯ï¼Œæ¨¡å‹é¦–å…ˆå‘ç°äº†æ•´ä½“çš„å•è¯-ç©ºæ ¼ç»“æ„ï¼Œç„¶åè¿…é€Ÿå¼€å§‹å­¦ä¹ å•è¯ï¼›å…ˆæ˜¯çŸ­å•è¯ï¼Œç„¶åé€æ¸å­¦ä¹ é•¿å•è¯ã€‚è·¨è¶Šå¤šä¸ªå•è¯çš„ä¸»é¢˜å’Œä¸»é¢˜ï¼ˆä»¥åŠä¸€èˆ¬çš„é•¿æœŸä¾èµ–å…³ç³»ï¼‰è¦åˆ°å¾ˆä¹…ä»¥åæ‰ä¼šå¼€å§‹å‡ºç°ã€‚</p>
<h2 id="visualizing-the-predictions-and-the-neuron-firings-in-the-rnn"><a href="#Visualizing-the-predictions-and-the-â€œneuronâ€-firings-in-the-RNN" class="headerlink" title="Visualizing the predictions and the â€œneuronâ€ firings in the RNN"></a>Visualizing the predictions and the â€œneuronâ€ firings in the RNN</h2><p>å¯è§†åŒ– RNN ä¸­çš„é¢„æµ‹å’Œâ€œç¥ç»å…ƒâ€æ”¾ç”µ</p>
<p>Another fun visualization is to look at the predicted distributions over characters. In the visualizations below we feed a Wikipedia RNN model character data from the validation set (shown along the blue/green rows) and under every character we visualize (in red) the top 5 guesses that the model assigns for the next character. The guesses are colored by their probability (so dark red = judged as very likely, white = not very likely). For example, notice that there are stretches of characters where the model is extremely confident about the next letter (e.g., the model is very confident about characters during the _<a target="_blank" rel="noopener" href="http://www._">http://www._</a> sequence).<br>å¦ä¸€ä¸ªæœ‰è¶£çš„å¯è§†åŒ–æ˜¯æŸ¥çœ‹æ¨¡å‹å¯¹å­—ç¬¦çš„é¢„æµ‹åˆ†å¸ƒã€‚åœ¨ä¸‹é¢çš„å¯è§†åŒ–ä¸­ï¼Œæˆ‘ä»¬å‘ä¸€ä¸ªè®­ç»ƒå¥½çš„Wikipedia RNNæ¨¡å‹ï¼Œæä¾›éªŒè¯é›†çš„å­—ç¬¦æ•°æ®ï¼ˆæ˜¾ç¤ºåœ¨è“è‰²/ç»¿è‰²è¡Œä¸Šï¼‰ï¼Œå¹¶åœ¨æ¯ä¸ªå­—ç¬¦ä¸‹æ–¹å¯è§†åŒ–ï¼ˆç”¨çº¢è‰²è¡¨ç¤ºï¼‰æ¨¡å‹å¯¹ä¸‹ä¸€ä¸ªå­—ç¬¦çš„å‰5ä¸ªçŒœæµ‹ã€‚çŒœæµ‹æ ¹æ®å…¶æ¦‚ç‡è¿›è¡Œç€è‰²ï¼ˆæ·±çº¢è‰²=è¢«è®¤ä¸ºéå¸¸å¯èƒ½ï¼Œç™½è‰²=ä¸å¤ªå¯èƒ½ï¼‰ã€‚ä¾‹å¦‚ï¼Œè¯·æ³¨æ„åœ¨æŸäº›å­—ç¬¦åºåˆ—ä¸­ï¼Œæ¨¡å‹å¯¹ä¸‹ä¸€ä¸ªå­—ç¬¦æä¸ºè‡ªä¿¡(ä¾‹å¦‚ï¼Œæ¨¡å‹å¯¹ <a target="_blank" rel="noopener" href="http://www">http://www</a> åºåˆ—ä¸­çš„å­—ç¬¦éå¸¸æœ‰ä¿¡å¿ƒï¼‰</p>
<p>The input character sequence (blue/green) is colored based on the _firing_ of a randomly chosen neuron in the hidden representation of the RNN. Think about it as green = very excited and blue = not very excited (for those familiar with details of LSTMs, these are values between [-1,1] in the hidden state vector, which is just the gated and tanhâ€™d LSTM cell state). Intuitively, this is visualizing the firing rate of some neuron in the â€œbrainâ€ of the RNN while it reads the input sequence. Different neurons might be looking for different patterns; Below weâ€™ll look at 4 different ones that I found and thought were interesting or interpretable (many also arenâ€™t):<br>è¾“å…¥å­—ç¬¦åºåˆ—ï¼ˆè“è‰²/ç»¿è‰²ï¼‰æ˜¯æ ¹æ®RNNéšè—è¡¨ç¤ºä¸­éšæœºé€‰æ‹©çš„ä¸€ä¸ªç¥ç»å…ƒçš„æ¿€æ´»æƒ…å†µè¿›è¡Œç€è‰²çš„ã€‚å¯ä»¥ç†è§£ä¸ºç»¿è‰²=éå¸¸æ¿€åŠ¨ï¼Œè“è‰²=ä¸å¤ªæ¿€åŠ¨ï¼ˆå¯¹äºç†Ÿæ‚‰LSTMç»†èŠ‚çš„äººæ¥è¯´ï¼Œè¿™äº›å€¼åœ¨éšè—çŠ¶æ€å‘é‡ä¸­ä»‹äº[-1,1]ä¹‹é—´ï¼Œè¿™æ˜¯ç»è¿‡é—¨æ§å’Œtanhå‡½æ•°å¤„ç†çš„LSTMå•å…ƒçŠ¶æ€ï¼‰ã€‚ç›´è§‚åœ°è¯´ï¼Œè¿™æ˜¯åœ¨å¯è§†åŒ–RNNâ€œè„‘ä¸­â€æŸä¸ªç¥ç»å…ƒåœ¨è¯»å–è¾“å…¥åºåˆ—æ—¶çš„æ¿€æ´»ç‡ã€‚ä¸åŒçš„ç¥ç»å…ƒå¯èƒ½åœ¨å¯»æ‰¾ä¸åŒçš„æ¨¡å¼ï¼›ä¸‹é¢æˆ‘ä»¬å°†æŸ¥çœ‹4ä¸ªæˆ‘å‘ç°æœ‰è¶£æˆ–å¯è§£é‡Šçš„ç¥ç»å…ƒï¼ˆä¹Ÿæœ‰å¾ˆå¤šæ˜¯æ— æ³•è§£é‡Šçš„ï¼‰ï¼š</p>
<img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="/2472be8a/5.png" class>
<p>The neuron highlighted in this image seems to get very excited about URLs and turns off outside of the URLs. The LSTM is likely using this neuron to remember if it is inside a URL or not.<br>æ­¤å›¾åƒä¸­çªå‡ºæ˜¾ç¤ºçš„ç¥ç»å…ƒä¼¼ä¹å¯¹ URL éå¸¸å…´å¥‹ï¼Œå¹¶åœ¨ URL ä¹‹å¤–å…³é—­ã€‚LSTM å¯èƒ½ä½¿ç”¨è¿™ä¸ªç¥ç»å…ƒæ¥è®°ä½å®ƒæ˜¯å¦åœ¨ URL å†…ã€‚</p>
<img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="/2472be8a/6.png" class>
<p>The highlighted neuron here gets very excited when the RNN is inside the [[ ]] markdown environment and turns off outside of it. Interestingly, the neuron canâ€™t turn on right after it sees the character â€œ[â€œ, it must wait for the second â€œ[â€œ and then activate. This task of counting whether the model has seen one or two â€œ[â€œ is likely done with a different neuron.<br>å½“ RNN ä½äº [[ ]] markdown ç¯å¢ƒå†…éƒ¨å¹¶åœ¨å…¶å¤–éƒ¨å…³é—­æ—¶ï¼Œæ­¤å¤„çªå‡ºæ˜¾ç¤ºçš„ç¥ç»å…ƒä¼šéå¸¸å…´å¥‹ã€‚æœ‰è¶£çš„æ˜¯ï¼Œç¥ç»å…ƒåœ¨çœ‹åˆ°å­—ç¬¦â€œ[â€åæ— æ³•ç«‹å³æ‰“å¼€ï¼Œå®ƒå¿…é¡»ç­‰å¾…ç¬¬äºŒä¸ªâ€œ[â€ç„¶åæ¿€æ´»ã€‚è®¡ç®—æ¨¡å‹æ˜¯å¦çœ‹åˆ°ä¸€ä¸ªæˆ–ä¸¤ä¸ªâ€œ[â€çš„ä»»åŠ¡å¯èƒ½æ˜¯ç”¨ä¸åŒçš„ç¥ç»å…ƒå®Œæˆçš„ã€‚</p>
<img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="/2472be8a/7.png" class>
<p>Here we see a neuron that varies seemingly linearly across the [[ ]] environment. In other words its activation is giving the RNN a time-aligned coordinate system across the [[ ]] scope. The RNN can use this information to make different characters more or less likely depending on how early/late it is in the [[ ]] scope (perhaps?).<br>åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬çœ‹åˆ°ä¸€ä¸ªç¥ç»å…ƒï¼Œå®ƒåœ¨[[ ]]ç¯å¢ƒä¸­ä¼¼ä¹å‘ˆçº¿æ€§å˜åŒ–ã€‚æ¢å¥è¯è¯´ï¼Œå®ƒçš„æ¿€æ´»ä¸º RNN æä¾›äº†ä¸€ä¸ªè·¨ [[ ]] èŒƒå›´çš„æ—¶é—´å¯¹é½åæ ‡ç³»ã€‚RNN å¯ä»¥ä½¿ç”¨æ­¤ä¿¡æ¯æˆ–å¤šæˆ–å°‘åœ°ä½¿ä¸åŒçš„å­—ç¬¦æ›´æœ‰å¯èƒ½ï¼Œå…·ä½“å–å†³äºå®ƒåœ¨ [[ ]] èŒƒå›´å†…çš„æ—©/æ™šï¼ˆä¹Ÿè®¸ï¼Ÿ</p>
<img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="/2472be8a/8.png" class>
<p>Here is another neuron that has very local behavior: it is relatively silent but sharply turns off right after the first â€œwâ€ in the â€œwwwâ€ sequence. The RNN might be using this neuron to count up how far in the â€œwwwâ€ sequence it is, so that it can know whether it should emit another â€œwâ€, or if it should start the URL.<br>è¿™æ˜¯å¦ä¸€ä¸ªå…·æœ‰éå¸¸å±€éƒ¨è¡Œä¸ºçš„ç¥ç»å…ƒï¼šå®ƒç›¸å¯¹å®‰é™ï¼Œä½†åœ¨â€œwwwâ€åºåˆ—ä¸­çš„ç¬¬ä¸€ä¸ªâ€œwâ€ä¹‹åæ€¥å‰§å…³é—­ã€‚RNN å¯èƒ½æ­£åœ¨ä½¿ç”¨è¿™ä¸ªç¥ç»å…ƒæ¥è®¡ç®—å®ƒåœ¨â€œwwwâ€åºåˆ—ä¸­çš„è·ç¦»ï¼Œä»¥ä¾¿å®ƒå¯ä»¥çŸ¥é“å®ƒæ˜¯å¦åº”è¯¥å‘å‡ºå¦ä¸€ä¸ªâ€œwâ€ï¼Œæˆ–è€…å®ƒæ˜¯å¦åº”è¯¥å¯åŠ¨ URLã€‚</p>
<p>Of course, a lot of these conclusions are slightly hand-wavy as the hidden state of the RNN is a huge, high-dimensional and largely distributed representation. These visualizations were produced with custom HTML/CSS/Javascript, you can see a sketch of whatâ€™s involved <a target="_blank" rel="noopener" href="http://cs.stanford.edu/people/karpathy/viscode.zip">here</a> if youâ€™d like to create something similar.<br>å½“ç„¶ï¼Œå¾ˆå¤šè¿™äº›ç»“è®ºéƒ½æœ‰äº›ç¬¼ç»Ÿï¼Œå› ä¸ºRNNçš„éšè—çŠ¶æ€æ˜¯ä¸€ä¸ªå·¨å¤§çš„ã€é«˜ç»´çš„ã€å¹¿æ³›åˆ†å¸ƒçš„è¡¨ç¤ºã€‚è¿™äº›å¯è§†åŒ–æ˜¯ä½¿ç”¨è‡ªå®šä¹‰çš„HTML/CSS/Javascriptç”Ÿæˆçš„ï¼Œå¦‚æœä½ æƒ³åˆ›å»ºç±»ä¼¼çš„ä¸œè¥¿ï¼Œå¯ä»¥åœ¨è¿™é‡Œçœ‹åˆ°æ¶‰åŠçš„å†…å®¹ç¤ºä¾‹ã€‚</p>
<p>We can also condense this visualization by excluding the most likely predictions and only visualize the text, colored by activations of a cell. We can see that in addition to a large portion of cells that do not do anything interpretible, about 5% of them turn out to have learned quite interesting and interpretible algorithms:<br>æˆ‘ä»¬è¿˜å¯ä»¥é€šè¿‡æ’é™¤æœ€å¯èƒ½çš„é¢„æµ‹å¹¶ä»…æ ¹æ®å•å…ƒæ¿€æ´»æƒ…å†µå¯¹æ–‡æœ¬è¿›è¡Œç€è‰²æ¥ç®€åŒ–è¿™ç§å¯è§†åŒ–ã€‚æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œé™¤äº†å¤§éƒ¨åˆ†ä¸å¯è§£é‡Šçš„å•å…ƒå¤–ï¼Œå¤§çº¦æœ‰5%çš„å•å…ƒå­¦ä¼šäº†ç›¸å½“æœ‰è¶£ä¸”å¯è§£é‡Šçš„ç®—æ³•ï¼š<br><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="/2472be8a/9.png" class><br><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="/2472be8a/10.png" class><br>Again, what is beautiful about this is that we didnâ€™t have to hardcode at any point that if youâ€™re trying to predict the next character it might, for example, be useful to keep track of whether or not you are currently inside or outside of quote. We just trained the LSTM on raw data and it decided that this is a useful quantitity to keep track of. In other words one of its cells gradually tuned itself during training to become a quote detection cell, since this helps it better perform the final task. This is one of the cleanest and most compelling examples of where the power in Deep Learning models (and more generally end-to-end training) is coming from.<br>å†æ¬¡å¼ºè°ƒï¼Œè¿™å…¶ä¸­çš„ç¾å¦™ä¹‹å¤„åœ¨äºï¼Œæˆ‘ä»¬ä¸éœ€è¦åœ¨ä»»ä½•æ—¶å€™ç¡¬ç¼–ç ï¼Œä¾‹å¦‚åœ¨é¢„æµ‹ä¸‹ä¸€ä¸ªå­—ç¬¦æ—¶éœ€è¦è·Ÿè¸ªå½“å‰æ˜¯å¦åœ¨å¼•å·å†…æˆ–å¼•å·å¤–ã€‚æˆ‘ä»¬åªæ˜¯å¯¹LSTMè¿›è¡ŒåŸå§‹æ•°æ®çš„è®­ç»ƒï¼Œå®ƒè‡ªå·±å†³å®šè·Ÿè¸ªè¿™ä¸ªä¿¡æ¯æ˜¯æœ‰ç”¨çš„ã€‚æ¢å¥è¯è¯´ï¼Œå…¶ä¸­ä¸€ä¸ªå•å…ƒåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é€æ¸è°ƒæ•´è‡ªå·±ï¼Œå˜æˆäº†ä¸€ä¸ªå¼•å·æ£€æµ‹å•å…ƒï¼Œå› ä¸ºè¿™æœ‰åŠ©äºå®ƒæ›´å¥½åœ°å®Œæˆæœ€ç»ˆä»»åŠ¡ã€‚è¿™æ˜¯æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼ˆæ›´å¹¿æ³›åœ°è¯´ï¼Œç«¯åˆ°ç«¯è®­ç»ƒï¼‰åŠ›é‡çš„æœ€æ¸…æ™°å’Œæœ€æœ‰è¯´æœåŠ›çš„ä¾‹å­ä¹‹ä¸€ã€‚</p>
<h1 id="source-code-æºä»£ç "><a href="#Source-Code-æºä»£ç " class="headerlink" title="Source Code æºä»£ç "></a>Source Code æºä»£ç </h1><p>I hope Iâ€™ve convinced you that training character-level language models is a very fun exercise. You can train your own models using the <a target="_blank" rel="noopener" href="https://github.com/karpathy/char-rnn">char-rnn code</a> I released on Github (under MIT license). It takes one large text file and trains a character-level model that you can then sample from. Also, it helps if you have a GPU or otherwise training on CPU will be about a factor of 10x slower. In any case, if you end up training on some data and getting fun results let me know! And if you get lost in the Torch/Lua codebase remember that all it is is just a more fancy version of this <a target="_blank" rel="noopener" href="https://gist.github.com/karpathy/d4dee566867f8291f086">100-line gist</a>.<br>æˆ‘å¸Œæœ›æˆ‘å·²ç»è®©ä½ ç›¸ä¿¡ï¼Œè®­ç»ƒå­—ç¬¦çº§è¯­è¨€æ¨¡å‹æ˜¯ä¸€ä¸ªéå¸¸æœ‰è¶£çš„ç»ƒä¹ ã€‚ä½ å¯ä»¥ä½¿ç”¨æˆ‘åœ¨Githubä¸Šå‘å¸ƒçš„char-rnnä»£ç ï¼ˆåŸºäºMITè®¸å¯è¯ï¼‰æ¥è®­ç»ƒä½ è‡ªå·±çš„æ¨¡å‹ã€‚å®ƒéœ€è¦ä¸€ä¸ªå¤§å‹æ–‡æœ¬æ–‡ä»¶ï¼Œå¹¶è®­ç»ƒä¸€ä¸ªå­—ç¬¦çº§æ¨¡å‹ï¼Œä½ å¯ä»¥ä»ä¸­è¿›è¡Œé‡‡æ ·ã€‚æ­¤å¤–ï¼Œå¦‚æœä½ æœ‰GPUï¼Œè¿™ä¼šæ›´æœ‰å¸®åŠ©ï¼Œå¦åˆ™åœ¨CPUä¸Šè®­ç»ƒçš„é€Ÿåº¦å¤§çº¦ä¼šæ…¢10å€ã€‚ä¸ç®¡æ€æ ·ï¼Œå¦‚æœä½ åœ¨ä¸€äº›æ•°æ®ä¸Šè®­ç»ƒå¹¶å¾—åˆ°äº†æœ‰è¶£çš„ç»“æœï¼Œè¯·å‘Šè¯‰æˆ‘ï¼å¦‚æœä½ åœ¨Torch/Luaä»£ç åº“ä¸­è¿·å¤±äº†æ–¹å‘ï¼Œè¯·è®°ä½ï¼Œè¿™åªæ˜¯è¿™ä¸ª100è¡Œä»£ç ç¤ºä¾‹çš„æ›´å¤æ‚ç‰ˆæœ¬ã€‚</p>
<p>_Brief digression._ The code is written in <a target="_blank" rel="noopener" href="http://torch.ch/">Torch 7</a>, which has recently become my favorite deep learning framework. Iâ€™ve only started working with Torch/LUA over the last few months and it hasnâ€™t been easy (I spent a good amount of time digging through the raw Torch code on Github and asking questions on their _gitter_ to get things done), but once you get a hang of things it offers a lot of flexibility and speed. Iâ€™ve also worked with Caffe and Theano in the past and I believe Torch, while not perfect, gets its levels of abstraction and philosophy right better than others. In my view the desirable features of an effective framework are:<br>ç®€å•æ’æ›²ä¸€ä¸‹ã€‚è¿™æ®µä»£ç æ˜¯ç”¨Torch 7ç¼–å†™çš„ï¼Œå®ƒæœ€è¿‘æˆä¸ºäº†æˆ‘æœ€å–œæ¬¢çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ã€‚æˆ‘åªæ˜¯è¿‡å»å‡ ä¸ªæœˆæ‰å¼€å§‹ä½¿ç”¨Torch/LUAï¼Œè¿‡ç¨‹å¹¶ä¸å®¹æ˜“ï¼ˆæˆ‘èŠ±äº†å¤§é‡æ—¶é—´åœ¨Githubä¸ŠæŒ–æ˜Torchçš„æºä»£ç ï¼Œå¹¶åœ¨ä»–ä»¬çš„gitterä¸Šæé—®ä»¥è§£å†³é—®é¢˜ï¼‰ï¼Œä½†ä¸€æ—¦ä½ æŒæ¡äº†å®ƒï¼Œå®ƒå°±èƒ½æä¾›å¾ˆå¤§çš„çµæ´»æ€§å’Œé€Ÿåº¦ã€‚æˆ‘è¿‡å»ä¹Ÿä½¿ç”¨è¿‡Caffeå’ŒTheanoï¼Œæˆ‘è®¤ä¸ºTorchè™½ç„¶ä¸å®Œç¾ï¼Œä½†å®ƒåœ¨æŠ½è±¡å±‚æ¬¡å’Œç†å¿µä¸Šåšå¾—æ¯”å…¶ä»–æ¡†æ¶æ›´å¥½ã€‚åœ¨æˆ‘çœ‹æ¥ï¼Œä¸€ä¸ªæœ‰æ•ˆæ¡†æ¶çš„ç†æƒ³ç‰¹æ€§æ˜¯ï¼š</p>
<ol>
<li>CPU/GPU transparent Tensor library with a lot of functionality (slicing, array/matrix operations, etc. )<br><strong>CPU/GPUé€æ˜çš„å¼ é‡åº“</strong>ï¼šå…·æœ‰ä¸°å¯Œçš„åŠŸèƒ½ï¼ˆåˆ‡ç‰‡ã€æ•°ç»„/çŸ©é˜µæ“ä½œç­‰ï¼‰ã€‚ï¼‰</li>
<li>An entirely separate code base in a scripting language (ideally Python) that operates over Tensors and implements all Deep Learning stuff (forward/backward, computation graphs, etc)<br><strong>å®Œå…¨ç‹¬ç«‹çš„è„šæœ¬è¯­è¨€ä»£ç åº“</strong>ï¼šç†æƒ³æƒ…å†µä¸‹æ˜¯Pythonï¼Œæ“ä½œå¼ é‡å¹¶å®ç°æ‰€æœ‰æ·±åº¦å­¦ä¹ ç›¸å…³åŠŸèƒ½ï¼ˆå‰å‘/åå‘ä¼ æ’­ã€è®¡ç®—å›¾ç­‰ï¼‰ã€‚</li>
<li>It should be possible to easily share pretrained models (Caffe does this well, others donâ€™t), and crucially<br><strong>èƒ½å¤Ÿè½»æ¾å…±äº«é¢„è®­ç»ƒæ¨¡å‹</strong>ï¼šCaffeåœ¨è¿™æ–¹é¢åšå¾—å¾ˆå¥½ï¼Œå…¶ä»–æ¡†æ¶åˆ™ä¸å°½å¦‚äººæ„ã€‚</li>
<li>NO compilation step (or at least not as currently done in Theano). The trend in Deep Learning is towards larger, more complex networks that are are time-unrolled in complex graphs. It is critical that these do not compile for a long time or development time greatly suffers. Second, by compiling one gives up interpretability and the ability to log/debug effectively. If there is an _option_ to compile the graph once it has been developed for efficiency in prod thatâ€™s fine.<br><strong>æ²¡æœ‰ç¼–è¯‘æ­¥éª¤</strong>ï¼šæˆ–è‡³å°‘ä¸åƒTheanoç›®å‰é‚£æ ·ã€‚æ·±åº¦å­¦ä¹ çš„å‘å±•è¶‹åŠ¿æ˜¯ä½¿ç”¨æ›´å¤§ã€æ›´å¤æ‚çš„ç½‘ç»œï¼Œè¿™äº›ç½‘ç»œåœ¨å¤æ‚çš„è®¡ç®—å›¾ä¸­è¿›è¡Œæ—¶é—´å±•å¼€ã€‚å…³é”®æ˜¯è¿™äº›å›¾ä¸åº”é•¿æ—¶é—´ç¼–è¯‘ï¼Œå¦åˆ™ä¼šä¸¥é‡å½±å“å¼€å‘æ—¶é—´ã€‚å…¶æ¬¡ï¼Œé€šè¿‡ç¼–è¯‘ï¼Œä¼šå¤±å»å¯è§£é‡Šæ€§å’Œæœ‰æ•ˆè®°å½•/è°ƒè¯•çš„èƒ½åŠ›ã€‚å¦‚æœæœ‰é€‰é¡¹å¯ä»¥åœ¨å¼€å‘å®Œæˆåç¼–è¯‘å›¾ä»¥æé«˜ç”Ÿäº§æ•ˆç‡ï¼Œé‚£ä¹Ÿå¾ˆå¥½ã€‚</li>
</ol>
<h1 id="further-reading-å»¶ä¼¸é˜…è¯»"><a href="#Further-Reading-å»¶ä¼¸é˜…è¯»" class="headerlink" title="Further Reading å»¶ä¼¸é˜…è¯»"></a>Further Reading å»¶ä¼¸é˜…è¯»</h1><p>Before the end of the post I also wanted to position RNNs in a wider context and provide a sketch of the current research directions. RNNs have recently generated a significant amount of buzz and excitement in the field of Deep Learning. Similar to Convolutional Networks they have been around for decades but their full potential has only recently started to get widely recognized, in large part due to our growing computational resources. Hereâ€™s a brief sketch of a few recent developments (definitely not complete list, and a lot of this work draws from research back to 1990s, see related work sections):<br>åœ¨æ–‡ç« çš„ç»“å°¾ï¼Œæˆ‘è¿˜æƒ³æŠŠRNNæ”¾åœ¨æ›´å¹¿æ³›çš„èƒŒæ™¯ä¸­ï¼Œå¹¶æä¾›å½“å‰ç ”ç©¶æ–¹å‘çš„æ¦‚è¿°ã€‚æœ€è¿‘ï¼ŒRNNåœ¨æ·±åº¦å­¦ä¹ é¢†åŸŸå¼•èµ·äº†å¤§é‡å…³æ³¨å’Œå…´å¥‹ã€‚ç±»ä¼¼äºå·ç§¯ç½‘ç»œï¼ŒRNNå·²ç»å­˜åœ¨äº†å‡ åå¹´ï¼Œä½†å®ƒä»¬çš„å…¨éƒ¨æ½œåŠ›ç›´åˆ°æœ€è¿‘æ‰å¼€å§‹è¢«å¹¿æ³›è®¤å¯ï¼Œè¿™åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šè¦å½’åŠŸäºæˆ‘ä»¬ä¸æ–­å¢é•¿çš„è®¡ç®—èµ„æºã€‚ä»¥ä¸‹æ˜¯ä¸€äº›æœ€è¿‘å‘å±•çš„ç®€è¦æ¦‚è¿°ï¼ˆç»ä¸æ˜¯å®Œæ•´çš„åˆ—è¡¨ï¼Œå…¶ä¸­å¾ˆå¤šå·¥ä½œå¯ä»¥è¿½æº¯åˆ°1990å¹´ä»£ï¼Œè¯¦è§ç›¸å…³ç ”ç©¶éƒ¨åˆ†ï¼‰ï¼š</p>
<p>In the domain of <strong>NLP/Speech</strong>, RNNs <a target="_blank" rel="noopener" href="http://www.jmlr.org/proceedings/papers/v32/graves14.pdf">transcribe speech to text</a>, perform <a target="_blank" rel="noopener" href="http://arxiv.org/abs/1409.3215">machine translation</a>, <a target="_blank" rel="noopener" href="http://www.cs.toronto.edu/~graves/handwriting.html">generate handwritten text</a>, and of course, they have been used as powerful language models <a target="_blank" rel="noopener" href="http://www.cs.utoronto.ca/~ilya/pubs/2011/LANG-RNN.pdf">(Sutskever et al.)</a> <a target="_blank" rel="noopener" href="http://arxiv.org/abs/1308.0850">(Graves)</a> <a target="_blank" rel="noopener" href="http://www.rnnlm.org/">(Mikolov et al.)</a> (both on the level of characters and words). Currently it seems that word-level models work better than character-level models, but this is surely a temporary thing.<br>åœ¨NLP/Speeché¢†åŸŸï¼ŒRNNç”¨äºå°†è¯­éŸ³è½¬å½•ä¸ºæ–‡æœ¬ã€æ‰§è¡Œæœºå™¨ç¿»è¯‘ã€ç”Ÿæˆæ‰‹å†™æ–‡æœ¬ï¼Œå½“ç„¶ï¼Œå®ƒä»¬ä¹Ÿè¢«ç”¨ä½œå¼ºå¤§çš„è¯­è¨€æ¨¡å‹ï¼ˆSutskeverç­‰äººï¼‰ï¼ˆGravesï¼‰ï¼ˆMikolovç­‰äººï¼‰ï¼ˆåŒ…æ‹¬å­—ç¬¦çº§å’Œå•è¯çº§ï¼‰ã€‚ç›®å‰çœ‹æ¥ï¼Œå•è¯çº§æ¨¡å‹æ¯”å­—ç¬¦çº§æ¨¡å‹æ•ˆæœæ›´å¥½ï¼Œä½†è¿™è‚¯å®šåªæ˜¯æš‚æ—¶çš„ã€‚</p>
<p><strong>Computer Vision.</strong> RNNs are also quickly becoming pervasive in Computer Vision. For example, weâ€™re seeing RNNs in frame-level <a target="_blank" rel="noopener" href="http://arxiv.org/abs/1411.4389">video classification</a>, <a target="_blank" rel="noopener" href="http://arxiv.org/abs/1411.4555">image captioning</a> (also including my own work and many others), <a target="_blank" rel="noopener" href="http://arxiv.org/abs/1505.00487">video captioning</a> and very recently <a target="_blank" rel="noopener" href="http://arxiv.org/abs/1505.02074">visual question answering</a>. My personal favorite RNNs in Computer Vision paper is <a target="_blank" rel="noopener" href="http://arxiv.org/abs/1406.6247">Recurrent Models of Visual Attention</a>, both due to its high-level direction (sequential processing of images with glances) and the low-level modeling (REINFORCE learning rule that is a special case of policy gradient methods in Reinforcement Learning, which allows one to train models that perform non-differentiable computation (taking glances around the image in this case)). Iâ€™m confident that this type of hybrid model that consists of a blend of CNN for raw perception coupled with an RNN glance policy on top will become pervasive in perception, especially for more complex tasks that go beyond classifying some objects in plain view.<br>è®¡ç®—æœºè§†è§‰ã€‚RNNä¹Ÿè¿…é€Ÿåœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸæ™®åŠã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬çœ‹åˆ°RNNç”¨äºå¸§çº§è§†é¢‘åˆ†ç±»ã€å›¾åƒæè¿°ç”Ÿæˆï¼ˆåŒ…æ‹¬æˆ‘è‡ªå·±çš„å·¥ä½œå’Œè®¸å¤šå…¶ä»–äººçš„å·¥ä½œï¼‰ã€è§†é¢‘æè¿°ç”Ÿæˆä»¥åŠæœ€è¿‘çš„è§†è§‰é—®ç­”ã€‚æˆ‘ä¸ªäººæœ€å–œæ¬¢çš„è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„RNNè®ºæ–‡æ˜¯ã€Šè§†è§‰æ³¨æ„åŠ›çš„é€’å½’æ¨¡å‹ã€‹ï¼Œå› ä¸ºå®ƒåœ¨é«˜å±‚æ¬¡æ–¹å‘ï¼ˆé€šè¿‡æ‰«è§†å¯¹å›¾åƒè¿›è¡Œé¡ºåºå¤„ç†ï¼‰å’Œä½å±‚æ¬¡å»ºæ¨¡ï¼ˆREINFORCEå­¦ä¹ è§„åˆ™ï¼Œè¿™æ˜¯å¼ºåŒ–å­¦ä¹ ä¸­ç­–ç•¥æ¢¯åº¦æ–¹æ³•çš„ç‰¹ä¾‹ï¼Œå…è®¸è®­ç»ƒæ‰§è¡Œä¸å¯å¾®è®¡ç®—çš„æ¨¡å‹ï¼ˆåœ¨æœ¬ä¾‹ä¸­æ˜¯ç¯é¡¾å›¾åƒï¼‰ï¼‰ä¸Šéƒ½å¾ˆå‡ºè‰²ã€‚æˆ‘ç›¸ä¿¡è¿™ç§æ··åˆæ¨¡å‹ï¼Œå³ç»“åˆäº†ç”¨äºåŸå§‹æ„ŸçŸ¥çš„CNNå’Œç”¨äºæ‰«è§†ç­–ç•¥çš„RNNçš„æ¨¡å‹ï¼Œå°†åœ¨æ„ŸçŸ¥é¢†åŸŸæ™®åŠï¼Œç‰¹åˆ«æ˜¯åœ¨è¶…è¶Šç®€å•å¯¹è±¡åˆ†ç±»çš„å¤æ‚ä»»åŠ¡ä¸­ã€‚</p>
<p><strong>Inductive Reasoning, Memories and Attention.</strong> Another extremely exciting direction of research is oriented towards addressing the limitations of vanilla recurrent networks. One problem is that RNNs are not inductive: They memorize sequences extremely well, but they donâ€™t necessarily always show convincing signs of generalizing in the _correct_ way (Iâ€™ll provide pointers in a bit that make this more concrete). A second issue is they unnecessarily couple their representation size to the amount of computation per step. For instance, if you double the size of the hidden state vector youâ€™d quadruple the amount of FLOPS at each step due to the matrix multiplication. Ideally, weâ€™d like to maintain a huge representation/memory (e.g. containing all of Wikipedia or many intermediate state variables), while maintaining the ability to keep computation per time step fixed.<br>å½’çº³æ¨ç†ã€è®°å¿†å’Œæ³¨æ„åŠ›ã€‚å¦ä¸€ä¸ªæå…¶ä»¤äººå…´å¥‹çš„ç ”ç©¶æ–¹å‘æ˜¯è§£å†³æ™®é€šå¾ªç¯ç½‘ç»œçš„å±€é™æ€§ã€‚ä¸€ä¸ªé—®é¢˜æ˜¯RNNä¸å…·å¤‡å½’çº³èƒ½åŠ›ï¼šå®ƒä»¬éå¸¸æ“…é•¿è®°å¿†åºåˆ—ï¼Œä½†ä¸ä¸€å®šæ€»æ˜¯èƒ½ä»¥æ­£ç¡®çš„æ–¹å¼è¡¨ç°å‡ºä»¤äººä¿¡æœçš„æ³›åŒ–èƒ½åŠ›ï¼ˆç¨åæˆ‘ä¼šæä¾›ä¸€äº›å…·ä½“çš„ä¾‹å­ï¼‰ã€‚ç¬¬äºŒä¸ªé—®é¢˜æ˜¯å®ƒä»¬ä¸å¿…è¦åœ°å°†è¡¨ç¤ºå¤§å°ä¸æ¯æ­¥è®¡ç®—é‡è€¦åˆåœ¨ä¸€èµ·ã€‚ä¾‹å¦‚ï¼Œå¦‚æœä½ å°†éšè—çŠ¶æ€å‘é‡çš„å¤§å°åŠ å€ï¼Œé‚£ä¹ˆç”±äºçŸ©é˜µä¹˜æ³•ï¼Œæ¯æ­¥çš„æµ®ç‚¹è¿ç®—é‡ï¼ˆFLOPSï¼‰å°†å¢åŠ å››å€ã€‚ç†æƒ³æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¸Œæœ›ä¿æŒä¸€ä¸ªå·¨å¤§çš„è¡¨ç¤º/è®°å¿†ï¼ˆä¾‹å¦‚åŒ…å«æ•´ä¸ªç»´åŸºç™¾ç§‘æˆ–è®¸å¤šä¸­é—´çŠ¶æ€å˜é‡ï¼‰ï¼ŒåŒæ—¶ä¿æŒæ¯ä¸ªæ—¶é—´æ­¥çš„è®¡ç®—é‡å›ºå®šã€‚</p>
<p>The first convincing example of moving towards these directions was developed in DeepMindâ€™s <a target="_blank" rel="noopener" href="http://arxiv.org/abs/1410.5401">Neural Turing Machines</a> paper. This paper sketched a path towards models that can perform read/write operations between large, external memory arrays and a smaller set of memory registers (think of these as our working memory) where the computation happens. Crucially, the NTM paper also featured very interesting memory addressing mechanisms that were implemented with a (soft, and fully-differentiable) attention model. The concept of <strong>soft attention</strong> has turned out to be a powerful modeling feature and was also featured in <a target="_blank" rel="noopener" href="http://arxiv.org/abs/1409.0473">Neural Machine Translation by Jointly Learning to Align and Translate</a> for Machine Translation and <a target="_blank" rel="noopener" href="http://arxiv.org/abs/1503.08895">Memory Networks</a> for (toy) Question Answering. In fact, Iâ€™d go as far as to say that<br>ç¬¬ä¸€ä¸ªæœç€è¿™äº›æ–¹å‘å‰è¿›çš„ä»¤äººä¿¡æœçš„ä¾‹å­æ˜¯DeepMindçš„ã€Šç¥ç»å›¾çµæœºã€‹ï¼ˆNeural Turing Machinesï¼‰è®ºæ–‡ã€‚è¿™ç¯‡è®ºæ–‡å‹¾ç”»äº†ä¸€ä¸ªæ¨¡å‹çš„è·¯å¾„ï¼Œè¿™äº›æ¨¡å‹å¯ä»¥åœ¨å¤§å‹å¤–éƒ¨å­˜å‚¨é˜µåˆ—å’Œä¸€å°ç»„è®¡ç®—å‘ç”Ÿçš„å­˜å‚¨å¯„å­˜å™¨ï¼ˆå¯ä»¥å°†è¿™äº›è§†ä¸ºæˆ‘ä»¬çš„å·¥ä½œè®°å¿†ï¼‰ä¹‹é—´æ‰§è¡Œè¯»/å†™æ“ä½œã€‚è‡³å…³é‡è¦çš„æ˜¯ï¼ŒNTMè®ºæ–‡è¿˜å±•ç¤ºäº†éå¸¸æœ‰è¶£çš„è®°å¿†å¯»å€æœºåˆ¶ï¼Œè¿™äº›æœºåˆ¶é€šè¿‡ä¸€ä¸ªï¼ˆè½¯ä¸”å®Œå…¨å¯å¾®åˆ†çš„ï¼‰æ³¨æ„åŠ›æ¨¡å‹å®ç°ã€‚è½¯æ³¨æ„åŠ›çš„æ¦‚å¿µè¢«è¯æ˜æ˜¯ä¸€ä¸ªå¼ºå¤§çš„å»ºæ¨¡ç‰¹æ€§ï¼Œå®ƒä¹Ÿå‡ºç°åœ¨ã€Šé€šè¿‡è”åˆå­¦ä¹ å¯¹é½å’Œç¿»è¯‘çš„ç¥ç»æœºå™¨ç¿»è¯‘ã€‹å’Œã€Šè®°å¿†ç½‘ç»œç”¨äºï¼ˆç©å…·ï¼‰é—®ç­”ã€‹ä¸­ã€‚å®é™…ä¸Šï¼Œæˆ‘ç”šè‡³å¯ä»¥è¯´</p>
<p>The concept of <strong>attention</strong> is the most interesting recent architectural innovation in neural networks.<br>æ³¨æ„åŠ›çš„æ¦‚å¿µæ˜¯ç¥ç»ç½‘ç»œä¸­æœ€è¿‘æœ€æœ‰è¶£çš„æ¶æ„åˆ›æ–°ã€‚</p>
<p>Now, I donâ€™t want to dive into too many details but a soft attention scheme for memory addressing is convenient because it keeps the model fully-differentiable, but unfortunately one sacrifices efficiency because everything that can be attended to is attended to (but softly). Think of this as declaring a pointer in C that doesnâ€™t point to a specific address but instead defines an entire distribution over all addresses in the entire memory, and dereferencing the pointer returns a weighted sum of the pointed content (that would be an expensive operation!). This has motivated multiple authors to swap soft attention models for <strong>hard attention</strong> where one samples a particular chunk of memory to attend to (e.g. a read/write action for some memory cell instead of reading/writing from all cells to some degree). This model is significantly more philosophically appealing, scalable and efficient, but unfortunately it is also non-differentiable. This then calls for use of techniques from the Reinforcement Learning literature (e.g. REINFORCE) where people are perfectly used to the concept of non-differentiable interactions. This is very much ongoing work but these hard attention models have been explored, for example, in <a target="_blank" rel="noopener" href="http://arxiv.org/abs/1503.01007">Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets</a>, <a target="_blank" rel="noopener" href="http://arxiv.org/abs/1505.00521">Reinforcement Learning Neural Turing Machines</a>, and <a target="_blank" rel="noopener" href="http://arxiv.org/abs/1502.03044">Show Attend and Tell</a>.<br>ç°åœ¨ï¼Œæˆ‘ä¸æƒ³æ·±å…¥ç ”ç©¶å¤ªå¤šç»†èŠ‚ï¼Œä½†ç”¨äºå†…å­˜å¯»å€çš„è½¯æ³¨æ„åŠ›æ–¹æ¡ˆå¾ˆæ–¹ä¾¿ï¼Œå› ä¸ºå®ƒä½¿æ¨¡å‹ä¿æŒå®Œå…¨å¯å¾®åˆ†ï¼Œä½†ä¸å¹¸çš„æ˜¯ï¼Œäººä»¬ç‰ºç‰²äº†æ•ˆç‡ï¼Œå› ä¸ºæ‰€æœ‰å¯ä»¥å¤„ç†çš„ä¸œè¥¿éƒ½è¢«å¤„ç†äº†ï¼ˆä½†å¾ˆè½¯ï¼‰ã€‚å¯ä»¥æŠŠå®ƒæƒ³è±¡æˆåœ¨ C è¯­è¨€ä¸­å£°æ˜ä¸€ä¸ªæŒ‡é’ˆï¼Œè¯¥æŒ‡é’ˆä¸æŒ‡å‘ç‰¹å®šåœ°å€ï¼Œè€Œæ˜¯åœ¨æ•´ä¸ªå†…å­˜ä¸­çš„æ‰€æœ‰åœ°å€ä¸Šå®šä¹‰æ•´ä¸ªåˆ†å¸ƒï¼Œå¹¶ä¸”å–æ¶ˆå¼•ç”¨æŒ‡é’ˆä¼šè¿”å›æŒ‡å‘å†…å®¹çš„åŠ æƒæ€»å’Œï¼ˆè¿™å°†æ˜¯ä¸€ä¸ªæ˜‚è´µçš„æ“ä½œï¼è¿™ä¿ƒä½¿å¤šä½ä½œè€…å°†è½¯æ³¨æ„åŠ›æ¨¡å‹æ¢æˆç¡¬æ³¨æ„åŠ›æ¨¡å‹ï¼Œå…¶ä¸­ä¸€ä¸ªäººå¯¹è¦å…³æ³¨çš„ç‰¹å®šå†…å­˜å—è¿›è¡Œé‡‡æ ·ï¼ˆä¾‹å¦‚ï¼Œå¯¹æŸäº›è®°å¿†å•å…ƒè¿›è¡Œè¯»/å†™æ“ä½œï¼Œè€Œä¸æ˜¯åœ¨æŸç§ç¨‹åº¦ä¸Šä»æ‰€æœ‰å•å…ƒè¯»å–/å†™å…¥ï¼‰ã€‚è¿™ä¸ªæ¨¡å‹åœ¨å“²å­¦ä¸Šæ›´å…·å¸å¼•åŠ›ã€å¯æ‰©å±•æ€§å’Œæ•ˆç‡ï¼Œä½†ä¸å¹¸çš„æ˜¯ï¼Œå®ƒä¹Ÿæ˜¯ä¸å¯å¾®åˆ†çš„ã€‚ç„¶åï¼Œè¿™éœ€è¦ä½¿ç”¨å¼ºåŒ–å­¦ä¹ æ–‡çŒ®ä¸­çš„æŠ€æœ¯ï¼ˆä¾‹å¦‚REINFORCEï¼‰ï¼Œåœ¨è¿™äº›æŠ€æœ¯ä¸­ï¼Œäººä»¬å®Œå…¨ä¹ æƒ¯äº†ä¸å¯å¾®åˆ†äº¤äº’çš„æ¦‚å¿µã€‚è¿™æ˜¯ä¸€é¡¹æ­£åœ¨è¿›è¡Œçš„å·¥ä½œï¼Œä½†è¿™äº›ç¡¬æ³¨æ„åŠ›æ¨¡å‹å·²ç»è¢«æ¢ç´¢è¿‡ï¼Œä¾‹å¦‚ï¼Œåœ¨ä½¿ç”¨å †æ ˆå¢å¼ºçš„å¾ªç¯ç½‘ç»œæ¨æ–­ç®—æ³•æ¨¡å¼ã€å¼ºåŒ–å­¦ä¹ ç¥ç»å›¾çµæœºå’Œæ˜¾ç¤ºã€å‡ºå¸­å’Œè®²è¿°ä¸­ã€‚</p>
<p><strong>People</strong>. If youâ€™d like to read up on RNNs I recommend theses from <a target="_blank" rel="noopener" href="http://www.cs.toronto.edu/~graves/">Alex Graves</a>, <a target="_blank" rel="noopener" href="http://www.cs.toronto.edu/~ilya/">Ilya Sutskever</a> and <a target="_blank" rel="noopener" href="http://www.rnnlm.org/">Tomas Mikolov</a>. For more about REINFORCE and more generally Reinforcement Learning and policy gradient methods (which REINFORCE is a special case of) <a target="_blank" rel="noopener" href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Home.html">David Silver</a>â€™s class, or one of <a target="_blank" rel="noopener" href="http://www.cs.berkeley.edu/~pabbeel/">Pieter Abbeel</a>â€™s classes.<br>äººã€‚å¦‚æœä½ æƒ³æ·±å…¥äº†è§£RNNï¼Œæˆ‘æ¨èé˜…è¯»Alex Gravesã€Ilya Sutskeverå’ŒTomas Mikolovçš„è®ºæ–‡ã€‚å…³äºREINFORCEå’Œæ›´å¹¿æ³›çš„å¼ºåŒ–å­¦ä¹ åŠç­–ç•¥æ¢¯åº¦æ–¹æ³•ï¼ˆREINFORCEæ˜¯å…¶ç‰¹ä¾‹ï¼‰ï¼Œå¯ä»¥å‚è€ƒDavid Silverçš„è¯¾ç¨‹ï¼Œæˆ–Pieter Abbeelçš„è¯¾ç¨‹ä¹‹ä¸€ã€‚</p>
<p><strong>Code</strong>. If youâ€™d like to play with training RNNs I hear good things about <a target="_blank" rel="noopener" href="https://github.com/fchollet/keras">keras</a> or <a target="_blank" rel="noopener" href="https://github.com/IndicoDataSolutions/Passage">passage</a> for Theano, the <a target="_blank" rel="noopener" href="https://github.com/karpathy/char-rnn">code</a> released with this post for Torch, or <a target="_blank" rel="noopener" href="https://gist.github.com/karpathy/587454dc0146a6ae21fc">this gist</a> for raw numpy code I wrote a while ago that implements an efficient, batched LSTM forward and backward pass. You can also have a look at my numpy-based <a target="_blank" rel="noopener" href="https://github.com/karpathy/neuraltalk">NeuralTalk</a> which uses an RNN/LSTM to caption images, or maybe this <a target="_blank" rel="noopener" href="http://jeffdonahue.com/lrcn/">Caffe</a> implementation by Jeff Donahue.<br>ä»£ç ã€‚å¦‚æœä½ æƒ³å°è¯•è®­ç»ƒRNNï¼Œæˆ‘å¬è¯´kerasæˆ–passage for Theanoçš„è¯„ä»·å¾ˆå¥½ï¼Œå¯ä»¥å‚è€ƒæœ¬æ–‡å‘å¸ƒçš„ç”¨äºTorchçš„ä»£ç ï¼Œæˆ–è€…æˆ‘ä¹‹å‰ç¼–å†™çš„è¿™ä¸ªå®ç°äº†é«˜æ•ˆæ‰¹é‡LSTMå‰å‘å’Œåå‘ä¼ é€’çš„çº¯numpyä»£ç ã€‚ä½ ä¹Ÿå¯ä»¥çœ‹çœ‹æˆ‘åŸºäºnumpyçš„NeuralTalkï¼Œå®ƒä½¿ç”¨RNN/LSTMç”Ÿæˆå›¾åƒæè¿°ï¼Œæˆ–è€…çœ‹çœ‹Jeff Donahueçš„è¿™ä¸ªCaffeå®ç°ã€‚</p>
<h1 id="conclusion-ç»“è®º"><a href="#Conclusion-ç»“è®º" class="headerlink" title="Conclusion ç»“è®º"></a>Conclusion ç»“è®º</h1><p>Weâ€™ve learned about RNNs, how they work, why they have become a big deal, weâ€™ve trained an RNN character-level language model on several fun datasets, and weâ€™ve seen where RNNs are going. You can confidently expect a large amount of innovation in the space of RNNs, and I believe they will become a pervasive and critical component to intelligent systems.<br>æˆ‘ä»¬å·²ç»äº†è§£äº†RNNï¼Œå®ƒä»¬æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Œä¸ºä»€ä¹ˆå®ƒä»¬å˜å¾—å¦‚æ­¤é‡è¦ã€‚æˆ‘ä»¬åœ¨å‡ ä¸ªæœ‰è¶£çš„æ•°æ®é›†ä¸Šè®­ç»ƒäº†ä¸€ä¸ªRNNå­—ç¬¦çº§è¯­è¨€æ¨¡å‹ï¼Œå¹¶ä¸”çœ‹åˆ°äº†RNNçš„å‘å±•æ–¹å‘ã€‚ä½ å¯ä»¥è‡ªä¿¡åœ°æœŸå¾…åœ¨RNNé¢†åŸŸå‡ºç°å¤§é‡çš„åˆ›æ–°ï¼Œæˆ‘ç›¸ä¿¡å®ƒä»¬å°†æˆä¸ºæ™ºèƒ½ç³»ç»Ÿä¸­æ™®éä¸”å…³é”®çš„ç»„æˆéƒ¨åˆ†ã€‚</p>
<p>Lastly, to add some <strong>meta</strong> to this post, I trained an RNN on the source file of this blog post. Unfortunately, at about 46K characters I havenâ€™t written enough data to properly feed the RNN, but the returned sample (generated with low temperature to get a more typical sample) is:<br>æœ€åï¼Œä¸ºäº†ç»™è¿™ç¯‡æ–‡ç« å¢åŠ ä¸€äº›å…ƒå†…å®¹ï¼Œæˆ‘ç”¨è¿™ç¯‡åšå®¢æ–‡ç« çš„æºæ–‡ä»¶è®­ç»ƒäº†ä¸€ä¸ªRNNã€‚ä¸å¹¸çš„æ˜¯ï¼Œå¤§çº¦46Kå­—ç¬¦çš„æ•°æ®é‡è¿˜ä¸è¶³ä»¥å……åˆ†è®­ç»ƒRNNï¼Œä½†ç”Ÿæˆçš„æ ·æœ¬ï¼ˆåœ¨ä½æ¸©ä¸‹ç”Ÿæˆï¼Œä»¥è·å¾—æ›´å…¸å‹çš„æ ·æœ¬ï¼‰å¦‚ä¸‹ï¼š</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">I&#x27;ve the RNN with and works, but the computed with program of the </span><br><span class="line">RNN with and the computed of the RNN with with and the code</span><br></pre></td></tr></table></figure>
<p>Yes, the post was about RNN and how well it works, so clearly this works :). See you next time!<br>æ˜¯çš„ï¼Œè¿™ç¯‡æ–‡ç« æ˜¯å…³äº RNN åŠå…¶å·¥ä½œæƒ…å†µçš„ï¼Œæ‰€ä»¥å¾ˆæ˜æ˜¾è¿™:)å·¥ä½œã€‚ä¸‹æ¬¡å†è§ï¼</p>
<h1 id="edit-extra-links-ç¼–è¾‘é¢å¤–é“¾æ¥"><a href="#EDIT-extra-links-ç¼–è¾‘ï¼ˆé¢å¤–é“¾æ¥ï¼‰" class="headerlink" title="EDIT (extra links): ç¼–è¾‘ï¼ˆé¢å¤–é“¾æ¥ï¼‰"></a>EDIT (extra links): ç¼–è¾‘ï¼ˆé¢å¤–é“¾æ¥ï¼‰</h1><p>Videos: è§†é¢‘ï¼š</p>
<ul>
<li>I gave a talk on this work at the <a target="_blank" rel="noopener" href="https://skillsmatter.com/skillscasts/6611-visualizing-and-understanding-recurrent-networks">London Deep Learning meetup (video)</a>.<br>æˆ‘åœ¨ä¼¦æ•¦æ·±åº¦å­¦ä¹ èšä¼šä¸Šå°±è¿™é¡¹å·¥ä½œå‘è¡¨äº†æ¼”è®²ï¼ˆè§†é¢‘ï¼‰ã€‚</li>
</ul>
<p>Discussions: è®¨è®ºï¼š</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://news.ycombinator.com/item?id=9584325">HN discussion HN è®¨è®º</a></li>
<li>Reddit discussion on <a target="_blank" rel="noopener" href="http://www.reddit.com/r/MachineLearning/comments/36s673/the_unreasonable_effectiveness_of_recurrent/">r/machinelearning</a><br>Reddit å…³äº r/machinelearning çš„è®¨è®º</li>
<li>Reddit discussion on <a target="_blank" rel="noopener" href="http://www.reddit.com/r/programming/comments/36su8d/the_unreasonable_effectiveness_of_recurrent/">r/programming</a><br>Reddit ä¸Šå…³äº r/programming çš„è®¨è®º</li>
</ul>
<p>Replies: ç­”å¤ï¼š</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://twitter.com/yoavgo">Yoav Goldberg</a> compared these RNN results to <a target="_blank" rel="noopener" href="http://nbviewer.ipython.org/gist/yoavg/d76121dfde2618422139">n-gram maximum likelihood (counting) baseline</a><br>Yoav Goldberg å°†è¿™äº› RNN ç»“æœä¸ n-gram æœ€å¤§ä¼¼ç„¶ï¼ˆè®¡æ•°ï¼‰åŸºçº¿è¿›è¡Œäº†æ¯”è¾ƒ</li>
<li><a target="_blank" rel="noopener" href="https://twitter.com/nylk">@nylk</a> trained char-rnn on <a target="_blank" rel="noopener" href="https://gist.github.com/nylki/1efbaa36635956d35bcc">cooking recipes</a>. They look great!<br>@nylkåŸ¹è®­äº†char-rnnçš„çƒ¹é¥ªé£Ÿè°±ã€‚å®ƒä»¬çœ‹èµ·æ¥å¾ˆæ£’ï¼</li>
<li><a target="_blank" rel="noopener" href="https://twitter.com/MrChrisJohnson">@MrChrisJohnson</a> trained char-rnn on Eminem lyrics and then synthesized a rap song with robotic voice reading it out. Hilarious :)<br>@MrChrisJohnsonç”¨ Eminem çš„æ­Œè¯è®­ç»ƒäº† char-rnnï¼Œç„¶ååˆæˆäº†ä¸€é¦–å¸¦æœ‰æœºå™¨äººå£°éŸ³çš„è¯´å”±æ­Œæ›²ã€‚æç¬‘:)</li>
<li><a target="_blank" rel="noopener" href="https://twitter.com/samim">@samim</a> trained char-rnn on <a target="_blank" rel="noopener" href="https://medium.com/@samim/obama-rnn-machine-generated-political-speeches-c8abd18a2ea0">Obama Speeches</a>. They look fun!<br>@samimåŸ¹è®­äº†å¥¥å·´é©¬æ¼”è®²çš„char-rnnã€‚ä»–ä»¬çœ‹èµ·æ¥å¾ˆæœ‰è¶£ï¼</li>
<li><a target="_blank" rel="noopener" href="https://twitter.com/seaandsailor">JoÃ£o Felipe</a> trained char-rnn irish folk music and <a target="_blank" rel="noopener" href="https://soundcloud.com/seaandsailor/sets/char-rnn-composes-irish-folk-music">sampled music</a><br>è‹¥æ˜‚Â·è´¹åˆ©ä½©ï¼ˆJoÃ£o Felipeï¼‰è®­ç»ƒäº†char-rnnçˆ±å°”å…°æ°‘é—´éŸ³ä¹å¹¶é‡‡æ ·äº†éŸ³ä¹</li>
<li><a target="_blank" rel="noopener" href="https://twitter.com/boblsturm">Bob Sturm</a> also trained char-rnn on <a target="_blank" rel="noopener" href="https://highnoongmt.wordpress.com/2015/05/22/lisls-stis-recurrent-neural-networks-for-folk-music-generation/">music in ABC notation</a><br>é²å‹ƒÂ·æ–¯ç‰¹å§†ï¼ˆBob Sturmï¼‰è¿˜å¯¹char-rnnè¿›è¡Œäº†ABCè®°è°±æ³•çš„éŸ³ä¹åŸ¹è®­</li>
<li><a target="_blank" rel="noopener" href="https://twitter.com/RNN_Bible">RNN Bible bot</a> by <a target="_blank" rel="noopener" href="https://twitter.com/the__glu/with_replies">Maximilien</a><br>RNN Bible bot çš„ Maximilien</li>
<li><a target="_blank" rel="noopener" href="http://cpury.github.io/learning-holiness/">Learning Holiness</a> learning the Bible<br>å­¦ä¹ åœ£æ´ å­¦ä¹ åœ£ç»</li>
<li><a target="_blank" rel="noopener" href="https://www.terminal.com/tiny/ZMcqdkWGOM">Terminal.com snapshot</a> that has char-rnn set up and ready to go in a browser-based virtual machine (thanks <a target="_blank" rel="noopener" href="https://www.twitter.com/samim">@samim</a>)<br>Terminal.com å·²è®¾ç½® char-rnn å¹¶å‡†å¤‡åœ¨åŸºäºæµè§ˆå™¨çš„è™šæ‹Ÿæœºä¸­ä½¿ç”¨çš„å¿«ç…§ï¼ˆæ„Ÿè°¢ @samimï¼‰</li>
</ul>
<h1 id="æ³¨é‡Š"><a href="#æ³¨é‡Š" class="headerlink" title="æ³¨é‡Š"></a>æ³¨é‡Š</h1><h2 id="1vanilla-ç¥ç»ç½‘ç»œ"><a href="#1-Vanilla-ç¥ç»ç½‘ç»œ" class="headerlink" title="1.Vanilla ç¥ç»ç½‘ç»œ"></a>1.Vanilla ç¥ç»ç½‘ç»œ</h2><p>â€œVanilla ç¥ç»ç½‘ç»œâ€é€šå¸¸æŒ‡çš„æ˜¯æœ€åŸºæœ¬ã€æœ€ç®€å•çš„ç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œæ²¡æœ‰ä½¿ç”¨ä»»ä½•ç‰¹æ®Šçš„å±‚æˆ–å¤æ‚çš„æ¶æ„ã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒä¸€èˆ¬æŒ‡çš„æ˜¯ç®€å•çš„å‰é¦ˆç¥ç»ç½‘ç»œï¼ˆFeedforward Neural Network, FNNï¼‰</p>
<h2 id="2-å‘é‡vectorvs-åºåˆ—sequence"><a href="#2-å‘é‡ï¼ˆVectorï¼‰vs-åºåˆ—ï¼ˆSequenceï¼‰" class="headerlink" title="2. å‘é‡ï¼ˆVectorï¼‰vs åºåˆ—ï¼ˆSequenceï¼‰"></a>2. å‘é‡ï¼ˆVectorï¼‰vs åºåˆ—ï¼ˆSequenceï¼‰</h2><p><strong>å‘é‡ï¼ˆVectorï¼‰</strong>ï¼š</p>
<ul>
<li><strong>å®šä¹‰</strong>ï¼šåœ¨æ•°å­¦å’Œè®¡ç®—æœºç§‘å­¦ä¸­ï¼Œå‘é‡æ˜¯ä¸€ç»„æœ‰åºçš„æ•°å€¼ã€‚è¿™äº›æ•°å€¼å¯ä»¥è¡¨ç¤ºå¤šç»´ç©ºé—´ä¸­çš„ä¸€ä¸ªç‚¹æˆ–æŸä¸ªç‰¹å®šçš„æ•°æ®ç»“æ„ã€‚</li>
<li><strong>ç‰¹æ€§</strong>ï¼š<ul>
<li>å›ºå®šé•¿åº¦ï¼šå‘é‡çš„é•¿åº¦æ˜¯å›ºå®šçš„ï¼Œä¾‹å¦‚ä¸€ä¸ªåŒ…å«ä¸‰ä¸ªæ•°å€¼çš„å‘é‡$x_1, x_2, x_3$ã€‚ è¿™ç§å¤„ç†æ–¹å¼å¯¹äºå›¾åƒåˆ†ç±»ã€å›ºå®šé•¿åº¦çš„æ–‡æœ¬åˆ†ç±»ç­‰ä»»åŠ¡éå¸¸æœ‰æ•ˆã€‚</li>
<li>æ— æ—¶é—´ä¾èµ–ï¼šå‘é‡ä¸­çš„å…ƒç´ æ²¡æœ‰æ—¶é—´æˆ–é¡ºåºä¸Šçš„ä¾èµ–å…³ç³»ã€‚ä¾‹å¦‚ï¼Œä¸€å¼ å›¾ç‰‡çš„åƒç´ æ•°æ®å¯ä»¥ä½œä¸ºè¾“å…¥å‘é‡ï¼Œä½†è¿™äº›åƒç´ ä¹‹é—´æ²¡æœ‰æ—¶é—´é¡ºåºä¸Šçš„å…³ç³»ã€‚</li>
</ul>
</li>
<li><strong>ç¤ºä¾‹</strong>ï¼š<ul>
<li>å›¾åƒå¤„ç†ä¸­çš„åƒç´ å€¼å‘é‡ã€‚</li>
<li>é™æ€æ–‡æœ¬åˆ†ç±»ä¸­çš„å•è¯å‘é‡ã€‚</li>
</ul>
</li>
<li><strong>å‘é‡è¾“å…¥è¾“å‡º</strong>ï¼šå‘é‡è¾“å…¥è¾“å‡ºé€šå¸¸æŒ‡çš„æ˜¯ç¥ç»ç½‘ç»œå¤„ç†å›ºå®šé•¿åº¦çš„å‘é‡ï¼Œå³ä¸€ç»„å›ºå®šå¤§å°çš„æ•°å€¼è¾“å…¥å’Œè¾“å‡ºã€‚</li>
<li><strong>åº”ç”¨åœºæ™¯</strong>ï¼š<ul>
<li>å›¾åƒåˆ†ç±»ï¼šè¾“å…¥æ˜¯ä¸€ä¸ªå›ºå®šå¤§å°çš„å›¾åƒå‘é‡ï¼Œè¾“å‡ºæ˜¯ä¸€ä¸ªç±»åˆ«æ ‡ç­¾å‘é‡ã€‚</li>
<li>é™æ€æ–‡æœ¬åˆ†ç±»ï¼šè¾“å…¥æ˜¯ä¸€ä¸ªè¡¨ç¤ºå•ä¸ªæ–‡æ¡£çš„å‘é‡ï¼Œè¾“å‡ºæ˜¯åˆ†ç±»æ ‡ç­¾ã€‚</li>
</ul>
</li>
</ul>
<p><strong>åºåˆ—ï¼ˆSequenceï¼‰</strong>ï¼š</p>
<ul>
<li><strong>å®šä¹‰</strong>ï¼šåºåˆ—æ˜¯ä¸€ç»„æŒ‰ç‰¹å®šé¡ºåºæ’åˆ—çš„æ•°æ®ï¼Œé€šå¸¸æ˜¯æ—¶é—´æˆ–é¡ºåºç›¸å…³çš„ã€‚</li>
<li><strong>ç‰¹æ€§</strong>ï¼š<ul>
<li>å˜é•¿ï¼šåºåˆ—çš„é•¿åº¦å¯ä»¥å˜åŒ–ï¼Œä¾‹å¦‚ä¸€æ®µæ–‡æœ¬å¯ä»¥æ˜¯ä¸€ä¸ªå­—ç¬¦åºåˆ—$c_1, c_2, â€¦, c_t$</li>
<li>æœ‰æ—¶é—´ä¾èµ–ï¼šåºåˆ—ä¸­çš„å…ƒç´ æœ‰æ—¶é—´æˆ–é¡ºåºä¸Šçš„ä¾èµ–å…³ç³»ã€‚åç»­å…ƒç´ ä¾èµ–äºå‰é¢å‡ºç°çš„å…ƒç´ ã€‚ä¾‹å¦‚ï¼Œåœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­ï¼Œä¸€ä¸ªå¥å­çš„å•è¯é¡ºåºå†³å®šäº†å¥å­çš„æ„ä¹‰ã€‚</li>
</ul>
</li>
<li><strong>ç¤ºä¾‹</strong>ï¼š<ul>
<li>æ—¶é—´åºåˆ—æ•°æ®ï¼Œå¦‚è‚¡ç¥¨ä»·æ ¼çš„æ¯æ—¥è®°å½•ã€‚</li>
<li>æ–‡æœ¬æ•°æ®ï¼Œå¦‚ä¸€æ®µå¥å­ä¸­çš„å•è¯åºåˆ—ã€‚</li>
</ul>
</li>
<li><strong>åºåˆ—è¾“å…¥è¾“å‡º</strong>ï¼šåºåˆ—è¾“å…¥è¾“å‡ºæŒ‡çš„æ˜¯ç¥ç»ç½‘ç»œå¤„ç†ä¸€ç³»åˆ—çš„è¾“å…¥æ•°æ®ï¼Œè¿™äº›æ•°æ®æœ‰æ—¶é—´æˆ–é¡ºåºä¸Šçš„ä¾èµ–å…³ç³»ã€‚RNNï¼ˆå¾ªç¯ç¥ç»ç½‘ç»œï¼‰å°±æ˜¯å¤„ç†åºåˆ—æ•°æ®çš„å…¸å‹æ¨¡å‹ã€‚</li>
<li><strong>åº”ç”¨åœºæ™¯</strong>ï¼š<ul>
<li>è¯­è¨€æ¨¡å‹å’Œæ–‡æœ¬ç”Ÿæˆï¼šè¾“å…¥æ˜¯ä¸€ä¸ªæ–‡æœ¬åºåˆ—ï¼Œè¾“å‡ºæ˜¯ä¸‹ä¸€ä¸ªå­—ç¬¦æˆ–å•è¯çš„é¢„æµ‹åºåˆ—ã€‚</li>
<li>æœºå™¨ç¿»è¯‘ï¼šè¾“å…¥æ˜¯æºè¯­è¨€çš„å¥å­åºåˆ—ï¼Œè¾“å‡ºæ˜¯ç›®æ ‡è¯­è¨€çš„å¥å­åºåˆ—ã€‚</li>
<li>è¯­éŸ³è¯†åˆ«ï¼šè¾“å…¥æ˜¯è¯­éŸ³ä¿¡å·çš„æ—¶é—´åºåˆ—ï¼Œè¾“å‡ºæ˜¯å¯¹åº”çš„æ–‡æœ¬åºåˆ—ã€‚</li>
</ul>
</li>
</ul>
<h2 id="3-rnn-computation"><a href="#3-RNN-computation" class="headerlink" title="3. RNN computation"></a>3. RNN computation</h2><p>è®ºæ–‡æ­£æ–‡ä¸­å¯¹è¯¥è¿‡ç¨‹çš„æè¿°æ–‡å­—è¾ƒå¤šï¼Œæˆ‘åå€’è§‰å¾—ç»“åˆæ•°å­¦å…¬å¼åæ›´å¥½ç†è§£ã€‚<br>è¿™é‡Œæè¿°å°±æ˜¯ RNN çš„å‰å‘ä¼ æ’­è¿‡ç¨‹ï¼Œ å¦‚æœä½ äº†è§£äº†åŸºç¡€ç¥ç»ç½‘ç»œçš„å‰å‘ä¼ æ’­è¿‡ç¨‹ï¼Œé‚£ä¹ˆRNNä¹Ÿéå¸¸å¥½ç†è§£ã€‚</p>
<p>ç®€å•ç†è§£å°±æ˜¯ ç›¸æ¯”åœ¨ç¥ç»ç½‘ç»œä¸€æ–‡ä¸­è®²è¿°çš„åŸºç¡€ å‰å‘ä¼ æ’­è¿‡ç¨‹ä¸­ï¼Œé€’å½’ç¥ç»ç½‘ç»œåœ¨æ­¤åŸºç¡€ä¸Šå¢åŠ äº†ä¸€ä¸ªéšè—å±‚åˆ°éšè—å±‚çš„æƒé‡çŸ©é˜µå‚ä¸è®¡ç®—ã€‚</p>
<p>è¿™3ä¸ªæƒé‡çŸ©é˜µåˆ†åˆ«å¯¹åº”æ–‡ä¸­W_hhï¼ˆéšè—å±‚åˆ°éšè—å±‚ï¼‰ã€W_xhï¼ˆè¾“å…¥å±‚åˆ°éšè—å±‚ï¼‰å’ŒW_hyï¼ˆéšè—å±‚åˆ°è¾“å‡ºå±‚ï¼‰,ä¸‹é¢ä»¥$â„_ğ‘¡=tanhâ¡(ğ‘Š_{â„â„}â„_{ğ‘¡âˆ’1}+ğ‘Š_{ğ‘¥â„}ğ‘¥_ğ‘¡)$ æ¥æ‹†è§£æ•´ä¸ªè¿‡ç¨‹</p>
<p>å‡è®¾ç½‘ç»œç»“æ„å’Œå‚æ•°å¦‚ä¸‹</p>
<ol>
<li><strong>è¾“å…¥å±‚</strong>ï¼š2ä¸ªèŠ‚ç‚¹ï¼Œè¡¨ç¤ºè¾“å…¥ç‰¹å¾ $x_1$ å’Œ $x_2$ã€‚</li>
<li><strong>éšè—å±‚</strong>ï¼š2ä¸ªèŠ‚ç‚¹ï¼Œè¡¨ç¤ºéšè—çŠ¶æ€ $h_1$ å’Œ $h_2$â€‹ï¼Œ,ä½¿ç”¨tanhæ¿€æ´»å‡½æ•°ã€‚</li>
<li><strong>è¾“å‡ºå±‚</strong>ï¼š1ä¸ªèŠ‚ç‚¹ï¼Œè¡¨ç¤ºè¾“å‡º $y$ï¼Œä½¿ç”¨çº¿æ€§æ¿€æ´»å‡½æ•°ã€‚</li>
</ol>
<h4 id="æƒé‡çŸ©é˜µ"><a href="#æƒé‡çŸ©é˜µ" class="headerlink" title="æƒé‡çŸ©é˜µ"></a>æƒé‡çŸ©é˜µ</h4><ol>
<li><p><strong>è¾“å…¥å±‚åˆ°éšè—å±‚çš„æƒé‡</strong>ï¼š $W_{xh} = \begin{bmatrix} W_{11} &amp; W_{12} \ W_{21} &amp; W_{22} \end{bmatrix}$</p>
<p>$W_{11}$ã€$W_{12}$â€‹ è¿æ¥ $x_1$â€‹ åˆ° $h_1$â€‹ å’Œ $h_2$â€‹ï¼Œ$W_{21}$ã€$W_{22}$â€‹ è¿æ¥ $x_2$â€‹ åˆ° $h_1$â€‹ å’Œ $h_2$ã€‚</p>
</li>
<li><p><strong>éšè—å±‚åˆ°éšè—å±‚çš„æƒé‡</strong>ï¼š$W_{hh} = \begin{bmatrix} U_{11} &amp; U_{12} \ U_{21} &amp; U_{22} \end{bmatrix}$</p>
<ul>
<li>$U_{11}$å’Œ $U_{12}$ è¿æ¥ $h_1(t-1)$åˆ° $h_1(t)$ å’Œ $h_2(t)$ã€‚</li>
<li>$U_{21}$ å’Œ $U_{22}$ è¿æ¥ $h_2(t-1)$ åˆ° $h_1(t)$ å’Œ $h_2(t)$ã€‚<br>[[#5. åºåˆ—æ•°æ®çš„å¤„ç†]]</li>
</ul>
</li>
<li><p><strong>éšè—å±‚åˆ°è¾“å‡ºå±‚çš„æƒé‡</strong>ï¼š $W_{hy} = \begin{bmatrix} W_{31} &amp; W_{32} \end{bmatrix}$</p>
<ul>
<li>$W_{31}$â€‹ å’Œ $W_{32}$åˆ†åˆ«è¿æ¥$h_1$ å’Œ$h_2$åˆ° $y$ã€‚</li>
</ul>
</li>
</ol>
<h3 id="å‰å‘ä¼ æ’­è¿‡ç¨‹"><a href="#å‰å‘ä¼ æ’­è¿‡ç¨‹" class="headerlink" title="å‰å‘ä¼ æ’­è¿‡ç¨‹"></a>å‰å‘ä¼ æ’­è¿‡ç¨‹</h3><p>å¯¹äºæ¯ä¸ªæ—¶é—´æ­¥ $t$ï¼Œå‰å‘ä¼ æ’­çš„è®¡ç®—æ­¥éª¤å¦‚ä¸‹ï¼š</p>
<h4 id="1-è¾“å…¥å±‚åˆ°éšè—å±‚"><a href="#1-è¾“å…¥å±‚åˆ°éšè—å±‚" class="headerlink" title="1. è¾“å…¥å±‚åˆ°éšè—å±‚"></a>1. <strong>è¾“å…¥å±‚åˆ°éšè—å±‚</strong></h4><p>è®¡ç®—å½“å‰æ—¶é—´æ­¥çš„éšè—çŠ¶æ€, å³æ­£æ–‡ä¸­çš„â€œéšè—çŠ¶æ€çš„æ›´æ–°â€<br>$â„_ğ‘¡=tanhâ¡(ğ‘Š_{â„â„}â„_{ğ‘¡âˆ’1}+ğ‘Š_{ğ‘¥â„}ğ‘¥_ğ‘¡)$<br>å…¶ä¸­ï¼Œ$\mathbf{x}_t$â€‹ æ˜¯å½“å‰æ—¶é—´æ­¥çš„è¾“å…¥å‘é‡ï¼Œ$\mathbf{h}_{t-1}$æ˜¯å‰ä¸€æ—¶é—´æ­¥çš„éšè—çŠ¶æ€ã€‚<br>å…·ä½“å±•å¼€å¦‚ä¸‹ï¼š</p>
<p>$\begin{bmatrix} h_{1t} \ h_{2t} \end{bmatrix} = \text{tanh} \left( \begin{bmatrix} W_{11} &amp; W_{12} \ W_{21} &amp; W_{22} \end{bmatrix} \begin{bmatrix} x_{1t} \ x_{2t} \end{bmatrix} + \begin{bmatrix} U_{11} &amp; U_{12} \ U_{21} &amp; U_{22} \end{bmatrix} \begin{bmatrix} h_{1(t-1)} \ h_{2(t-1)} \end{bmatrix} + \begin{bmatrix} b_1 \ b_2 \end{bmatrix} \right)$</p>
<p>åˆ†å¼€è®¡ç®—ï¼š<br>$h_{1t} = \text{tanh}(W_{11} x_{1t} + W_{12} x_{2t} + U_{11} h_{1(t-1)} + U_{12} h_{2(t-1)} )$</p>
<p>$h_{2t} = \text{tanh}(W_{21} x_{1t} + W_{22} x_{2t} + U_{21} h_{1(t-1)} + U_{22} h_{2(t-1)})$</p>
<h4 id="2-éšè—å±‚åˆ°è¾“å‡ºå±‚"><a href="#2-éšè—å±‚åˆ°è¾“å‡ºå±‚" class="headerlink" title="2.  éšè—å±‚åˆ°è¾“å‡ºå±‚"></a>2.  <strong>éšè—å±‚åˆ°è¾“å‡ºå±‚</strong></h4><p>è®¡ç®—å½“å‰æ—¶é—´æ­¥çš„è¾“å‡ºã€‚ $y_t = W_{hy} \mathbf{h}_t$<br>å…·ä½“å±•å¼€å¦‚ä¸‹ï¼š<br>$y_t = \begin{bmatrix} W_{31} &amp; W_{32} \end{bmatrix} \begin{bmatrix} h_{1t} \ h_{2t} \end{bmatrix}$</p>
<p>åˆ†å¼€è®¡ç®—ï¼š</p>
<p>$y_t = W_{31} h_{1t} + W_{32} h_{2t}$</p>
<p>âš ï¸ï¼š tanh æ˜¯éçº¿æ€§æ¿€æ´»å‡½æ•°</p>
<h2 id="4-1-of-kç¼–ç "><a href="#4-1-of-kç¼–ç " class="headerlink" title="4. 1-of-kç¼–ç "></a>4. 1-of-kç¼–ç </h2><p>1-of-kç¼–ç ï¼Œä¹Ÿç§°ä¸ºç‹¬çƒ­ç¼–ç ï¼ˆOne-Hot Encodingï¼‰ï¼Œæ˜¯ä¸€ç§å°†åˆ†ç±»æ•°æ®è½¬æ¢ä¸ºäºŒè¿›åˆ¶å‘é‡çš„æ–¹æ³•ã€‚å®ƒçš„ç›®çš„æ˜¯å°†éæ•°å€¼å‹çš„ç±»åˆ«å˜é‡è½¬åŒ–ä¸ºé€‚åˆäºæœºå™¨å­¦ä¹ ç®—æ³•å¤„ç†çš„æ•°å€¼å‹æ•°æ®ã€‚</p>
<h4 id="å·¥ä½œåŸç†"><a href="#å·¥ä½œåŸç†" class="headerlink" title="å·¥ä½œåŸç†"></a>å·¥ä½œåŸç†</h4><p>å‡è®¾æœ‰ä¸€ä¸ªç±»åˆ«å˜é‡ï¼Œå®ƒæœ‰ $k$ ä¸ªä¸åŒçš„ç±»åˆ«ã€‚1-of-kç¼–ç å°†æ¯ä¸ªç±»åˆ«è¡¨ç¤ºä¸ºä¸€ä¸ªé•¿åº¦ä¸º $k$ çš„äºŒè¿›åˆ¶å‘é‡ï¼Œå…¶ä¸­åªæœ‰ä¸€ä¸ªä½ç½®ä¸º1ï¼Œå…¶ä»–ä½ç½®ä¸º0ã€‚</p>
<p>ä¾‹å¦‚ï¼Œè€ƒè™‘ä¸€ä¸ªæœ‰å››ä¸ªç±»åˆ«çš„å˜é‡ï¼šâ€Aâ€, â€œBâ€, â€œCâ€, â€œDâ€ã€‚å…¶1-of-kç¼–ç å¦‚ä¸‹ï¼š</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>ç±»åˆ«</th>
<th>1-of-kç¼–ç </th>
</tr>
</thead>
<tbody>
<tr>
<td>A</td>
<td>[1, 0, 0, 0]</td>
</tr>
<tr>
<td>B</td>
<td>[0, 1, 0, 0]</td>
</tr>
<tr>
<td>C</td>
<td>[0, 0, 1, 0]</td>
</tr>
<tr>
<td>D</td>
<td>[0, 0, 0, 1]</td>
</tr>
</tbody>
</table>
</div>
<h4 id="ä¼˜ç‚¹"><a href="#ä¼˜ç‚¹" class="headerlink" title="ä¼˜ç‚¹"></a>ä¼˜ç‚¹</h4><ul>
<li><strong>æ¶ˆé™¤æ’åºå…³ç³»</strong>ï¼šç‹¬çƒ­ç¼–ç å°†åˆ†ç±»å˜é‡è½¬åŒ–ä¸ºäºŒè¿›åˆ¶å‘é‡ï¼Œé¿å…äº†å¯¹åˆ†ç±»å˜é‡çš„è¯¯è§£ï¼Œå³è®¤ä¸ºå®ƒä»¬ä¹‹é—´å­˜åœ¨æ’åºå…³ç³»ã€‚</li>
<li><strong>é€‚åˆæ¨¡å‹å¤„ç†</strong>ï¼šè®¸å¤šæœºå™¨å­¦ä¹ ç®—æ³•ï¼ˆå¦‚çº¿æ€§å›å½’ã€é€»è¾‘å›å½’ç­‰ï¼‰æ— æ³•ç›´æ¥å¤„ç†éæ•°å€¼å‹æ•°æ®ï¼Œ1-of-kç¼–ç ä½¿è¿™äº›æ•°æ®é€‚åˆäºè¿™äº›ç®—æ³•ã€‚</li>
</ul>
<h4 id="ç¼ºç‚¹"><a href="#ç¼ºç‚¹" class="headerlink" title="ç¼ºç‚¹"></a>ç¼ºç‚¹</h4><ul>
<li><strong>é«˜ç»´åº¦é—®é¢˜</strong>ï¼šå½“ç±»åˆ«æ•°é‡è¾ƒå¤šæ—¶ï¼Œç¼–ç åçš„å‘é‡é•¿åº¦ä¼šå˜å¾—å¾ˆé•¿ï¼Œå¯¼è‡´é«˜ç»´åº¦é—®é¢˜ï¼Œå¢åŠ è®¡ç®—å’Œå­˜å‚¨æˆæœ¬ã€‚</li>
</ul>
<h4 id="å®é™…åº”ç”¨"><a href="#å®é™…åº”ç”¨" class="headerlink" title="å®é™…åº”ç”¨"></a>å®é™…åº”ç”¨</h4><ul>
<li><strong>æ–‡æœ¬å¤„ç†</strong>ï¼šåœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä»»åŠ¡ä¸­ï¼Œ1-of-kç¼–ç å¸¸ç”¨äºå°†å•è¯è½¬åŒ–ä¸ºäºŒè¿›åˆ¶å‘é‡ã€‚</li>
<li><strong>åˆ†ç±»ä»»åŠ¡</strong>ï¼šåœ¨åˆ†ç±»ä»»åŠ¡ä¸­ï¼Œç”¨äºå°†ç±»åˆ«æ ‡ç­¾è½¬åŒ–ä¸ºæ¨¡å‹å¯ä»¥å¤„ç†çš„æ•°å€¼å‹æ•°æ®ã€‚</li>
</ul>
<p>1-of-kç¼–ç æ˜¯æ•°æ®é¢„å¤„ç†ä¸­çš„ä¸€ç§é‡è¦æŠ€æœ¯ï¼Œå¹¿æ³›åº”ç”¨äºå„ç§æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ ä»»åŠ¡ä¸­ã€‚å®ƒå¸®åŠ©å°†åˆ†ç±»æ•°æ®è½¬åŒ–ä¸ºæ•°å€¼æ•°æ®ï¼Œä½¿å¾—å„ç§æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†è¿™äº›æ•°æ®ã€‚</p>
<h2 id="5-ç½®ä¿¡åº¦"><a href="#5-ç½®ä¿¡åº¦" class="headerlink" title="5. ç½®ä¿¡åº¦"></a>5. ç½®ä¿¡åº¦</h2><p>åœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œç½®ä¿¡åº¦ï¼ˆconfidenceï¼‰æ˜¯è¡¡é‡æ¨¡å‹é¢„æµ‹ç»“æœç¡®å®šæ€§çš„ä¸€ä¸ªæŒ‡æ ‡ã€‚ç½®ä¿¡åº¦å¯ä»¥ç†è§£ä¸ºæ¨¡å‹å¯¹å…¶é¢„æµ‹çš„æŸä¸ªç»“æœæ˜¯æ­£ç¡®çš„ä¿¡å¿ƒç¨‹åº¦ã€‚</p>
<p>åœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œç½®ä¿¡åº¦é€šå¸¸ä»¥æ¦‚ç‡å€¼çš„å½¢å¼è¡¨ç¤ºï¼Œåæ˜ äº†æ¨¡å‹å¯¹æŸä¸ªé¢„æµ‹çš„ç¡®å®šæ€§ã€‚ä¾‹å¦‚ï¼Œåœ¨åˆ†ç±»ä»»åŠ¡ä¸­ï¼Œæ¨¡å‹å¯¹æŸä¸ªæ ·æœ¬å±äºæŸä¸€ç±»çš„ç½®ä¿¡åº¦å¯èƒ½æ˜¯80%ï¼Œè¡¨ç¤ºæ¨¡å‹è®¤ä¸ºè¯¥æ ·æœ¬å±äºè¯¥ç±»çš„æ¦‚ç‡ä¸º80%ã€‚<br>å…·ä½“åº”ç”¨ï¼š</p>
<ul>
<li><strong>åˆ†ç±»ä»»åŠ¡</strong>ï¼šå¦‚å›¾åƒåˆ†ç±»ï¼Œæ¨¡å‹è¾“å‡ºæ¯ä¸ªç±»åˆ«çš„æ¦‚ç‡åˆ†å¸ƒã€‚ä¾‹å¦‚ï¼Œå¯¹äºä¸€å¼ å›¾ç‰‡ï¼Œæ¨¡å‹å¯èƒ½è¾“å‡ºï¼š[çŒ«: 0.7, ç‹—: 0.2, å…”å­: 0.1]ï¼Œå…¶ä¸­çŒ«çš„ç½®ä¿¡åº¦æœ€é«˜ã€‚</li>
<li><p><strong>ç½®ä¿¡åº¦é˜ˆå€¼</strong>ï¼šåœ¨æŸäº›åº”ç”¨ä¸­ï¼Œå¯èƒ½ä¼šè®¾ç½®ä¸€ä¸ªç½®ä¿¡åº¦é˜ˆå€¼ï¼Œåªæœ‰å½“é¢„æµ‹çš„ç½®ä¿¡åº¦è¶…è¿‡æŸä¸ªå€¼æ—¶ï¼Œæ‰è®¤ä¸ºé¢„æµ‹æœ‰æ•ˆã€‚</p>
<h4 id="ç½®ä¿¡åº¦çš„è®¡ç®—"><a href="#ç½®ä¿¡åº¦çš„è®¡ç®—" class="headerlink" title="ç½®ä¿¡åº¦çš„è®¡ç®—"></a>ç½®ä¿¡åº¦çš„è®¡ç®—</h4></li>
<li><p><strong>æ¦‚ç‡åˆ†å¸ƒ</strong>ï¼šä½¿ç”¨Softmaxå‡½æ•°å°†æ¨¡å‹è¾“å‡ºçš„logitsè½¬åŒ–ä¸ºæ¦‚ç‡åˆ†å¸ƒï¼Œè¿™äº›æ¦‚ç‡å€¼å³ä¸ºç½®ä¿¡åº¦ã€‚</p>
</li>
<li><strong>Softmaxå…¬å¼</strong>ï¼šå¯¹äºç¬¬ $i$ ä¸ªè¾“å‡ºèŠ‚ç‚¹ï¼Œç½®ä¿¡åº¦ $P(y_i)$ ä¸ºï¼š $P(y_i) = \frac{e^{z_i}}{\sum_{j} e^{z_j}}$ å…¶ä¸­ï¼Œ $z_i$ æ˜¯ç¬¬ $i$ ä¸ªèŠ‚ç‚¹çš„logitã€‚<h3 id="logits"><a href="#Logits" class="headerlink" title="Logits"></a>Logits</h3></li>
</ul>
<p>Logits æ˜¯ç¥ç»ç½‘ç»œåœ¨åˆ†ç±»ä»»åŠ¡ä¸­çš„è¾“å‡ºå±‚çš„åŸå§‹å¾—åˆ†ã€‚å®ƒä»¬æ˜¯æœªç»è¿‡å½’ä¸€åŒ–çš„æ•°å€¼ï¼Œç”¨äºè¡¨ç¤ºæ¯ä¸ªç±»åˆ«çš„ç›¸å¯¹ç½®ä¿¡åº¦ã€‚Logits æ˜¯é€šè¿‡å‰å‘ä¼ æ’­è®¡ç®—å¾—åˆ°çš„ï¼Œè¡¨ç¤ºæ¨¡å‹å¯¹æ¯ä¸ªå¯èƒ½çš„ç±»åˆ«çš„åˆå§‹ä¼°è®¡ã€‚</p>
<h4 id="å…·ä½“ç†è§£"><a href="#å…·ä½“ç†è§£" class="headerlink" title="å…·ä½“ç†è§£"></a>å…·ä½“ç†è§£</h4><ol>
<li><strong>åŸå§‹å¾—åˆ†</strong>ï¼šLogits æ˜¯ç¥ç»ç½‘ç»œæœ€åä¸€å±‚è¾“å‡ºçš„åŸå§‹åˆ†æ•°ï¼Œè¿™äº›åˆ†æ•°è¿˜æ²¡æœ‰è¢«è½¬æ¢ä¸ºæ¦‚ç‡ã€‚</li>
<li><strong>ç”¨é€”</strong>ï¼šåœ¨åˆ†ç±»ä»»åŠ¡ä¸­ï¼ŒLogits è¢«ç”¨æ¥è®¡ç®—æ¯ä¸ªç±»åˆ«çš„æ¦‚ç‡ï¼Œè¿™é€šå¸¸é€šè¿‡ Softmax å‡½æ•°å®Œæˆã€‚</li>
<li><strong>Softmax è½¬æ¢</strong>ï¼šSoftmax å‡½æ•°å°† Logits è½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒï¼Œä½¿å¾—è¿™äº›æ¦‚ç‡çš„å’Œä¸º1ã€‚å…¬å¼å¦‚ä¸Š</li>
</ol>
<h2 id="6-softmax"><a href="#6-Softmax" class="headerlink" title="6. Softmax"></a>6. Softmax</h2><p>[[0-ç¥ç»ç½‘ç»œï¼ˆNeural Networksï¼‰#^cfe178]]</p>
<h1 id="7-è¶…å‚æ•°-temperature-æ¸©åº¦"><a href="#7-è¶…å‚æ•°-Temperature-æ¸©åº¦" class="headerlink" title="7. è¶…å‚æ•° Temperature æ¸©åº¦"></a>7. è¶…å‚æ•° Temperature æ¸©åº¦</h1><p>Temperatureæ˜¯åœ¨æ¨¡å‹ä½¿ç”¨æˆ–è°ƒä¼˜è¿‡ç¨‹ä¸­è®¾å®šçš„å‚æ•°ï¼Œå¹¶ä¸é€šè¿‡æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­çš„ä¼˜åŒ–ç®—æ³•æ¥æ›´æ–°ï¼Œå› æ­¤å®ƒå±äºè¶…å‚æ•°ã€‚<br>Temperatureä½œä¸ºè¶…å‚æ•°ï¼Œåœ¨ç¥ç»ç½‘ç»œä¸­ç”¨æ¥æ§åˆ¶Softmaxè¾“å‡ºçš„æ¦‚ç‡åˆ†å¸ƒå¹³æ»‘ç¨‹åº¦ï¼Œå®ƒçš„ä½œç”¨æ˜¯è°ƒèŠ‚æ¨¡å‹ç”Ÿæˆæ ·æœ¬çš„å¤šæ ·æ€§å’Œç¡®å®šæ€§ã€‚</p>
<h4 id="temperatureçš„å…¬å¼"><a href="#Temperatureçš„å…¬å¼" class="headerlink" title="Temperatureçš„å…¬å¼"></a>Temperatureçš„å…¬å¼</h4><p>Softmaxå‡½æ•°å¸¦æœ‰Temperatureçš„å…¬å¼å¦‚ä¸‹ï¼š</p>
<p>$P(y_i) = \frac{e^{z_i / T}}{\sum_{j} e^{z_j / T}}$</p>
<p>å…¶ä¸­ï¼Œ$z_i$ æ˜¯logitså€¼ï¼Œ$T$ æ˜¯Temperatureå‚æ•°ã€‚</p>
<h4 id="temperatureçš„å½±å“"><a href="#Temperatureçš„å½±å“" class="headerlink" title="Temperatureçš„å½±å“"></a>Temperatureçš„å½±å“</h4><ol>
<li><p><strong>é«˜Temperatureï¼ˆT &gt; 1ï¼‰</strong>ï¼š</p>
<ul>
<li><strong>å¹³æ»‘åˆ†å¸ƒ</strong>ï¼šä½¿æ¦‚ç‡åˆ†å¸ƒæ›´åŠ å¹³æ»‘ï¼Œå¢åŠ ç”Ÿæˆæ ·æœ¬çš„å¤šæ ·æ€§ã€‚</li>
<li><strong>å¢åŠ éšæœºæ€§</strong>ï¼šè¾“å‡ºç±»åˆ«ä¹‹é—´çš„æ¦‚ç‡å·®è·ç¼©å°ï¼Œä½¿å¾—é€‰æ‹©æ›´éšæœºã€‚</li>
</ul>
<p>ç¤ºä¾‹ï¼šå‡è®¾ logits ä¸º $[2.0,1.0,0.1]$ï¼Œä½¿ç”¨ $T = 2$ æ—¶ï¼Œæ¦‚ç‡å¯èƒ½å˜å¾—æ›´æ¥è¿‘ï¼Œå¦‚ $[0.4,0.35,0.25]$ã€‚</p>
</li>
<li><p><strong>ä½Temperatureï¼ˆ0 &lt; T &lt; 1ï¼‰</strong>ï¼š</p>
<ul>
<li><strong>å°–é”åˆ†å¸ƒ</strong>ï¼šä½¿æ¦‚ç‡åˆ†å¸ƒæ›´åŠ å°–é”ï¼Œå¢åŠ ç”Ÿæˆæ ·æœ¬çš„ç¡®å®šæ€§ã€‚</li>
<li><strong>å‡å°‘éšæœºæ€§</strong>ï¼šè¾“å‡ºç±»åˆ«ä¹‹é—´çš„æ¦‚ç‡å·®è·åŠ å¤§ï¼Œä½¿å¾—é€‰æ‹©æ›´ç¡®å®šã€‚</li>
</ul>
<p>ç¤ºä¾‹ï¼šå‡è®¾ logits ä¸º $[2.0,1.0,0.1]$ï¼Œä½¿ç”¨ $T = 0.5$ æ—¶ï¼Œæ¦‚ç‡å¯èƒ½å˜å¾—æ›´å°–é”ï¼Œå¦‚ $[0.7,0.25,0.05]$ã€‚</p>
</li>
<li><p><strong>Temperatureç­‰äº1</strong>ï¼š</p>
<ul>
<li><strong>æ ‡å‡†Softmax</strong>ï¼šæ¦‚ç‡åˆ†å¸ƒä¿æŒåŸæ ·ï¼Œä¸åšä»»ä½•è°ƒæ•´ã€‚</li>
</ul>
</li>
</ol>
<h4 id="ä½œç”¨åŠåº”ç”¨åœºæ™¯"><a href="#ä½œç”¨åŠåº”ç”¨åœºæ™¯" class="headerlink" title="ä½œç”¨åŠåº”ç”¨åœºæ™¯"></a>ä½œç”¨åŠåº”ç”¨åœºæ™¯</h4><ol>
<li><strong>æ–‡æœ¬ç”Ÿæˆ</strong>ï¼š<ul>
<li>æ§åˆ¶ç”Ÿæˆæ–‡æœ¬çš„å¤šæ ·æ€§å’Œåˆ›é€ æ€§ã€‚åœ¨æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸­ï¼Œé«˜Temperatureå¯èƒ½ç”Ÿæˆæ›´æœ‰åˆ›æ„ä½†ä¸ä¸€å®šåˆç†çš„å¥å­ï¼Œè€Œä½Temperatureå¯èƒ½ç”Ÿæˆæ›´åˆç†ä½†ç¼ºä¹å¤šæ ·æ€§çš„å¥å­ã€‚</li>
</ul>
</li>
<li><strong>æ¢ç´¢ä¸åˆ©ç”¨</strong>ï¼š<ul>
<li>åœ¨å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œé«˜Temperatureç”¨äºæ¢ç´¢å¤šæ ·æ€§ç­–ç•¥ï¼Œä½Temperatureç”¨äºåˆ©ç”¨å·²çŸ¥çš„æœ€ä½³ç­–ç•¥ã€‚</li>
</ul>
</li>
</ol>

<div class="article-footer fs14">
    <section id="license">
      <div class="header"><span>è®¸å¯åè®®</span></div>
      <div class="body"><p>æœ¬æ–‡é‡‡ç”¨ <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">ç½²å-éå•†ä¸šæ€§ä½¿ç”¨-ç›¸åŒæ–¹å¼å…±äº« 4.0 å›½é™…</a> è®¸å¯åè®®ï¼Œè½¬è½½è¯·æ³¨æ˜å‡ºå¤„ã€‚</p>
</div>
    </section>
    </div>
</article>
<div class="related-wrap" id="read-next"><section class="body"><div class="item" id="prev"><div class="note">è¾ƒæ–°æ–‡ç« </div><a href="/7057a5e3/">RECURRENT NEURAL NETWORK REGULARIZATION</a></div><div class="item" id="next"><div class="note">è¾ƒæ—©æ–‡ç« </div><a href="/6ac941eb/">é€’å½’ç¥ç»ç½‘ç»œï¼ˆRecurrent Neural Networks, RNNsï¼‰</a></div></section></div>






<footer class="page-footer footnote"><hr><div class="text"><p>æœ¬ç«™ç”± <a href="/">Sun Yan</a> ä½¿ç”¨ <a target="_blank" rel="noopener" href="https://github.com/xaoxuu/hexo-theme-stellar/tree/1.28.1">Stellar 1.28.1</a> ä¸»é¢˜åˆ›å»ºã€‚<br>æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ«å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨ <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> è®¸å¯åè®®ï¼Œè½¬è½½è¯·æ³¨æ˜å‡ºå¤„ã€‚</p>
</div></footer>
<div class="main-mask" onclick="sidebar.dismiss()"></div></div><aside class="l_right">
<div class="widgets">



<widget class="widget-wrapper toc" id="data-toc" collapse="false"><div class="widget-header dis-select"><span class="name">æœ¬æ–‡ç›®å½•</span><a class="cap-action" onclick="sidebar.toggleTOC()" ><svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6h11m-11 6h11m-11 6h11M4 6h1v4m-1 0h2m0 8H4c0-1 2-2 2-3s-1-1.5-2-1"/></svg></a></div><div class="widget-body"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#recurrent-neural-networks-%E9%80%92%E5%BD%92%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-text">Recurrent Neural Networks  é€’å½’ç¥ç»ç½‘ç»œ</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#sequences"><span class="toc-text">Sequences</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#rnn-computation"><span class="toc-text">RNN computation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#going-deep"><span class="toc-text">Going deep</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#getting-fancy"><span class="toc-text">Getting fancy</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#character-level-language-models"><span class="toc-text">Character-Level Language Models</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#fun-with-rnns"><span class="toc-text">Fun with RNNs</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#paul-graham-generator-%E4%BF%9D%E7%BD%97%E6%A0%BC%E9%9B%B7%E5%8E%84%E5%A7%86%E5%8F%91%E7%94%B5%E6%9C%BA"><span class="toc-text">Paul Graham generator ä¿ç½—Â·æ ¼é›·å„å§†å‘ç”µæœº</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#shakespeare-%E8%8E%8E%E5%A3%AB%E6%AF%94%E4%BA%9A"><span class="toc-text">Shakespeare èå£«æ¯”äºš</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#wikipedia-%E7%BB%B4%E5%9F%BA%E7%99%BE%E7%A7%91"><span class="toc-text">Wikipedia ç»´åŸºç™¾ç§‘</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#algebraic-geometry-latex-%E4%BB%A3%E6%95%B0%E5%87%A0%E4%BD%95latex"><span class="toc-text">Algebraic Geometry (Latex)  ä»£æ•°å‡ ä½•ï¼ˆLatexï¼‰</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#linux-source-code-linux-%E6%BA%90%E4%BB%A3%E7%A0%81"><span class="toc-text">Linux Source Code Linux æºä»£ç </span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#generating-baby-names-%E7%94%9F%E6%88%90%E5%A9%B4%E5%84%BF%E5%90%8D%E5%AD%97"><span class="toc-text">Generating Baby Names ç”Ÿæˆå©´å„¿åå­—</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#understanding-whats-going-on"><span class="toc-text">Understanding whatâ€™s going on</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#the-evolution-of-samples-while-training"><span class="toc-text">The evolution of samples while training</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#visualizing-the-predictions-and-the-neuron-firings-in-the-rnn"><span class="toc-text">Visualizing the predictions and the â€œneuronâ€ firings in the RNN</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#source-code-%E6%BA%90%E4%BB%A3%E7%A0%81"><span class="toc-text">Source Code æºä»£ç </span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#further-reading-%E5%BB%B6%E4%BC%B8%E9%98%85%E8%AF%BB"><span class="toc-text">Further Reading å»¶ä¼¸é˜…è¯»</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#conclusion-%E7%BB%93%E8%AE%BA"><span class="toc-text">Conclusion ç»“è®º</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#edit-extra-links-%E7%BC%96%E8%BE%91%E9%A2%9D%E5%A4%96%E9%93%BE%E6%8E%A5"><span class="toc-text">EDIT (extra links): ç¼–è¾‘ï¼ˆé¢å¤–é“¾æ¥ï¼‰</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%B3%A8%E9%87%8A"><span class="toc-text">æ³¨é‡Š</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1vanilla-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-text">1.Vanilla ç¥ç»ç½‘ç»œ</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E5%90%91%E9%87%8Fvectorvs-%E5%BA%8F%E5%88%97sequence"><span class="toc-text">2. å‘é‡ï¼ˆVectorï¼‰vs åºåˆ—ï¼ˆSequenceï¼‰</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-rnn-computation"><span class="toc-text">3. RNN computation</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%9D%83%E9%87%8D%E7%9F%A9%E9%98%B5"><span class="toc-text">æƒé‡çŸ©é˜µ</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%BF%87%E7%A8%8B"><span class="toc-text">å‰å‘ä¼ æ’­è¿‡ç¨‹</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E8%BE%93%E5%85%A5%E5%B1%82%E5%88%B0%E9%9A%90%E8%97%8F%E5%B1%82"><span class="toc-text">1. è¾“å…¥å±‚åˆ°éšè—å±‚</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E9%9A%90%E8%97%8F%E5%B1%82%E5%88%B0%E8%BE%93%E5%87%BA%E5%B1%82"><span class="toc-text">2.  éšè—å±‚åˆ°è¾“å‡ºå±‚</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-1-of-k%E7%BC%96%E7%A0%81"><span class="toc-text">4. 1-of-kç¼–ç </span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86"><span class="toc-text">å·¥ä½œåŸç†</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BC%98%E7%82%B9"><span class="toc-text">ä¼˜ç‚¹</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BC%BA%E7%82%B9"><span class="toc-text">ç¼ºç‚¹</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8"><span class="toc-text">å®é™…åº”ç”¨</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-%E7%BD%AE%E4%BF%A1%E5%BA%A6"><span class="toc-text">5. ç½®ä¿¡åº¦</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BD%AE%E4%BF%A1%E5%BA%A6%E7%9A%84%E8%AE%A1%E7%AE%97"><span class="toc-text">ç½®ä¿¡åº¦çš„è®¡ç®—</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#logits"><span class="toc-text">Logits</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%85%B7%E4%BD%93%E7%90%86%E8%A7%A3"><span class="toc-text">å…·ä½“ç†è§£</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-softmax"><span class="toc-text">6. Softmax</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#7-%E8%B6%85%E5%8F%82%E6%95%B0-temperature-%E6%B8%A9%E5%BA%A6"><span class="toc-text">7. è¶…å‚æ•° Temperature æ¸©åº¦</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#temperature%E7%9A%84%E5%85%AC%E5%BC%8F"><span class="toc-text">Temperatureçš„å…¬å¼</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#temperature%E7%9A%84%E5%BD%B1%E5%93%8D"><span class="toc-text">Temperatureçš„å½±å“</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BD%9C%E7%94%A8%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF"><span class="toc-text">ä½œç”¨åŠåº”ç”¨åœºæ™¯</span></a></li></ol></li></ol></li></ol></li></ol></div><div class="widget-footer">

<a class="top" onclick="util.scrollTop()"><svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 24 24"><g fill="none" stroke="currentColor" stroke-width="1.5"><path d="M2 12c0-4.714 0-7.071 1.464-8.536C4.93 2 7.286 2 12 2c4.714 0 7.071 0 8.535 1.464C22 4.93 22 7.286 22 12c0 4.714 0 7.071-1.465 8.535C19.072 22 16.714 22 12 22s-7.071 0-8.536-1.465C2 19.072 2 16.714 2 12Z"/><path stroke-linecap="round" stroke-linejoin="round" d="m9 15.5l3-3l3 3m-6-4l3-3l3 3"/></g></svg><span>å›åˆ°é¡¶éƒ¨</span></a><a class="buttom" onclick="util.scrollComment()"><svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M10.46 1.25h3.08c1.603 0 2.86 0 3.864.095c1.023.098 1.861.3 2.6.752a5.75 5.75 0 0 1 1.899 1.899c.452.738.654 1.577.752 2.6c.095 1.004.095 2.261.095 3.865v1.067c0 1.141 0 2.036-.05 2.759c-.05.735-.153 1.347-.388 1.913a5.75 5.75 0 0 1-3.112 3.112c-.805.334-1.721.408-2.977.43a10.81 10.81 0 0 0-.929.036c-.198.022-.275.054-.32.08c-.047.028-.112.078-.224.232c-.121.166-.258.396-.476.764l-.542.916c-.773 1.307-2.69 1.307-3.464 0l-.542-.916a10.605 10.605 0 0 0-.476-.764c-.112-.154-.177-.204-.224-.232c-.045-.026-.122-.058-.32-.08c-.212-.023-.49-.03-.93-.037c-1.255-.021-2.171-.095-2.976-.429A5.75 5.75 0 0 1 1.688 16.2c-.235-.566-.338-1.178-.389-1.913c-.049-.723-.049-1.618-.049-2.76v-1.066c0-1.604 0-2.86.095-3.865c.098-1.023.3-1.862.752-2.6a5.75 5.75 0 0 1 1.899-1.899c.738-.452 1.577-.654 2.6-.752C7.6 1.25 8.857 1.25 10.461 1.25M6.739 2.839c-.914.087-1.495.253-1.959.537A4.25 4.25 0 0 0 3.376 4.78c-.284.464-.45 1.045-.537 1.96c-.088.924-.089 2.11-.089 3.761v1c0 1.175 0 2.019.046 2.685c.045.659.131 1.089.278 1.441a4.25 4.25 0 0 0 2.3 2.3c.515.214 1.173.294 2.429.316h.031c.398.007.747.013 1.037.045c.311.035.616.104.909.274c.29.17.5.395.682.645c.169.232.342.525.538.856l.559.944a.52.52 0 0 0 .882 0l.559-.944c.196-.331.37-.624.538-.856c.182-.25.392-.476.682-.645c.293-.17.598-.24.909-.274c.29-.032.639-.038 1.037-.045h.032c1.255-.022 1.913-.102 2.428-.316a4.25 4.25 0 0 0 2.3-2.3c.147-.352.233-.782.278-1.441c.046-.666.046-1.51.046-2.685v-1c0-1.651 0-2.837-.089-3.762c-.087-.914-.253-1.495-.537-1.959a4.25 4.25 0 0 0-1.403-1.403c-.464-.284-1.045-.45-1.96-.537c-.924-.088-2.11-.089-3.761-.089h-3c-1.651 0-2.837 0-3.762.089" clip-rule="evenodd"/><path fill="currentColor" d="M9 11a1 1 0 1 1-2 0a1 1 0 0 1 2 0m4 0a1 1 0 1 1-2 0a1 1 0 0 1 2 0m4 0a1 1 0 1 1-2 0a1 1 0 0 1 2 0"/></svg><span>å‚ä¸è®¨è®º</span></a></div></widget>
</div></aside><div class='float-panel blur'>
  <button type='button' style='display:none' class='laptop-only rightbar-toggle mobile' onclick='sidebar.rightbar()'>
    <svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6h11m-11 6h11m-11 6h11M4 6h1v4m-1 0h2m0 8H4c0-1 2-2 2-3s-1-1.5-2-1"/></svg>
  </button>
  <button type='button' style='display:none' class='mobile-only leftbar-toggle mobile' onclick='sidebar.leftbar()'>
    <svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 24 24"><g fill="none" stroke="currentColor" stroke-width="1.5"><path d="M2 11c0-3.771 0-5.657 1.172-6.828C4.343 3 6.229 3 10 3h4c3.771 0 5.657 0 6.828 1.172C22 5.343 22 7.229 22 11v2c0 3.771 0 5.657-1.172 6.828C19.657 21 17.771 21 14 21h-4c-3.771 0-5.657 0-6.828-1.172C2 18.657 2 16.771 2 13z"/><path id="sep" stroke-linecap="round" d="M5.5 10h6m-5 4h4m4.5 7V3"/></g></svg>
  </button>
</div>
</div><div class="scripts">
<script type="text/javascript">
  const ctx = {
    date_suffix: {
      just: `åˆšåˆš`,
      min: `åˆ†é’Ÿå‰`,
      hour: `å°æ—¶å‰`,
      day: `å¤©å‰`,
    },
    root : `/`,
  };

  // required plugins (only load if needs)
  if (`local_search`) {
    ctx.search = {};
    ctx.search.service = `local_search`;
    if (ctx.search.service == 'local_search') {
      let service_obj = Object.assign({}, `{"field":"all","path":"/search.json","content":true,"sort":"-date"}`);
      ctx.search[ctx.search.service] = service_obj;
    }
  }
  const def = {
    avatar: `https://gcore.jsdelivr.net/gh/cdn-x/placeholder@1.0.12/avatar/round/3442075.svg`,
    cover: `https://gcore.jsdelivr.net/gh/cdn-x/placeholder@1.0.12/cover/76b86c0226ffd.svg`,
  };
  const deps = {
    jquery: `https://cdn.bootcdn.net/ajax/libs/jquery/3.7.1/jquery.min.js`,
    marked: `https://cdn.bootcdn.net/ajax/libs/marked/4.0.18/marked.min.js`
  }
  

</script>

<script type="text/javascript">
  const utils = {
    // æ‡’åŠ è½½ css https://github.com/filamentgroup/loadCSS
    css: (href, before, media, attributes) => {
      var doc = window.document;
      var ss = doc.createElement("link");
      var ref;
      if (before) {
        ref = before;
      } else {
        var refs = (doc.body || doc.getElementsByTagName("head")[0]).childNodes;
        ref = refs[refs.length - 1];
      }
      var sheets = doc.styleSheets;
      if (attributes) {
        for (var attributeName in attributes) {
          if (attributes.hasOwnProperty(attributeName)) {
            ss.setAttribute(attributeName, attributes[attributeName]);
          }
        }
      }
      ss.rel = "stylesheet";
      ss.href = href;
      ss.media = "only x";
      function ready(cb) {
        if (doc.body) {
          return cb();
        }
        setTimeout(function () {
          ready(cb);
        });
      }
      ready(function () {
        ref.parentNode.insertBefore(ss, before ? ref : ref.nextSibling);
      });
      var onloadcssdefined = function (cb) {
        var resolvedHref = ss.href;
        var i = sheets.length;
        while (i--) {
          if (sheets[i].href === resolvedHref) {
            return cb();
          }
        }
        setTimeout(function () {
          onloadcssdefined(cb);
        });
      };
      function loadCB() {
        if (ss.addEventListener) {
          ss.removeEventListener("load", loadCB);
        }
        ss.media = media || "all";
      }
      if (ss.addEventListener) {
        ss.addEventListener("load", loadCB);
      }
      ss.onloadcssdefined = onloadcssdefined;
      onloadcssdefined(loadCB);
      return ss;
    },

    js: (src, opt) => new Promise((resolve, reject) => {
      var script = document.createElement('script');
      if (src.startsWith('/')){
        src = ctx.root + src.substring(1);
      }
      script.src = src;
      if (opt) {
        for (let key of Object.keys(opt)) {
          script[key] = opt[key]
        }
      } else {
        // é»˜è®¤å¼‚æ­¥ï¼Œå¦‚æœéœ€è¦åŒæ­¥ï¼Œç¬¬äºŒä¸ªå‚æ•°ä¼ å…¥ {} å³å¯
        script.async = true
      }
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    }),

    jq: (fn) => {
      if (typeof jQuery === 'undefined') {
        utils.js(deps.jquery).then(fn)
      } else {
        fn()
      }
    },
    
    onLoading: (el) => {
      if (el) {
        $(el).append('<div class="loading-wrap"><svg xmlns="http://www.w3.org/2000/svg" width="2em" height="2em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><g fill="none" stroke="currentColor" stroke-linecap="round" stroke-width="2"><path stroke-dasharray="60" stroke-dashoffset="60" stroke-opacity=".3" d="M12 3C16.9706 3 21 7.02944 21 12C21 16.9706 16.9706 21 12 21C7.02944 21 3 16.9706 3 12C3 7.02944 7.02944 3 12 3Z"><animate fill="freeze" attributeName="stroke-dashoffset" dur="1.3s" values="60;0"/></path><path stroke-dasharray="15" stroke-dashoffset="15" d="M12 3C16.9706 3 21 7.02944 21 12"><animate fill="freeze" attributeName="stroke-dashoffset" dur="0.3s" values="15;0"/><animateTransform attributeName="transform" dur="1.5s" repeatCount="indefinite" type="rotate" values="0 12 12;360 12 12"/></path></g></svg></div>');
      }
    },
    onLoadSuccess: (el) => {
      if (el) {
        $(el).find('.loading-wrap').remove();
      }
    },
    onLoadFailure: (el) => {
      if (el) {
        $(el).find('.loading-wrap svg').remove();
        $(el).find('.loading-wrap').append('<svg xmlns="http://www.w3.org/2000/svg" width="2em" height="2em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><g fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2"><path stroke-dasharray="60" stroke-dashoffset="60" d="M12 3L21 20H3L12 3Z"><animate fill="freeze" attributeName="stroke-dashoffset" dur="0.5s" values="60;0"/></path><path stroke-dasharray="6" stroke-dashoffset="6" d="M12 10V14"><animate fill="freeze" attributeName="stroke-dashoffset" begin="0.6s" dur="0.2s" values="6;0"/></path></g><circle cx="12" cy="17" r="1" fill="currentColor" fill-opacity="0"><animate fill="freeze" attributeName="fill-opacity" begin="0.8s" dur="0.4s" values="0;1"/></circle></svg>');
        $(el).find('.loading-wrap').addClass('error');
      }
    },
    request: (el, url, callback, onFailure) => {
      let retryTimes = 3;
      utils.onLoading(el);
      function req() {
        return new Promise((resolve, reject) => {
          let status = 0; // 0 ç­‰å¾… 1 å®Œæˆ 2 è¶…æ—¶
          let timer = setTimeout(() => {
            if (status === 0) {
              status = 2;
              timer = null;
              reject('è¯·æ±‚è¶…æ—¶');
              if (retryTimes == 0) {
                onFailure();
              }
            }
          }, 5000);
          fetch(url).then(function(response) {
            if (status !== 2) {
              clearTimeout(timer);
              resolve(response);
              timer = null;
              status = 1;
            }
            if (response.ok) {
              return response.json();
            }
            throw new Error('Network response was not ok.');
          }).then(function(data) {
            retryTimes = 0;
            utils.onLoadSuccess(el);
            callback(data);
          }).catch(function(error) {
            if (retryTimes > 0) {
              retryTimes -= 1;
              setTimeout(() => {
                req();
              }, 5000);
            } else {
              utils.onLoadFailure(el);
              onFailure();
            }
          });
        });
      }
      req();
    },
  };
</script>

<script>
  const sidebar = {
    leftbar: () => {
      if (l_body) {
        l_body.toggleAttribute('leftbar');
        l_body.removeAttribute('rightbar');
      }
    },
    rightbar: () => {
      if (l_body) {
        l_body.toggleAttribute('rightbar');
        l_body.removeAttribute('leftbar');
      }
    },
    dismiss: () => {
      if (l_body) {
        l_body.removeAttribute('leftbar');
        l_body.removeAttribute('rightbar');
      }
    },
    toggleTOC: () => {
      document.querySelector('#data-toc').classList.toggle('collapse');
    }
  }
</script>

<!-- required -->
<script src="/js/main.js?v=1.28.1" async></script>

<!-- optional -->



<script defer>
  window.addEventListener('DOMContentLoaded', (event) => {
    ctx.services = Object.assign({}, JSON.parse(`{"mdrender":{"js":"/js/services/mdrender.js"},"siteinfo":{"js":"/js/services/siteinfo.js","api":null},"ghinfo":{"js":"/js/services/ghinfo.js"},"sites":{"js":"/js/services/sites.js"},"friends":{"js":"/js/services/friends.js"},"timeline":{"js":"/js/services/timeline.js"},"fcircle":{"js":"/js/services/fcircle.js"},"weibo":{"js":"/js/services/weibo.js"},"memos":{"js":"/js/services/memos.js"}}`));
    for (let id of Object.keys(ctx.services)) {
      const js = ctx.services[id].js;
      if (id == 'siteinfo') {
        ctx.cardlinks = document.querySelectorAll('a.link-card[cardlink]');
        if (ctx.cardlinks?.length > 0) {
          utils.js(js, { defer: true }).then(function () {
            setCardLink(ctx.cardlinks);
          });
        }
      } else {
        const els = document.getElementsByClassName(`ds-${id}`);
        if (els?.length > 0) {
          utils.jq(() => {
            if (id == 'timeline' || 'memos' || 'marked') {
              utils.js(deps.marked).then(function () {
                utils.js(js, { defer: true });
              });
            } else {
              utils.js(js, { defer: true });
            }
          });
        }
      }
    }
  });
</script>

<script>
  window.addEventListener('DOMContentLoaded', (event) => {
    ctx.search = {
      path: `/search.json`,
    }
    utils.js('/js/search/local-search.js', { defer: true });
  });
</script><script>
  window.FPConfig = {
    delay: 0,
    ignoreKeywords: [],
    maxRPS: 5,
    hoverDelay: 25
  };
</script>
<script defer src="https://cdn.bootcdn.net/ajax/libs/flying-pages/2.1.2/flying-pages.min.js"></script><script defer src="https://cdn.bootcdn.net/ajax/libs/vanilla-lazyload/17.8.4/lazyload.min.js"></script>
<script>
  // https://www.npmjs.com/package/vanilla-lazyload
  // Set the options globally
  // to make LazyLoad self-initialize
  window.lazyLoadOptions = {
    elements_selector: ".lazy",
  };
  // Listen to the initialization event
  // and get the instance of LazyLoad
  window.addEventListener(
    "LazyLoad::Initialized",
    function (event) {
      window.lazyLoadInstance = event.detail.instance;
    },
    false
  );
  document.addEventListener('DOMContentLoaded', function () {
    window.lazyLoadInstance?.update();
  });
</script><script>
  ctx.fancybox = {
    selector: `.timenode p>img`,
    css: `https://cdn.bootcdn.net/ajax/libs/fancyapps-ui/5.0.22/fancybox/fancybox.min.css`,
    js: `https://cdn.bootcdn.net/ajax/libs/fancyapps-ui/5.0.22/fancybox/fancybox.umd.min.js`
  };
  var selector = '[data-fancybox]:not(.error)';
  if (ctx.fancybox.selector) {
    selector += `, ${ctx.fancybox.selector}`
  }
  var needFancybox = document.querySelectorAll(selector).length !== 0;
  if (!needFancybox) {
    const els = document.getElementsByClassName('ds-memos');
    if (els != undefined && els.length > 0) {
      needFancybox = true;
    }
  }
  if (needFancybox) {
    utils.css(ctx.fancybox.css);
    utils.js(ctx.fancybox.js, { defer: true }).then(function () {
      Fancybox.bind(selector, {
        hideScrollbar: false,
        Thumbs: {
          autoStart: false,
        },
        caption: (fancybox, slide) => {
          return slide.triggerEl.alt || slide.triggerEl.dataset.caption || null
        }
      });
    })
  }
</script>
<script>
  window.addEventListener('DOMContentLoaded', (event) => {
    const swiper_api = document.getElementById('swiper-api');
    if (swiper_api != undefined) {
      utils.css(`https://unpkg.com/swiper@10.3.1/swiper-bundle.min.css`);
      utils.js(`https://unpkg.com/swiper@10.3.1/swiper-bundle.min.js`, { defer: true }).then(function () {
        const effect = swiper_api.getAttribute('effect') || '';
        var swiper = new Swiper('.swiper#swiper-api', {
          slidesPerView: 'auto',
          spaceBetween: 8,
          centeredSlides: true,
          effect: effect,
          rewind: true,
          pagination: {
            el: '.swiper-pagination',
            clickable: true,
          },
          navigation: {
            nextEl: '.swiper-button-next',
            prevEl: '.swiper-button-prev',
          },
        });
      })
    }
  });
</script>
<script>
  document.addEventListener('DOMContentLoaded', function () {
    window.codeElements = document.querySelectorAll('.code');
    if (window.codeElements.length > 0) {
      ctx.copycode = {
        default_text: `Copy`,
        success_text: `Copied`,
        toast: `å¤åˆ¶æˆåŠŸ`,
      };
      utils.js('/js/plugins/copycode.js');
    }
  });
</script>


<!-- inject -->

</div><script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\(","\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="//cdn.bootcss.com/mathjax/2.7.3/MathJax.js?config=TeX-MML-AM_CHTML"></script>

</body></html>
