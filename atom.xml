<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Sun Yan</title>
  
  
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2024-07-12T05:35:52.145Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>Sun Yan</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>理解 Spring FacrotyBean</title>
    <link href="http://example.com/b4b9d0ea/"/>
    <id>http://example.com/b4b9d0ea/</id>
    <published>2024-07-12T05:24:52.000Z</published>
    <updated>2024-07-12T05:35:52.145Z</updated>
    
    <content type="html"><![CDATA[<p>FactoryBean 和BeanFactory 名字太像了，以至于容易混淆，但是从功能上讲完全不一样， 理解了它们各自的功能就可以完美区分这俩了</p><h1 id="1-beanfactory"><a href="#1-BeanFactory" class="headerlink" title="1. BeanFactory"></a>1. BeanFactory</h1><p>Spring IOC  容器，用于管理bean ,具体细节已经在<a href="https://sunyan.xyz/678b23b2/">Spring bean 实例化</a>中介绍过，就不再赘述，可以点击去阅读。</p><p>总的来说BeanFactory 时Spring 容器，<br>在Spring的启动过程，FactoryBean 是一个会被BeanFactory 管理的bean , 但是它相比其他普通业务bean,又有一些特殊的功能，比如生产出复杂配置的bean.</p><p>接下来详细介绍FactoryBean</p><h1 id="2-factorybean-是一个接口"><a href="#2-FactoryBean-是一个接口" class="headerlink" title="2. FactoryBean 是一个接口"></a>2. FactoryBean 是一个接口</h1><ul><li><strong><code>FactoryBean</code></strong> 接口提供了一种灵活的方式来创建复杂的对象。</li><li>实现 <code>FactoryBean</code> 接口的类，可以通过其 <code>getObject</code> 方法来创建并返回实际的对象实例。<code>FactoryBean</code> 可以用于创建单例、原型、代理对象或其他复杂的初始化逻辑。</li></ul><p><code>FactoryBean</code> 接口定义如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">interface</span> <span class="title class_">FactoryBean</span>&lt;T&gt; &#123;</span><br><span class="line">    T <span class="title function_">getObject</span><span class="params">()</span> <span class="keyword">throws</span> Exception;</span><br><span class="line">    Class&lt;?&gt; getObjectType();</span><br><span class="line">    <span class="type">boolean</span> <span class="title function_">isSingleton</span><span class="params">()</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><code>T getObject() throws Exception</code>：这个方法返回由 <code>FactoryBean</code> 创建的对象。这个对象可以是一个普通的 Bean，也可以是一个代理对象或其他复杂对象。</li><li><code>Class&lt;?&gt; getObjectType()</code>：这个方法返回由 <code>FactoryBean</code> 创建的对象的类型。Spring 使用这个信息来确定 Bean 的类型。</li><li><code>boolean isSingleton()</code>：这个方法指示由 <code>FactoryBean</code> 创建的对象是否是单例（singleton）。如果返回 <code>true</code>，Spring 容器会缓存该实例。</li></ul><h1 id="3-spring-容器启动过程中的factorybean"><a href="#3-Spring-容器启动过程中的FactoryBean" class="headerlink" title="3. Spring 容器启动过程中的FactoryBean"></a>3. Spring 容器启动过程中的FactoryBean</h1><p>接下来我们将通过 分析Spring  启动过程中 <code>FactoryBean</code>的实例化以及通过<code>FactoryBean.getObject</code> 的过程来加深对<code>FactoryBean</code>的理解。<br>这里我们用 <a href="https://sunyan.xyz/a10675df/">Spring AOP XML配置方式原理详解</a> 中的示例代码进行讲解</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">UserService</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">createUser</span><span class="params">(String username)</span> &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 模拟业务处理</span></span><br><span class="line">        System.out.println(<span class="string">&quot;Creating user: &quot;</span> + username);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">deleteUser</span><span class="params">(String username)</span> &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 模拟业务处理</span></span><br><span class="line">        System.out.println(<span class="string">&quot;Deleting user: &quot;</span> + username);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 定义一个前置增强</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">LoggingBeforeAdvice</span> <span class="keyword">implements</span> <span class="title class_">MethodBeforeAdvice</span> &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">before</span><span class="params">(Method method, Object[] args, Object target)</span> <span class="keyword">throws</span> Throwable &#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;Before method: &quot;</span> + method.getName());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 定义一个后置增强</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">LoggingAfterAdvice</span> <span class="keyword">implements</span> <span class="title class_">AfterReturningAdvice</span> &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">afterReturning</span><span class="params">(Object returnValue, Method method, Object[] args, Object target)</span> <span class="keyword">throws</span> Throwable &#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;After method: &quot;</span> + method.getName());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 定义一个 环绕增强</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">LoggingMethodInterceptor</span> <span class="keyword">implements</span> <span class="title class_">MethodInterceptor</span> &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> Object <span class="title function_">invoke</span><span class="params">(MethodInvocation invocation)</span> <span class="keyword">throws</span> Throwable &#123;</span><br><span class="line">        <span class="comment">// 方法执行前逻辑</span></span><br><span class="line">        <span class="type">long</span> <span class="variable">startTime</span> <span class="operator">=</span> System.currentTimeMillis();</span><br><span class="line">        System.out.println(invocation.getMethod().getName()+ <span class="string">&quot; 方法开始执行&quot;</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 通过反射执行目标方法</span></span><br><span class="line">        <span class="type">Object</span> <span class="variable">result</span> <span class="operator">=</span> invocation.proceed();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 方法执行后逻辑</span></span><br><span class="line">        <span class="comment">//System.out.println(&quot;After method: &quot; + invocation.getMethod().getName());</span></span><br><span class="line">        <span class="type">long</span> <span class="variable">endTime</span> <span class="operator">=</span> System.currentTimeMillis();</span><br><span class="line">        System.out.println(invocation.getMethod().getName()+ <span class="string">&quot;方法执行时间: &quot;</span> + (endTime - startTime) + <span class="string">&quot;ms&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> result;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=<span class="string">&quot;1.0&quot;</span> encoding=<span class="string">&quot;UTF-8&quot;</span>?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">beans</span> <span class="attr">xmlns</span>=<span class="string">&quot;http://www.springframework.org/schema/beans&quot;</span></span></span><br><span class="line"><span class="tag">       <span class="attr">xmlns:xsi</span>=<span class="string">&quot;http://www.w3.org/2001/XMLSchema-instance&quot;</span></span></span><br><span class="line"><span class="tag">       <span class="attr">xmlns:aop</span>=<span class="string">&quot;http://www.springframework.org/schema/aop&quot;</span></span></span><br><span class="line"><span class="tag">       <span class="attr">xsi:schemaLocation</span>=<span class="string">&quot;http://www.springframework.org/schema/beans</span></span></span><br><span class="line"><span class="string"><span class="tag">                           http://www.springframework.org/schema/beans/spring-beans.xsd</span></span></span><br><span class="line"><span class="string"><span class="tag">                           http://www.springframework.org/schema/aop</span></span></span><br><span class="line"><span class="string"><span class="tag">                           http://www.springframework.org/schema/aop/spring-aop.xsd&quot;</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 定义业务类的 Bean --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">bean</span> <span class="attr">id</span>=<span class="string">&quot;userService&quot;</span> <span class="attr">class</span>=<span class="string">&quot;com.example.codingInAction.service.UserService&quot;</span>/&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 定义 前置增强 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">bean</span> <span class="attr">id</span>=<span class="string">&quot;loggingBeforeAdvice&quot;</span> <span class="attr">class</span>=<span class="string">&quot;com.example.codingInAction.aop.LoggingBeforeAdvice&quot;</span>/&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 定义 后置增强 --&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">bean</span> <span class="attr">id</span>=<span class="string">&quot;loggingAfterAdvice&quot;</span> <span class="attr">class</span>=<span class="string">&quot;com.example.codingInAction.aop.LoggingAfterAdvice&quot;</span>/&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 定义 环绕增强 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">bean</span> <span class="attr">id</span>=<span class="string">&quot;loggingMethodInterceptor&quot;</span> <span class="attr">class</span>=<span class="string">&quot;com.example.codingInAction.aop.LoggingMethodInterceptor&quot;</span>/&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 使用 ProxyFactoryBean 配置代理对象 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">bean</span> <span class="attr">id</span>=<span class="string">&quot;userServiceProxy&quot;</span> <span class="attr">class</span>=<span class="string">&quot;org.springframework.aop.framework.ProxyFactoryBean&quot;</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- 目标对象 --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">&quot;target&quot;</span> <span class="attr">ref</span>=<span class="string">&quot;userService&quot;</span>/&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- 拦截器 --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">&quot;interceptorNames&quot;</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">list</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>loggingMethodInterceptor<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>loggingBeforeAdvice<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>loggingAfterAdvice<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line">            <span class="tag">&lt;/<span class="name">list</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- 强制使用CGLIB代理 --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">&quot;proxyTargetClass&quot;</span> <span class="attr">value</span>=<span class="string">&quot;true&quot;</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">bean</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">beans</span>&gt;</span></span><br></pre></td></tr></table></figure><p>启动代码</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">CodingInActionApplication</span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="type">ApplicationContext</span> <span class="variable">context</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">ClassPathXmlApplicationContext</span>(<span class="string">&quot;applicationContext.xml&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取代理对象</span></span><br><span class="line">        <span class="type">UserService</span> <span class="variable">userService</span> <span class="operator">=</span> (UserService) context.getBean(<span class="string">&quot;userServiceProxy&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 调用方法，观察 AOP 切面的效果</span></span><br><span class="line">        userService.createUser(<span class="string">&quot;john&quot;</span>);</span><br><span class="line">        userService.deleteUser(<span class="string">&quot;john&quot;</span>);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="amp-前缀的作用"><a href="#amp-前缀的作用" class="headerlink" title="&amp; 前缀的作用"></a><code>&amp;</code> 前缀的作用</h2><ul><li>使用 <code>getBean(&quot;&amp;beanName&quot;)</code> 可以获取 <code>FactoryBean</code> 实例本身，而不是 <code>FactoryBean</code> 创建的对象。</li><li>使用 <code>getBean(&quot;beanName&quot;)</code> 则获取由 <code>FactoryBean</code> 创建的对象。</li></ul><h2 id="31-factorybean-也是一个bean"><a href="#3-1-FactoryBean-也是一个bean" class="headerlink" title="3.1 FactoryBean 也是一个bean"></a>3.1 FactoryBean 也是一个bean</h2><p>FactoryBean 也是一个bean, 和普通bean 使用相同的逻辑进行实例化，所以也是被beanFactory 管理的。</p><p>如代码所示<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">ApplicationContext</span> <span class="variable">context</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">ClassPathXmlApplicationContext</span>(<span class="string">&quot;applicationContext.xml&quot;</span>);</span><br></pre></td></tr></table></figure></p><p>在以上Spring  容器的启动过程中，<code>userServiceProxy</code>作为一个FactoryBean 会通过getBean触发实例化流程。<br>可以看到getBean传参的时候，beanName 前增加了 &amp; 标识，说明这个getBean 流程需要获取的是<code>userServiceProxy</code>这个个FactoryBean ，而不是由它创建的对象</p><p>在实例化 FactoryBean 的过程中，并不需要<code>&amp;</code>标识 ， 在getBean 是否，需要判断 是返回FactoryBean 实例 还是Factorybean创建的对象时才需要<code>&amp;</code>标识。</p><img src="/b4b9d0ea/2.png" class><h3 id="311-transformedbeanname-方法"><a href="#3-1-1-transformedBeanName-方法" class="headerlink" title="3.1.1 transformedBeanName 方法"></a>3.1.1 <code>transformedBeanName</code> 方法</h3><p>进入getBean, 可以看到namet经过了transformedBeanName方法处理， 又把&amp; 去掉了。</p><img src="/b4b9d0ea/1.png" class><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">String</span> <span class="variable">beanName</span> <span class="operator">=</span> transformedBeanName(name);</span><br></pre></td></tr></table></figure><p>这行代码的作用是将传入的 <code>name</code> 转换为实际的 Bean 名称,在 Spring 框架中，Bean 名称可能包含一些特殊的前缀或后缀，用于处理特殊情况或指示某种行为。<code>transformedBeanName</code> 方法用于解析和处理这些前缀或后缀，以获得实际的 Bean 名称。</p><p>在这个例子中就是去掉工厂 Bean 前缀 <code>&amp;</code>。</p><p><code>&amp;</code> 前缀的作用 告诉程序 ，我是来获取 <code>FactoryBean</code> 本身，而不是获取<code>FactoryBean</code>  创建的对象的。<br>所以 <code>&amp;</code>  本身不属于bean名称的一部分，需要经过<code>transformedBeanName</code> 方法会去掉这个前缀，以获得实际的 Bean 名称。</p><p>对于非FactoryBean 来讲， name 和beanName 都是一样的</p><h3 id="312-getobjectforbeaninstance"><a href="#3-1-2-getObjectForBeanInstance" class="headerlink" title="3.1.2 getObjectForBeanInstance"></a>3.1.2 getObjectForBeanInstance</h3><p>FactoryBean只有在实例化后才能通过getObject() 创建其他配置复杂的bean</p><p>在看IOC源码的时候，发现即使我们已经创建出来了对象的实例，还是要走一个方法再去处理下，这里就是对FactoryBean的处理，因为它可以产生对象，所以你getBean的时候取到的不是它本身，而是通过它生成的产品。【如果要取它本身，getBean(&amp;+beanName)】 我们先来回忆下IOC源码中那个处理FactoryBean的简略代码：<br>我们可以看到，无论是直接取单例的bean，还是创建单例、多例、自定义生命周期的bean，都会经过bean = getObjectForBeanInstance(sharedInstance, name, beanName, null);这个方法，我们现在就来看看这里到底是做了什么：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">protected</span> Object <span class="title function_">getObjectForBeanInstance</span><span class="params">(</span></span><br><span class="line"><span class="params">Object beanInstance, String name, String beanName, RootBeanDefinition mbd)</span> &#123;</span><br><span class="line"> </span><br><span class="line"><span class="comment">//如果是对FactoryBean的解引用，但bean对象不是FactoryBean，抛出异常</span></span><br><span class="line"><span class="keyword">if</span> (BeanFactoryUtils.isFactoryDereference(name) &amp;&amp; !(beanInstance <span class="keyword">instanceof</span> FactoryBean)) &#123;</span><br><span class="line"><span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">BeanIsNotAFactoryException</span>(transformedBeanName(name), beanInstance.getClass());</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"><span class="comment">//如果Bean实例不是FactoryBean，或者指定名称是FactoryBean的解引用，也就是普通的bean调用，则直接返回当前的Bean实例  </span></span><br><span class="line"><span class="keyword">if</span> (!(beanInstance <span class="keyword">instanceof</span> FactoryBean) || BeanFactoryUtils.isFactoryDereference(name)) &#123;</span><br><span class="line"><span class="keyword">return</span> beanInstance;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//处理对FactoryBean的调用</span></span><br><span class="line"><span class="type">Object</span> <span class="variable">object</span> <span class="operator">=</span> <span class="literal">null</span>;</span><br><span class="line"><span class="keyword">if</span> (mbd == <span class="literal">null</span>) &#123;</span><br><span class="line"><span class="comment">//从Bean工厂缓存中获取给定名称的实例对象</span></span><br><span class="line">object = getCachedObjectForFactoryBean(beanName);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> (object == <span class="literal">null</span>) &#123;</span><br><span class="line"><span class="comment">// Return bean instance from factory.</span></span><br><span class="line"><span class="type">FactoryBean</span> <span class="variable">factory</span> <span class="operator">=</span> (FactoryBean) beanInstance;</span><br><span class="line"><span class="comment">//如果从Bean工厂生产的Bean是单态模式的，则缓存</span></span><br><span class="line"><span class="keyword">if</span> (mbd == <span class="literal">null</span> &amp;&amp; containsBeanDefinition(beanName)) &#123;</span><br><span class="line">mbd = getMergedLocalBeanDefinition(beanName);</span><br><span class="line">&#125;</span><br><span class="line"><span class="type">boolean</span> <span class="variable">synthetic</span> <span class="operator">=</span> (mbd != <span class="literal">null</span> &amp;&amp; mbd.isSynthetic());</span><br><span class="line"><span class="comment">//调用FactoryBeanRegistrySupport类的getObjectFromFactoryBean方法，实现FactoryBean生产Bean对象实例的过程  </span></span><br><span class="line">object = getObjectFromFactoryBean(factory, beanName, !synthetic);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> object;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Bean工厂生产Bean实例对象</span></span><br><span class="line"><span class="keyword">protected</span> Object <span class="title function_">getObjectFromFactoryBean</span><span class="params">(FactoryBean factory,</span></span><br><span class="line"><span class="params">String beanName, <span class="type">boolean</span> shouldPostProcess)</span> &#123;</span><br><span class="line"><span class="comment">// Bean工厂是单态模式，并且Bean工厂缓存中存在指定名称的Bean实例对象</span></span><br><span class="line"><span class="keyword">if</span> (factory.isSingleton() &amp;&amp; containsSingleton(beanName)) &#123;</span><br><span class="line"><span class="keyword">synchronized</span> (getSingletonMutex()) &#123;</span><br><span class="line"><span class="comment">// 直接从Bean工厂缓存中获取指定名称的Bean实例对象</span></span><br><span class="line"><span class="type">Object</span> <span class="variable">object</span> <span class="operator">=</span> <span class="built_in">this</span>.factoryBeanObjectCache.get(beanName);</span><br><span class="line"><span class="comment">// Bean工厂缓存中没有指定名称的实例对象，则生产该实例对象</span></span><br><span class="line"><span class="keyword">if</span> (object == <span class="literal">null</span>) &#123;</span><br><span class="line"><span class="comment">// 调用Bean工厂的getObject方法生产指定Bean的实例对象</span></span><br><span class="line">object = doGetObjectFromFactoryBean(factory, beanName,</span><br><span class="line">shouldPostProcess);</span><br><span class="line"><span class="comment">// 将生产的实例对象添加到Bean工厂缓存中</span></span><br><span class="line"><span class="built_in">this</span>.factoryBeanObjectCache.put(beanName,</span><br><span class="line">(object != <span class="literal">null</span> ? object : NULL_OBJECT));</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> (object != NULL_OBJECT ? object : <span class="literal">null</span>);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 调用Bean工厂的getObject方法生产指定Bean的实例对象</span></span><br><span class="line"><span class="keyword">else</span> &#123;</span><br><span class="line"><span class="keyword">return</span> doGetObjectFromFactoryBean(factory, beanName,</span><br><span class="line">shouldPostProcess);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//调用Bean工厂的getObject方法生产指定Bean的实例对象  </span></span><br><span class="line">   <span class="keyword">private</span> Object <span class="title function_">doGetObjectFromFactoryBean</span><span class="params">(  </span></span><br><span class="line"><span class="params">           <span class="keyword">final</span> FactoryBean factory, <span class="keyword">final</span> String beanName, <span class="keyword">final</span> <span class="type">boolean</span> shouldPostProcess)</span>  </span><br><span class="line">           <span class="keyword">throws</span> BeanCreationException &#123;  </span><br><span class="line">       Object object;  </span><br><span class="line">       <span class="keyword">try</span> &#123;  </span><br><span class="line">           <span class="keyword">if</span> (System.getSecurityManager() != <span class="literal">null</span>) &#123;  </span><br><span class="line">               <span class="type">AccessControlContext</span> <span class="variable">acc</span> <span class="operator">=</span> getAccessControlContext();  </span><br><span class="line">               <span class="keyword">try</span> &#123;  </span><br><span class="line">                   <span class="comment">//实现PrivilegedExceptionAction接口的匿名内置类  </span></span><br><span class="line">                   <span class="comment">//根据JVM检查权限，然后决定BeanFactory创建实例对象  </span></span><br><span class="line">                   object = AccessController.doPrivileged(<span class="keyword">new</span> <span class="title class_">PrivilegedExceptionAction</span>&lt;Object&gt;() &#123;  </span><br><span class="line">                       <span class="keyword">public</span> Object <span class="title function_">run</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;  </span><br><span class="line">                               <span class="comment">//调用BeanFactory接口实现类的创建对象方法  </span></span><br><span class="line">                               <span class="keyword">return</span> factory.getObject();  </span><br><span class="line">                           &#125;  </span><br><span class="line">                       &#125;, acc);  </span><br><span class="line">               &#125;  </span><br><span class="line">               <span class="keyword">catch</span> (PrivilegedActionException pae) &#123;  </span><br><span class="line">                   <span class="keyword">throw</span> pae.getException();  </span><br><span class="line">               &#125;  </span><br><span class="line">           &#125;  </span><br><span class="line">           <span class="keyword">else</span> &#123;  </span><br><span class="line">           <span class="comment">//调用BeanFactory接口实现类的创建对象方法  </span></span><br><span class="line">               object = factory.getObject();  </span><br><span class="line">           &#125;  </span><br><span class="line">       &#125;  </span><br><span class="line">       <span class="keyword">catch</span> (FactoryBeanNotInitializedException ex) &#123;  </span><br><span class="line">           <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">BeanCurrentlyInCreationException</span>(beanName, ex.toString());  </span><br><span class="line">       &#125;  </span><br><span class="line">       <span class="keyword">catch</span> (Throwable ex) &#123;  </span><br><span class="line">           <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">BeanCreationException</span>(beanName, <span class="string">&quot;FactoryBean threw exception on object creation&quot;</span>, ex);  </span><br><span class="line">       &#125;  </span><br><span class="line">       <span class="comment">//创建出来的实例对象为null，或者因为单态对象正在创建而返回null  </span></span><br><span class="line">       <span class="keyword">if</span> (object == <span class="literal">null</span> &amp;&amp; isSingletonCurrentlyInCreation(beanName)) &#123;  </span><br><span class="line">           <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">BeanCurrentlyInCreationException</span>(  </span><br><span class="line">                   beanName, <span class="string">&quot;FactoryBean which is currently in creation returned null from getObject&quot;</span>);  </span><br><span class="line">       &#125;  </span><br><span class="line">       <span class="comment">//为创建出来的Bean实例对象添加BeanPostProcessor后置处理器  </span></span><br><span class="line">       <span class="keyword">if</span> (object != <span class="literal">null</span> &amp;&amp; shouldPostProcess) &#123;  </span><br><span class="line">           <span class="keyword">try</span> &#123;  </span><br><span class="line">               object = postProcessObjectFromFactoryBean(object, beanName);  </span><br><span class="line">           &#125;  </span><br><span class="line">           <span class="keyword">catch</span> (Throwable ex) &#123;  </span><br><span class="line">               <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">BeanCreationException</span>(beanName, <span class="string">&quot;Post-processing of the FactoryBean&#x27;s object failed&quot;</span>, ex);  </span><br><span class="line">           &#125;  </span><br><span class="line">       &#125;  </span><br><span class="line">       <span class="keyword">return</span> object;  </span><br><span class="line">   &#125; </span><br></pre></td></tr></table></figure><img src="/b4b9d0ea/3.png" class><p>经过<code>getObjectForBeanInstance</code> 处理，最终返回了<code>userServiceProxy</code>这个FactoryBean 本身<br><img src="/b4b9d0ea/4.png" class></p><h2 id="32-获取factorybean-创建的对象"><a href="#3-2-获取FactoryBean-创建的对象" class="headerlink" title="3.2 获取FactoryBean 创建的对象"></a>3.2 获取FactoryBean 创建的对象</h2><p>FactoryBean 本身已经实例化完成了，现在可以获取由它创建的对象了<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment">// 获取代理对象</span></span><br><span class="line"><span class="type">UserService</span> <span class="variable">userService</span> <span class="operator">=</span> (UserService) context.getBean(<span class="string">&quot;userServiceProxy&quot;</span>);</span><br></pre></td></tr></table></figure></p><p>由于userServiceProxy 是FactoryBean, 现在执行<code>getBean(&quot;beanName&quot;)</code> 说明我们要获取由它创建的对象了。下面跟着代码进行简单分析。</p><h3 id="321-transformedbeanname-方法"><a href="#3-2-1-transformedBeanName-方法" class="headerlink" title="3.2.1 transformedBeanName 方法"></a>3.2.1 <code>transformedBeanName</code> 方法</h3><p>进入源码，再次来到getBean 流程，可以看到，经过<code>transformedBeanName</code>处理后，name 和beanName 保持一致。</p><img src="/b4b9d0ea/5.png" class><h3 id="322-getobjectforbeaninstance"><a href="#3-2-2-getObjectForBeanInstance" class="headerlink" title="3.2.2 getObjectForBeanInstance"></a>3.2.2 getObjectForBeanInstance</h3><p>此时getSingleton,一级缓存中已经有了IOC 容器启动过程的中创建的实例， 直接获取即可。再次进入getObjectForBeanInstance 方法</p><img src="/b4b9d0ea/6.png" class><h3 id="323-factorybeangetobject"><a href="#3-2-3-FactoryBean-getObject" class="headerlink" title="3.2.3 FactoryBean.getObject"></a>3.2.3 FactoryBean.getObject</h3><p>在getObjectForBeanInstance 方法中，逐渐深入最终会进入到ProxyFactoryBean 重写的getObject 方法（getObject 是接口FactoryBean  中定义的方法还记得吗）</p><p>在getObject 方法中，可以获取userServiceProxy 创建的对象，即一个代理对象<br><img src="/b4b9d0ea/7.png" class></p><h1 id="4-factorybean-的应用场景"><a href="#4-FactoryBean-的应用场景" class="headerlink" title="4.  FactoryBean 的应用场景"></a>4.  <code>FactoryBean</code> 的应用场景</h1><p><code>FactoryBean</code> 可以用于各种高级应用场景，以下是一些典型的使用场景：</p><h2 id="41-创建代理对象"><a href="#4-1-创建代理对象" class="headerlink" title="4.1 创建代理对象"></a>4.1 创建代理对象</h2><p><code>FactoryBean</code> 常用于创建 AOP 代理对象。例如，Spring AOP 内部大量使用 <code>FactoryBean</code> 来创建代理对象。</p><p>在上面的动态代理Demo中，我们就展示了一个通过FactoryBean创建代理对象的复杂例子。</p><h2 id="42-创建复杂的-bean-实例"><a href="#4-2-创建复杂的-Bean-实例" class="headerlink" title="4.2 创建复杂的 Bean 实例"></a>4.2 创建复杂的 Bean 实例</h2><p>在某些情况下，创建一个 Bean 可能涉及复杂的初始化逻辑。通过 <code>FactoryBean</code>，我们可以将这些逻辑封装在 <code>getObject</code> 方法中，从而简化配置。</p><p>很多开源项目在集成Spring 时都使用到FactoryBean，比如 MyBatis3 提供 mybatis-spring项目中的 org.mybatis.spring.SqlSessionFactoryBean：<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- spring和MyBatis整合，不需要mybatis的配置映射文件 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">bean</span> <span class="attr">id</span>=<span class="string">&quot;sqlSessionFactory&quot;</span> <span class="attr">class</span>=<span class="string">&quot;org.mybatis.spring.SqlSessionFactoryBean&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">&quot;dataSource&quot;</span> <span class="attr">ref</span>=<span class="string">&quot;dataSource&quot;</span>/&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 自动扫描mapping.xml文件 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">&quot;mapperLocations&quot;</span> <span class="attr">value</span>=<span class="string">&quot;classpath:mapper/*.xml&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">bean</span>&gt;</span></span><br></pre></td></tr></table></figure></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">SqlSessionFactoryBean</span> <span class="keyword">implements</span> <span class="title class_">FactoryBean</span>&lt;SqlSessionFactory&gt;, InitializingBean, ApplicationListener&lt;ApplicationEvent&gt; &#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">Log</span> <span class="variable">LOGGER</span> <span class="operator">=</span> LogFactory.getLog(SqlSessionFactoryBean.class);</span><br><span class="line">...</span><br><span class="line"><span class="keyword">public</span> SqlSessionFactory <span class="title function_">getObject</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">this</span>.sqlSessionFactory == <span class="literal">null</span>) &#123;</span><br><span class="line">            <span class="built_in">this</span>.afterPropertiesSet();</span><br><span class="line">        &#125;</span><br><span class="line"> </span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">this</span>.sqlSessionFactory;</span><br><span class="line">    &#125;</span><br><span class="line">...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;FactoryBean 和BeanFactory 名字太像了，以至于容易混淆，但是从功能上讲完全不一样， 理解了它们各自的功能就可以完美区分这俩了&lt;/p&gt;
&lt;h1 id=&quot;1-beanfactory&quot;&gt;&lt;a href=&quot;#1-BeanFactory&quot; class=&quot;head</summary>
      
    
    
    
    
    <category term="java" scheme="http://example.com/tags/java/"/>
    
    <category term="Spring" scheme="http://example.com/tags/Spring/"/>
    
  </entry>
  
  <entry>
    <title>Spring 启动过程 拓展点</title>
    <link href="http://example.com/114991e5/"/>
    <id>http://example.com/114991e5/</id>
    <published>2024-07-11T09:18:57.946Z</published>
    <updated>2024-07-12T03:38:26.646Z</updated>
    
    <content type="html"><![CDATA[<p>Spring框架提供了丰富的扩展点，允许开发者在bean的生命周期的不同阶段插入自定义逻辑，从而增强应用的功能和灵活性。拓展点包括<code>BeanFactoryPostProcessor</code>、<code>BeanPostProcessor</code>、各种Aware等。下面来具体讲解各个拓展点的作用以及它们是如何介入到bean 的生命周期中。</p><p>除了<code>Aware</code>接口和<code>BeanPostProcessor</code>，还有许多其他扩展点，如<code>BeanFactoryPostProcessor</code>、<code>ApplicationListener</code>、<code>InitializingBean</code>、<code>FactoryBean</code>等。这些扩展点的合理使用可以极大地提高Spring应用的可扩展性和可维护性。</p><h2 id="1beanfactorypostprocessor"><a href="#1-BeanFactoryPostProcessor" class="headerlink" title="1.BeanFactoryPostProcessor"></a>1.BeanFactoryPostProcessor</h2><p><code>BeanFactoryPostProcessor</code> 是 Spring 框架中的一个函数式接口，它在bean 开始整个创建流程之前 起作用。</p><p>通过实现该接口可以在 Bean 开始整个创建流程之前读取 <code>BeanFactory</code> 中的 <code>BeanDefinition</code> 并进行修改，比如调整 Bean 的属性、依赖关系等定义。</p><p>Spring 自带了一些 <code>BeanFactoryPostProcessor</code> 的实现，用户也可以根据需要自定义实现以适应特定场景，从而实现更灵活的应用配置。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@FunctionalInterface</span>  </span><br><span class="line"><span class="keyword">public</span> <span class="keyword">interface</span> <span class="title class_">BeanFactoryPostProcessor</span> &#123;  </span><br><span class="line"><span class="keyword">void</span> <span class="title function_">postProcessBeanFactory</span><span class="params">(ConfigurableListableBeanFactory beanFactory)</span> <span class="keyword">throws</span> BeansException;  </span><br><span class="line">  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="11-注册时机-and-作用过程-invokebeanfactorypostprocessors"><a href="#1-1-注册时机-and-作用过程-invokeBeanFactoryPostProcessors" class="headerlink" title="1.1 注册时机 and 作用过程-invokeBeanFactoryPostProcessors"></a>1.1 注册时机 and 作用过程-invokeBeanFactoryPostProcessors</h3><p>前面说到<code>BeanFactoryPostProcessor</code> 是 Spring 框架在bean 开始整个创建流程之前 起作用。具体的位置如下,位于refresh 中的invokeBeanFactoryPostProcessors 方法中， 通过invoke 这个命名就可以知道这里需要调用所有<code>BeanFactoryPostProcessor</code> 的具体实现逻辑对bean 定义进行修改</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">AbstractApplicationContext.java</span><br><span class="line"><span class="meta">@Override</span>  </span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">refresh</span><span class="params">()</span> <span class="keyword">throws</span> BeansException, IllegalStateException &#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 1. 创建beanFactory   </span></span><br><span class="line"><span class="type">ConfigurableListableBeanFactory</span> <span class="variable">beanFactory</span> <span class="operator">=</span> obtainFreshBeanFactory();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2.BeanFactoryPostProcessor起作用</span></span><br><span class="line">invokeBeanFactoryPostProcessors(beanFactory);   </span><br><span class="line"></span><br><span class="line"><span class="comment">// 3.注册 BeanPostProcessor</span></span><br><span class="line">registerBeanPostProcessors(beanFactory);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 4. bean的生命周期管理和依赖关系的装配</span></span><br><span class="line">finishBeanFactoryInitialization(beanFactory);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>执行<code>BeanFactoryPostProcessor</code>逻辑前， 需要有<code>BeanFactoryPostProcessor</code>的实例，那么<code>BeanFactoryPostProcessor</code>是如何实例化的呢, 下面来看下 <code>BeanFactoryPostProcessor</code> 的实例化和具体起作用的过程</p><p>先进入<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">AbstractApplicationContext.java</span><br><span class="line"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">invokeBeanFactoryPostProcessors</span><span class="params">(ConfigurableListableBeanFactory beanFactory)</span> &#123;  PostProcessorRegistrationDelegate.invokeBeanFactoryPostProcessors(beanFactory, getBeanFactoryPostProcessors());    </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>再进入PostProcessorRegistrationDelegate.invokeBeanFactoryPostProcessors的方法，可以看到此时传进来了3个<code>BeanFactoryPostProcessor</code>，这3个<code>BeanFactoryPostProcessor</code> 是在应用上下文ApplicationContext 里面存储维护的，但是此时还没有看到自定义的</p><img src="/114991e5/1.png" class><p>来看一下PostProcessorRegistrationDelegate.invokeBeanFactoryPostProcessors 的具体处理逻辑.BeanFactoryPostProcessors将bean 分成了好几类分别进行处理，</p><h3 id="12-beandefinitionregistrypostprocessor"><a href="#1-2-BeanDefinitionRegistryPostProcessor" class="headerlink" title="1.2 BeanDefinitionRegistryPostProcessor"></a>1.2 BeanDefinitionRegistryPostProcessor</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">interface</span> <span class="title class_">BeanDefinitionRegistryPostProcessor</span> <span class="keyword">extends</span> <span class="title class_">BeanFactoryPostProcessor</span> &#123;  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">void</span> <span class="title function_">postProcessBeanDefinitionRegistry</span><span class="params">(BeanDefinitionRegistry registry)</span> <span class="keyword">throws</span> BeansException;  </span><br><span class="line">  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>BeanDefinitionRegistryPostProcessor 继承了BeanFactoryPostProcessor ， 并增加了一个方法， 在执行<code>BeanFactoryPostProcessor</code> 前，需要找出所有的<code>BeanDefinitionRegistryPostProcessor</code>, 并且先执行自己的<code>postProcessBeanDefinitionRegistry</code>方法，再执行继承来的<code>postProcessBeanFactory</code> 方法。<br>看以下重点代码,<code>BeanDefinitionRegistryPostProcessor</code>包括参数中传进来来的和从beanFactory 中找到的， 其中从beanFactory找到的BeanDefinitionRegistryPostProcessor，还没有进行实例化，需要实例化后才能使用，这个实例化的过程和业务bean 的实力化过程是同一个</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (beanFactory <span class="keyword">instanceof</span> BeanDefinitionRegistry) &#123;  </span><br><span class="line"><span class="type">BeanDefinitionRegistry</span> <span class="variable">registry</span> <span class="operator">=</span> (BeanDefinitionRegistry) beanFactory;  </span><br><span class="line">List&lt;BeanFactoryPostProcessor&gt; regularPostProcessors = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();  </span><br><span class="line">List&lt;BeanDefinitionRegistryPostProcessor&gt; registryProcessors = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();  </span><br><span class="line"><span class="comment">// 从传参中找到 BeanDefinitionRegistryPostProcessor </span></span><br><span class="line"><span class="keyword">for</span> (BeanFactoryPostProcessor postProcessor : beanFactoryPostProcessors) &#123;  </span><br><span class="line"><span class="keyword">if</span> (postProcessor <span class="keyword">instanceof</span> BeanDefinitionRegistryPostProcessor) &#123;  </span><br><span class="line"><span class="type">BeanDefinitionRegistryPostProcessor</span> <span class="variable">registryProcessor</span> <span class="operator">=</span>  </span><br><span class="line">(BeanDefinitionRegistryPostProcessor) postProcessor;  </span><br><span class="line">registryProcessor.postProcessBeanDefinitionRegistry(registry);  </span><br><span class="line">registryProcessors.add(registryProcessor);   </span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 从beanFactory 中找到BeanDefinitionRegistryPostProcessor并进行实例化</span></span><br><span class="line">postProcessorNames = beanFactory.getBeanNamesForType(BeanDefinitionRegistryPostProcessor.class, <span class="literal">true</span>, <span class="literal">false</span>);</span><br><span class="line"><span class="keyword">for</span> (String ppName : postProcessorNames) &#123;  </span><br><span class="line"><span class="keyword">if</span> (!processedBeans.contains(ppName) &amp;&amp; beanFactory.isTypeMatch(ppName, Ordered.class)) &#123;  </span><br><span class="line">currentRegistryProcessors.add(beanFactory.getBean(ppName, BeanDefinitionRegistryPostProcessor.class));  </span><br><span class="line">&#125;  </span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//先执行BeanDefinitionRegistryPostProcessor的postProcessBeanDefinitionRegistry方法</span></span><br><span class="line">invokeBeanDefinitionRegistryPostProcessors(currentRegistryProcessors, registry);</span><br><span class="line"><span class="comment">//再执行BeanFactoryPostProcessor的postProcessBeanFactory方法</span></span><br><span class="line">invokeBeanFactoryPostProcessors(registryProcessors, beanFactory);  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="13-执行顺序-ordered"><a href="#1-3-执行顺序-Ordered" class="headerlink" title="1.3 执行顺序- Ordered"></a>1.3 执行顺序- Ordered</h3><p>在invoke 完BeanDefinitionRegistryPostProcessor后，就开始按照优先级顺序处理BeanFactoryPostProcessor。</p><p>Spring提供了一些机制来管理多个<code>BeanFactoryPostProcessor</code>的执行顺序，其执行顺序分别是</p><ol><li><code>PriorityOrdered</code> 接口</li><li>优先级接口 <code>Ordered</code></li><li>NonOrdered</li></ol><h4 id="131-优先级接口-ordered"><a href="#1-3-1-优先级接口-Ordered" class="headerlink" title="1.3.1 优先级接口 Ordered"></a>1.3.1 优先级接口 <code>Ordered</code></h4><p>Spring允许<code>BeanFactoryPostProcessor</code>实现<code>Ordered</code>接口来指定执行顺序。<code>Ordered</code>接口要求实现一个方法<code>getOrder()</code>，该方法返回一个整数，定义了<code>BeanFactoryPostProcessor</code>的执行顺序。数值越小，优先级越高，越优先执行</p><p>例如，以下是一个<code>BeanFactoryPostProcessor</code>实现，它使用了<code>Ordered</code>接口来指定其执行顺序：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">  </span><br><span class="line"><span class="meta">@Component</span>  </span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">CustomBeanFactoryPostProcessor</span> <span class="keyword">implements</span> <span class="title class_">BeanFactoryPostProcessor</span>, Ordered &#123;  </span><br><span class="line">  </span><br><span class="line"><span class="meta">@Override</span>  </span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">postProcessBeanFactory</span><span class="params">(ConfigurableListableBeanFactory beanFactory)</span> <span class="keyword">throws</span> BeansException &#123;  </span><br><span class="line"><span class="comment">// 获取名为 &quot;myBean&quot; 的 BeanDefinitionBeanDefinition</span></span><br><span class="line">beanDefinition = beanFactory.getBeanDefinition(<span class="string">&quot;myBean&quot;</span>);  </span><br><span class="line">  </span><br><span class="line"><span class="comment">// 修改 &quot;myBean&quot; 的属性值  </span></span><br><span class="line">beanDefinition.getPropertyValues().add(<span class="string">&quot;name&quot;</span>, <span class="string">&quot;Modified by BeanFactoryPostProcessor&quot;</span>);  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"><span class="meta">@Override</span>  </span><br><span class="line"><span class="keyword">public</span> <span class="type">int</span> <span class="title function_">getOrder</span><span class="params">()</span> &#123;  </span><br><span class="line"><span class="keyword">return</span> <span class="number">10</span>; <span class="comment">// 数值越小，优先级越高  </span></span><br><span class="line">&#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="132-priorityordered-接口"><a href="#1-3-2-PriorityOrdered-接口" class="headerlink" title="1.3.2 PriorityOrdered 接口"></a>1.3.2 <strong><code>PriorityOrdered</code> 接口</strong></h4><p><code>PriorityOrdered</code>是<code>Ordered</code>接口的一个子接口，允许<code>BeanFactoryPostProcessor</code>在其他普通<code>Ordered</code>执行。这主要用于那些必须首先应用的处理器，比如那些涉及配置如何加载的处理器。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">CustomPriorityOrderedBeanFactoryPostProcessor</span> <span class="keyword">implements</span> <span class="title class_">BeanFactoryPostProcessor</span>, PriorityOrdered &#123;  </span><br><span class="line">  </span><br><span class="line"><span class="meta">@Override</span>  </span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">postProcessBeanFactory</span><span class="params">(ConfigurableListableBeanFactory beanFactory)</span> &#123;  </span><br><span class="line"><span class="comment">// 定制处理逻辑  </span></span><br><span class="line">System.out.println(<span class="string">&quot;Executing CustomPriorityOrderedBeanFactoryPostProcessor&quot;</span>);  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"><span class="meta">@Override</span>  </span><br><span class="line"><span class="keyword">public</span> <span class="type">int</span> <span class="title function_">getOrder</span><span class="params">()</span> &#123;  </span><br><span class="line"><span class="keyword">return</span> <span class="number">5</span>; <span class="comment">// 数值越小，优先级越高  </span></span><br><span class="line">&#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h4 id="133-nonordered"><a href="#1-3-3-NonOrdered" class="headerlink" title="1.3.3 NonOrdered"></a>1.3.3 NonOrdered</h4><p>NonOrdered并不是一个接口，指的是没有实现Ordered 或者 PriorityOrdered 的 <code>BeanFactoryPostProcessor</code>， 它会放在最后执行。</p><p>所以，综上，在Spring容器启动过程中，<code>BeanFactoryPostProcessor</code>的调用顺序如下：</p><ul><li>首先，实现<code>PriorityOrdered</code>的<code>BeanFactoryPostProcessor</code>按照它们的顺序值被调用。</li><li>其次，实现<code>Ordered</code>接口的普通<code>BeanFactoryPostProcessor</code>按照它们的顺序值被调用。</li><li>最后，未实现任何排序接口的<code>BeanFactoryPostProcessor</code>按照它们的注册顺序被调用。</li></ul><h4 id="134-处理逻辑"><a href="#1-3-4-处理逻辑" class="headerlink" title="1.3.4 处理逻辑"></a>1.3.4 处理逻辑</h4><p>在invoke 完BeanDefinitionRegistryPostProcessor后，就开始按照优先级顺序处理<code>BeanFactoryPostProcessor</code>。 代码逻辑会先找出所有的<code>BeanFactoryPostProcessor</code>， 然后将其分类为<code>PriorityOrderedBeanFactoryPostProcessor</code>、<code>OrderedBeanFactoryPostProcessor</code>、<code>NonOrderedBeanFactoryPostProcessor</code>, 并按照优先级顺序分别invoke <code>postProcessBeanFactory</code> 方法</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">invokeBeanFactoryPostProcessors</span><span class="params">(  </span></span><br><span class="line"><span class="params">ConfigurableListableBeanFactory beanFactory, List&lt;BeanFactoryPostProcessor&gt; beanFactoryPostProcessors)</span> &#123;</span><br><span class="line"><span class="comment">// 找出所有的 BeanFactoryPostProcessor</span></span><br><span class="line">String[] postProcessorNames =  </span><br><span class="line">beanFactory.getBeanNamesForType(BeanFactoryPostProcessor.class, <span class="literal">true</span>, <span class="literal">false</span>);</span><br><span class="line"><span class="comment">// 分类</span></span><br><span class="line"><span class="keyword">for</span> (String ppName : postProcessorNames) &#123; </span><br><span class="line"><span class="comment">// processedBeans保存已处理的BeanFactoryPostProcessor 名称 </span></span><br><span class="line"><span class="keyword">if</span> (processedBeans.contains(ppName)) &#123;  </span><br><span class="line"><span class="comment">// skip - already processed in first phase above  </span></span><br><span class="line">&#125;  </span><br><span class="line"><span class="keyword">else</span> <span class="keyword">if</span> (beanFactory.isTypeMatch(ppName, PriorityOrdered.class)) &#123;  priorityOrderedPostProcessors.add(beanFactory.getBean(ppName, BeanFactoryPostProcessor.class));  </span><br><span class="line">&#125;  </span><br><span class="line"><span class="keyword">else</span> <span class="keyword">if</span> (beanFactory.isTypeMatch(ppName, Ordered.class)) &#123; </span><br><span class="line"><span class="comment">// orderedPostProcessor 在后面也会通过getBean实例化，逻辑比较简单，我省略了，nonOrderedPostProcessor同理 </span></span><br><span class="line">orderedPostProcessorNames.add(ppName);  </span><br><span class="line">&#125;  </span><br><span class="line"><span class="keyword">else</span> &#123;  </span><br><span class="line">nonOrderedPostProcessorNames.add(ppName);  </span><br><span class="line">&#125;  </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 分别invoke postProcessBeanFactory方法</span></span><br><span class="line">invokeBeanFactoryPostProcessors(priorityOrderedPostProcessors, beanFactory);</span><br><span class="line">invokeBeanFactoryPostProcessors(orderedPostProcessors, beanFactory);</span><br><span class="line">invokeBeanFactoryPostProcessors(nonOrderedPostProcessors, beanFactory);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="14-beanfactorypostprocessor也是bean"><a href="#1-4-BeanFactoryPostProcessor也是bean" class="headerlink" title="1.4 BeanFactoryPostProcessor也是bean"></a>1.4 BeanFactoryPostProcessor也是bean</h3><p>无论是 <code>BeanDefinitionRegistryPostProcessor</code>,还是<code>BeanFactoryPostProcessor</code>,    在通过<code>getBeanNamesForType</code>获取名称后，invoke <code>postProcessBeanFactory</code>  方法之前，需要先进行实例化操作（都要调用方法了，肯定要有对象存在啊）。 BeanFactoryPostProcessor 的实例化过程和业务bean 的实例化过程 完全相同，都是getBean流程完成实例化操作。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 实例化 BeanDefinitionRegistryPostProcessor</span></span><br><span class="line">postProcessorNames = beanFactory.getBeanNamesForType(BeanDefinitionRegistryPostProcessor.class, <span class="literal">true</span>, <span class="literal">false</span>);</span><br><span class="line"></span><br><span class="line">currentRegistryProcessors.add(beanFactory.getBean(ppName, BeanDefinitionRegistryPostProcessor.class)); </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 实例化 BeanFactoryPostProcessor</span></span><br><span class="line">String[] postProcessorNames =  </span><br><span class="line">beanFactory.getBeanNamesForType(BeanFactoryPostProcessor.class, <span class="literal">true</span>, <span class="literal">false</span>);</span><br><span class="line"></span><br><span class="line">priorityOrderedPostProcessors.add(beanFactory.getBean(ppName, BeanFactoryPostProcessor.class));</span><br><span class="line"></span><br><span class="line">orderedPostProcessors.add(beanFactory.getBean(postProcessorName, BeanFactoryPostProcessor.class));</span><br><span class="line"></span><br><span class="line">nonOrderedPostProcessors.add(beanFactory.getBean(postProcessorName, BeanFactoryPostProcessor.class));</span><br><span class="line"></span><br></pre></td></tr></table></figure><img src="/114991e5/2.png" class><img src="/114991e5/3.png" class><img src="/114991e5/4.png" class><h2 id="2beanpostprocessor"><a href="#2-BeanPostProcessor" class="headerlink" title="2.BeanPostProcessor"></a>2.BeanPostProcessor</h2><p><code>BeanPostProcessor</code>是Spring框架中一个非常强大的扩展点，提供的一个接口，用于在bean的init操作前后执行自定义逻辑。Spring AOP就是通过<code>BeanPostProcessor</code>实现的</p><p><code>BeanPostProcessor</code>也是一个接口，包含2个方法</p><ul><li><strong>postProcessBeforeInitialization</strong>: 在bean init之前执行。</li><li><strong>postProcessAfterInitialization</strong>: 在bean init 之后执行。</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">interface</span> <span class="title class_">BeanPostProcessor</span> &#123;</span><br><span class="line">    Object <span class="title function_">postProcessBeforeInitialization</span><span class="params">(Object bean, String beanName)</span> <span class="keyword">throws</span> BeansException;</span><br><span class="line">    Object <span class="title function_">postProcessAfterInitialization</span><span class="params">(Object bean, String beanName)</span> <span class="keyword">throws</span> BeansException;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="21-注册时机-registerbeanpostprocessors"><a href="#2-1-注册时机-registerBeanPostProcessors" class="headerlink" title="2.1 注册时机-registerBeanPostProcessors"></a>2.1 注册时机-registerBeanPostProcessors</h3><p>依然回到<code>refresh</code>方法中 ，其中<code>registerBeanPostProcessors</code> 方法涉及到<code>BeanPostProcessor</code>的注册流程。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">AbstractApplicationContext.java</span><br><span class="line"><span class="meta">@Override</span>  </span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">refresh</span><span class="params">()</span> <span class="keyword">throws</span> BeansException, IllegalStateException &#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 1. 创建beanFactory   </span></span><br><span class="line"><span class="type">ConfigurableListableBeanFactory</span> <span class="variable">beanFactory</span> <span class="operator">=</span> obtainFreshBeanFactory();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2.BeanFactoryPostProcessor起作用</span></span><br><span class="line">invokeBeanFactoryPostProcessors(beanFactory);   </span><br><span class="line"></span><br><span class="line"><span class="comment">// 3.注册 BeanPostProcessor</span></span><br><span class="line">registerBeanPostProcessors(beanFactory);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 4. bean的生命周期管理和依赖关系的装配</span></span><br><span class="line">finishBeanFactoryInitialization(beanFactory);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>进入<code>registerBeanPostProcessors</code>分析具体代码</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">PostProcessorRegistrationDelegate.java</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">registerBeanPostProcessors</span><span class="params">(  </span></span><br><span class="line"><span class="params">ConfigurableListableBeanFactory beanFactory, AbstractApplicationContext applicationContext)</span> &#123;</span><br><span class="line"><span class="comment">// 找出所有 BeanPostProcessor 名称</span></span><br><span class="line">String[] postProcessorNames = beanFactory.getBeanNamesForType(BeanPostProcessor.class, <span class="literal">true</span>, <span class="literal">false</span>);</span><br><span class="line"><span class="comment">//按照优先级别 对BeanPostProcessor 名称 </span></span><br><span class="line">List&lt;BeanPostProcessor&gt; priorityOrderedPostProcessors = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();  </span><br><span class="line">List&lt;BeanPostProcessor&gt; internalPostProcessors = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();  </span><br><span class="line">List&lt;String&gt; orderedPostProcessorNames = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();  </span><br><span class="line">List&lt;String&gt; nonOrderedPostProcessorNames = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();  </span><br><span class="line"><span class="keyword">for</span> (String ppName : postProcessorNames) &#123;  </span><br><span class="line"><span class="keyword">if</span> (beanFactory.isTypeMatch(ppName, PriorityOrdered.class)) &#123;  </span><br><span class="line"><span class="type">BeanPostProcessor</span> <span class="variable">pp</span> <span class="operator">=</span> beanFactory.getBean(ppName, BeanPostProcessor.class);  </span><br><span class="line">priorityOrderedPostProcessors.add(pp);  </span><br><span class="line"></span><br><span class="line">&#125;  </span><br><span class="line"><span class="keyword">else</span> <span class="keyword">if</span> (beanFactory.isTypeMatch(ppName, Ordered.class)) &#123;  </span><br><span class="line"><span class="comment">// orderedPostProcessor 在后面也会通过getBean 实例化，逻辑比较简单，我省略了，nonOrderedPostProcessor同理</span></span><br><span class="line">orderedPostProcessorNames.add(ppName);  </span><br><span class="line">&#125;  </span><br><span class="line"><span class="keyword">else</span> &#123;  </span><br><span class="line">nonOrderedPostProcessorNames.add(ppName);  </span><br><span class="line">&#125;  </span><br><span class="line">&#125;</span><br><span class="line">registerBeanPostProcessors(beanFactory, priorityOrderedPostProcessors);</span><br><span class="line">registerBeanPostProcessors(beanFactory, orderedPostProcessors);</span><br><span class="line">registerBeanPostProcessors(beanFactory, internalPostProcessors);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以看到registerBeanPostProcessors 的整理流程比较类似，都是</p><ol><li>用beanFactory.getBeanNamesForType找出所有名字</li><li>分类</li><li>getBean 实例化</li></ol><p>一个比较明显的区别是 <code>BeanFactoryPostProcessor</code>在实例化后会直接调用接口postProcessBeanFactory执行逻辑，但是<code>BeanPostProcessor</code> 的实例只会先添加到beanFactory 中，等到业务bean的init 阶段才会执行逻辑。</p><p>由此可见 注册BeanFactoryPostProcessor 用到的是<code>invokeBeanFactoryPostProcessors</code>，<code>BeanPostProcessor</code> 只是<code>registerBeanPostProcessors</code>， 这里的方法命名还是相当准确的</p><h3 id="22-执行顺序-ordered"><a href="#2-2-执行顺序-Ordered" class="headerlink" title="2.2 执行顺序-Ordered"></a>2.2 执行顺序-Ordered</h3><p>从上面的代码中可以看到 Spring 管理多个<code>BeanPostProcessor</code>的执行顺序，其实现和<code>BeanFactoryPostProcessor</code> 完全一致，</p><ol><li><code>PriorityOrdered</code> 接口</li><li>优先级接口 <code>Ordered</code></li><li>NonOrdered</li></ol><p>这里给出一些实际代码示例就不再赘述了。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">MyBeanPostProcessor</span> <span class="keyword">implements</span> <span class="title class_">BeanPostProcessor</span>, Ordered &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> Object <span class="title function_">postProcessBeforeInitialization</span><span class="params">(Object bean, String beanName)</span> <span class="keyword">throws</span> BeansException &#123;</span><br><span class="line">        <span class="comment">// 自定义逻辑</span></span><br><span class="line">        <span class="keyword">return</span> bean;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> Object <span class="title function_">postProcessAfterInitialization</span><span class="params">(Object bean, String beanName)</span> <span class="keyword">throws</span> BeansException &#123;</span><br><span class="line">        <span class="comment">// 自定义逻辑</span></span><br><span class="line">        <span class="keyword">return</span> bean;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">getOrder</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">10</span>; <span class="comment">// 返回一个数值来指定执行顺序</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">PriorityLoggingBeanPostProcessor</span> <span class="keyword">implements</span> <span class="title class_">BeanPostProcessor</span>, PriorityOrdered &#123;</span><br><span class="line">    </span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> Object <span class="title function_">postProcessBeforeInitialization</span><span class="params">(Object bean, String beanName)</span> <span class="keyword">throws</span> BeansException &#123;</span><br><span class="line">        <span class="comment">// 在bean初始化之前打印日志</span></span><br><span class="line">        System.out.println(<span class="string">&quot;Before Initializing Bean &#x27;&quot;</span> + beanName + <span class="string">&quot;&#x27;: &quot;</span> + bean.getClass().getSimpleName());</span><br><span class="line">        <span class="keyword">return</span> bean;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> Object <span class="title function_">postProcessAfterInitialization</span><span class="params">(Object bean, String beanName)</span> <span class="keyword">throws</span> BeansException &#123;</span><br><span class="line">        <span class="comment">// 初始化后的处理可以在这里添加，此示例仅在初始化前打印</span></span><br><span class="line">        <span class="keyword">return</span> bean;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">getOrder</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="comment">// 返回值较低表示优先级较高，此处返回最高优先级</span></span><br><span class="line">        <span class="keyword">return</span> PriorityOrdered.HIGHEST_PRECEDENCE;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h3 id="23-beanpostprocessor也是bean"><a href="#2-3-BeanPostProcessor也是bean" class="headerlink" title="2.3 BeanPostProcessor也是bean"></a>2.3 BeanPostProcessor也是bean</h3><p>通过registerBeanPostProcessors 代码可知， <code>BeanPostProcessor</code>和<code>Bean FactoryPostProcessor</code> 一样, 通过getbean 进行实例化的<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">BeanPostProcessor pp = beanFactory.getBean(ppName, BeanPostProcessor.class); </span><br></pre></td></tr></table></figure></p><p>并在实例化后添加到beanFactory 中进行管理<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">PostProcessorRegistrationDelegate.java</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">registerBeanPostProcessors</span><span class="params">(  </span></span><br><span class="line"><span class="params">ConfigurableListableBeanFactory beanFactory, List&lt;BeanPostProcessor&gt; postProcessors)</span> &#123;  </span><br><span class="line"><span class="keyword">for</span> (BeanPostProcessor postProcessor : postProcessors) &#123;  </span><br><span class="line">beanFactory.addBeanPostProcessor(postProcessor);  </span><br><span class="line">&#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h3 id="24-spring-启动过程中会用到的-beanpostprocessor"><a href="#2-4-Spring-启动过程中会用到的-BeanPostProcessor" class="headerlink" title="2.4 Spring 启动过程中会用到的 BeanPostProcessor"></a>2.4 Spring 启动过程中会用到的 BeanPostProcessor</h3><h4 id="241-instantiationawarebeanpostprocessor"><a href="#2-4-1-InstantiationAwareBeanPostProcessor" class="headerlink" title="2.4.1 InstantiationAwareBeanPostProcessor"></a>2.4.1 InstantiationAwareBeanPostProcessor</h4><img src="/114991e5/5.png" class><p>图中置灰的2个方法，是继承自BeanPostProcessor的2个方法</p><p>InstantiationAwareBeanPostProcessor自有的3个方法中有2个方法名称和BeanPostProcessor中方式名称相同，但是入参和返回值略有不同</p><ol><li>postProcessBeforeInstantiation<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">@Nullable  </span><br><span class="line">default Object postProcessBeforeInstantiation(Class&lt;?&gt; beanClass, String beanName) throws BeansException &#123;  </span><br><span class="line">return null;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol><p>在bean 的实例化之前，会调用这个方法，其逻辑存在于下面代码中<code>resolveBeforeInstantiation</code>方法中。 这个方法可以返回一个代理对象，来替代 Bean 的默认实例化过程。但通常情况下，代理对象是在 Bean 实例化之后创建的，所以这个方法一般返回 <code>null</code>。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">@Override  </span><br><span class="line">protected Object createBean(String beanName, RootBeanDefinition mbd, @Nullable Object[] args)  </span><br><span class="line">throws BeanCreationException &#123;</span><br><span class="line">Object bean = resolveBeforeInstantiation(beanName, mbdToUse);  </span><br><span class="line">if (bean != null) &#123;  </span><br><span class="line">return bean;  </span><br><span class="line">&#125;</span><br><span class="line">Object beanInstance = doCreateBean(beanName, mbdToUse, args);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ol><li>postProcessAfterInstantiation<br>在 bean create之后，populate之前调用。返回 <code>true</code> 表示允许属性注入，返回 <code>false</code> 则跳过属性注入。</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">default boolean postProcessAfterInstantiation(Object bean, String beanName) throws BeansException &#123;  </span><br><span class="line">return true;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ol><li>postProcessProperties<br>在populate 属性注入过程中调用，可以对属性值进行检查或修改。<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">@Nullable  </span><br><span class="line">default PropertyValues postProcessProperties(PropertyValues pvs, Object bean, String beanName)  </span><br><span class="line">throws BeansException &#123;   </span><br><span class="line">return null;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol><h4 id="242-smartinstantiationawarebeanpostprocessor"><a href="#2-4-2-SmartInstantiationAwareBeanPostProcessor" class="headerlink" title="2.4.2 SmartInstantiationAwareBeanPostProcessor"></a>2.4.2 SmartInstantiationAwareBeanPostProcessor</h4><p><code>SmartInstantiationAwareBeanPostProcessor</code> 是 Spring 框架中的一个接口，它扩展了 <code>InstantiationAwareBeanPostProcessor</code> 接口，提供了更细粒度的控制和额外的钩子方法来介入 Spring Bean 的实例化过程。这个接口主要用于在 Bean create之前、create之后、populate之前、populate之后等多个阶段执行自定义逻辑，从而实现更加复杂和灵活的 Bean 初始化控制。</p><img src="/114991e5/6.png" class><ol><li>predictBeanType<br>在实际创建 Bean 实例之前预测其类型。这对于提前检测某些类型相关的元数据很有用。<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">default</span> Class&lt;?&gt; predictBeanType(Class&lt;?&gt; beanClass, String beanName) <span class="keyword">throws</span> BeansException &#123;  </span><br><span class="line"><span class="keyword">return</span> <span class="literal">null</span>;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol><ol><li><p>determineCandidateConstructors<br>确定要用于创建 Bean 实例的候选构造函数。允许自定义选择用于实例化 Bean 的构造函数。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">default</span> Constructor&lt;?&gt;[] determineCandidateConstructors(Class&lt;?&gt; beanClass, String beanName)  </span><br><span class="line"><span class="keyword">throws</span> BeansException &#123;  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">return</span> <span class="literal">null</span>;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>getEarlyBeanReference<br>允许在 Bean 实例化之前提前暴露 Bean 的引用。通常用于解决循环依赖问题</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">default</span> Object <span class="title function_">getEarlyBeanReference</span><span class="params">(Object bean, String beanName)</span> <span class="keyword">throws</span> BeansException &#123;  </span><br><span class="line"><span class="keyword">return</span> bean;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol><h3 id="25-beanfactorypostprocessor-vs-beanpostprocessor"><a href="#2-5-BeanFactoryPostProcessor-vs-BeanPostProcessor" class="headerlink" title="2.5 BeanFactoryPostProcessor vs BeanPostProcessor"></a>2.5 BeanFactoryPostProcessor vs BeanPostProcessor</h3><p><code>BeanFactoryPostProcessor</code>和<code>BeanPostProcessor</code>名称相似，但是作用大不相同，下面给出一些对比</p><p>为了更清楚地比较 <code>BeanFactoryPostProcessor</code> 和 <code>BeanPostProcessor</code>，下表总结了它们的关键差异：</p><div class="table-container"><table><thead><tr><th>特征</th><th>BeanFactoryPostProcessor</th><th>BeanPostProcessor</th></tr></thead><tbody><tr><td><strong>目的</strong></td><td>修改或调整 bean 的配置元数据</td><td>在 bean 初始化前后对实例进行修改或增强</td></tr><tr><td><strong>作用对象</strong></td><td>操作 <code>BeanDefinition</code>（bean 的定义）</td><td>直接操作 bean 实例</td></tr><tr><td><strong>执行时机</strong></td><td>在所有 bean 实例化前，容器启动过程中，加载完所有 bean 定义之后执行</td><td>对每个 bean，分别在初始化Initialization方法之前和之后执行</td></tr><tr><td><strong>方法接口</strong></td><td><code>postProcessBeanFactory(ConfigurableListableBeanFactory beanFactory)</code></td><td><code>postProcessBeforeInitialization(Object bean, String beanName)</code> <br> <code>postProcessAfterInitialization(Object bean, String beanName)</code></td></tr><tr><td><strong>影响范围</strong></td><td>可以修改整个容器中的所有 bean 定义</td><td>对每个 bean 实例的修改是独立的</td></tr><tr><td><strong>应用示例</strong></td><td>修改 bean 的作用域、解析配置文件中的占位符等</td><td>创建代理对象以实现 AOP，执行自定义初始化或清理代码等</td></tr><tr><td><strong>执行顺序控制接口</strong></td><td>通常可以通过实现 <code>Ordered</code> 或 <code>PriorityOrdered</code> 接口来控制执行顺序</td><td>同样可以实现 <code>Ordered</code> 或 <code>PriorityOrdered</code> 来控制执行顺序</td></tr></tbody></table></div><h2 id="3-无所不知的aware"><a href="#3-无所不知的Aware" class="headerlink" title="3. 无所不知的Aware"></a>3. 无所不知的Aware</h2><p>Spring框架中，<code>Aware</code>接口的作用是让bean能够拿到对Spring容器中的各种资源，从而增强bean的功能和灵活性。</p><p>不同的 Aware 基本都能够见名知意，Aware之前的名字就是可以拿到什么资源，例如<code>BeanNameAware</code>可以拿到BeanName，以此类推。调用时机需要注意：所有的Aware方法都是在init阶段之前调用的</p><h3 id="31-aware调用时机"><a href="#3-1-Aware调用时机" class="headerlink" title="3.1 Aware调用时机"></a>3.1 Aware调用时机</h3><p>在Spring 启动过程中涉及到Aware ,都是在populate后，init 之前调用的。</p><p>其调用代码如下，</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">protected Object initializeBean(String beanName, Object bean, @Nullable RootBeanDefinition mbd) &#123;  </span><br><span class="line">// 调用 bean 相关的 Aware</span><br><span class="line">invokeAwareMethods(beanName, bean);  </span><br><span class="line">// 调用 ApplicationContext 的相关 Aware</span><br><span class="line">applyBeanPostProcessorsBeforeInitialization(wrappedBean, beanName);   </span><br><span class="line">  </span><br><span class="line">invokeInitMethods(beanName, wrappedBean, mbd);   </span><br><span class="line"></span><br><span class="line">applyBeanPostProcessorsAfterInitialization(wrappedBean, beanName);  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在Spring 中一些会用到Aware 列举如下， 这也是在实际运行中它们会被调用的顺序</p><ol><li><strong>BeanNameAware</strong>: 最先被调用，用于设置bean的名称。</li><li><strong>BeanClassLoaderAware</strong>: 在bean实例化后，用于设置bean的类加载器。</li><li><strong>BeanFactoryAware</strong>: 在bean populate 完成后，用于设置bean的BeanFactory。</li><li><strong>EnvironmentAware</strong>: 在bean的populate 完成后，用于设置bean的环境信息。</li><li><strong>EmbeddedValueResolverAware</strong>: 在bean的populate 完成后，用于设置Spring EL解析器。</li><li><strong>ApplicationContextAware</strong>: 最后被调用，用于设置bean的ApplicationContext。</li></ol><p>下面将分组进行解释</p><h3 id="32-aware-group-1-bean-相关aware"><a href="#3-2-Aware-Group-1-Bean-相关Aware" class="headerlink" title="3.2 Aware Group 1- Bean 相关Aware"></a>3.2 Aware Group 1- Bean 相关Aware</h3><p>在invokeAwareMethods方法中，会调用以下3个Aware</p><ul><li><strong>BeanNameAware</strong>: 用于让bean获取其在容器中的名称。在实例化之后，设置依赖之前被调用。</li><li><strong>BeanClassLoaderAware</strong>: 用于让bean获取类加载器。这通常在bean实例化之后和依赖注入之前被调用。</li><li><strong>BeanFactoryAware</strong>: 用于让bean获取它所在的BeanFactory实例。在依赖注入之后，bean初始化之前被调用。</li></ul><p>点击查看invokeAwareMethods， 其逻辑涉及到3个Aware 接口<br><img src="/114991e5/7.png" class></p><h4 id="1beanfactoryaware"><a href="#1-BeanFactoryAware" class="headerlink" title="1.BeanFactoryAware"></a>1.BeanFactoryAware</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">interface</span> <span class="title class_">BeanFactoryAware</span> &#123;</span><br><span class="line">    <span class="keyword">void</span> <span class="title function_">setBeanFactory</span><span class="params">(BeanFactory beanFactory)</span> <span class="keyword">throws</span> BeansException;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><strong>作用</strong>：允许bean获取<code>BeanFactory</code>实例，从而能够访问Spring容器中的其他bean。</li><li><strong>使用场景</strong>：在需要动态获取或创建bean的场景下使用，例如实现复杂的依赖关系或自定义初始化逻辑。</li></ul><h4 id="2beannameaware"><a href="#2-BeanNameAware" class="headerlink" title="2.BeanNameAware"></a>2.BeanNameAware</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">interface</span> <span class="title class_">BeanNameAware</span> &#123;</span><br><span class="line">    <span class="keyword">void</span> <span class="title function_">setBeanName</span><span class="params">(String name)</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>实现 <code>BeanNameAware</code> 接口的 bean 会在 Spring 容器创建并初始化该 bean 时，调用 <code>setBeanName</code> 方法，将容器中为该 bean 配置的名称传递给它。这对于需要知道自己在容器中的名称的 bean 来说非常有用。</p><h3 id="33-aware-group-2-applicationcontext-相关aware"><a href="#3-3-Aware-Group-2-ApplicationContext-相关Aware" class="headerlink" title="3.3  Aware Group 2-ApplicationContext  相关Aware"></a>3.3  Aware Group 2-ApplicationContext  相关Aware</h3><p>并不是所有的Aware接口都使用同样的方式调用。Bean××Aware都是在代码中直接调用的，而ApplicationContext相关的Aware都是通过BeanPostProcessor#postProcessBeforeInitialization()实现的</p><ul><li><strong>EnvironmentAware</strong>: 允许bean访问Spring的Environment接口，用于获取环境属性和配置文件信息。</li><li><strong>EmbeddedValueResolverAware</strong>: 允许bean获取Spring的字符串值解析器，特别是用来解析Spring EL表达式（SpEL）。</li><li><strong>ApplicationContextAware</strong>: 允许bean获取ApplicationContext，这是BeanFactory的一个子接口，提供更多容器功能和应用上下文相关的信息。</li></ul><p>跟进代码最后来到ApplicationContextAwareProcessor中，ApplicationContextAwareProcessor 是一个BeanPostProcessor, 最后在其重写的postProcessBeforeInitialization方法中，会调用如下方法。<br><img src="/114991e5/8.png" class></p><h3 id="34-aware执行时机"><a href="#3-4-Aware执行时机" class="headerlink" title="3.4 Aware执行时机"></a>3.4 Aware执行时机</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">protected</span> Object <span class="title function_">initializeBean</span><span class="params">(String beanName, Object bean, <span class="meta">@Nullable</span> RootBeanDefinition mbd)</span> &#123;  </span><br><span class="line"></span><br><span class="line">invokeAwareMethods(beanName, bean);  </span><br><span class="line"></span><br><span class="line">applyBeanPostProcessorsBeforeInitialization(wrappedBean, beanName);   </span><br><span class="line">  </span><br><span class="line">invokeInitMethods(beanName, wrappedBean, mbd);   </span><br><span class="line"></span><br><span class="line">applyBeanPostProcessorsAfterInitialization(wrappedBean, beanName);  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>示例代码<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.springframework.beans.factory.BeanFactory;</span><br><span class="line"><span class="keyword">import</span> org.springframework.beans.factory.BeanFactoryAware;</span><br><span class="line"><span class="keyword">import</span> org.springframework.stereotype.Component;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Component</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">MyBeanFactoryAwareBean</span> <span class="keyword">implements</span> <span class="title class_">BeanFactoryAware</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> BeanFactory beanFactory;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setBeanFactory</span><span class="params">(BeanFactory beanFactory)</span> <span class="keyword">throws</span> BeansException &#123;</span><br><span class="line">        <span class="built_in">this</span>.beanFactory = beanFactory;</span><br><span class="line">        System.out.println(<span class="string">&quot;BeanFactory has been set: &quot;</span> + beanFactory);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">doSomething</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="comment">// 使用beanFactory获取另一个bean</span></span><br><span class="line">        <span class="type">MyOtherBean</span> <span class="variable">otherBean</span> <span class="operator">=</span> beanFactory.getBean(MyOtherBean.class);</span><br><span class="line">        otherBean.performTask();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h2 id="4bean生命周期"><a href="#4-Bean生命周期" class="headerlink" title="4.Bean生命周期"></a>4.Bean生命周期</h2><p>对bean 的生命周期做细化的话可以分为以下阶段</p><img src="/114991e5/9.png" class><ol><li><p><strong>实例化（Instantiation）</strong>：</p><ul><li>Spring容器使用构造函数或工厂方法实例化bean。</li></ul></li><li><p><strong>属性填充（Dependency Injection）</strong>：</p><ul><li>Spring容器进行依赖注入，将所有需要的属性和依赖注入到bean实例中。</li></ul></li><li><p><strong>设置特殊回调接口（Aware接口）</strong>：</p><ul><li>如果bean实现了<code>Aware</code>接口，Spring容器会在这个阶段调用相应的方法。</li><li>包括<code>BeanNameAware</code>、<code>BeanClassLoaderAware</code>、<code>BeanFactoryAware</code>等。</li></ul></li><li><p><strong>初始化前处理（BeanPostProcessor的<code>postProcessBeforeInitialization</code>方法）</strong>：</p><ul><li>Spring容器会在这个阶段调用所有注册的<code>BeanPostProcessor</code>的<code>postProcessBeforeInitialization</code>方法。</li></ul></li><li><p><strong>初始化（Initialization）</strong>：</p><ul><li>如果bean实现了<code>InitializingBean</code>接口，Spring容器会调用其<code>afterPropertiesSet</code>方法。</li><li>如果bean配置了自定义的init方法，Spring容器会调用该方法。</li></ul></li><li><p><strong>初始化后处理（BeanPostProcessor的<code>postProcessAfterInitialization</code>方法）</strong>：</p><ul><li>Spring容器会在这个阶段调用所有注册的<code>BeanPostProcessor</code>的<code>postProcessAfterInitialization</code>方法。</li></ul></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Spring框架提供了丰富的扩展点，允许开发者在bean的生命周期的不同阶段插入自定义逻辑，从而增强应用的功能和灵活性。拓展点包括&lt;code&gt;BeanFactoryPostProcessor&lt;/code&gt;、&lt;code&gt;BeanPostProcessor&lt;/code&gt;、各种A</summary>
      
    
    
    
    
    <category term="java" scheme="http://example.com/tags/java/"/>
    
    <category term="Spring" scheme="http://example.com/tags/Spring/"/>
    
  </entry>
  
  <entry>
    <title>Understanding LSTM Networks</title>
    <link href="http://example.com/4579c6a3/"/>
    <id>http://example.com/4579c6a3/</id>
    <published>2024-06-16T10:07:01.000Z</published>
    <updated>2024-06-16T14:23:22.633Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">Understanding LSTM Networks</a></p><h1 id="recurrent-neural-networks"><a href="#Recurrent-Neural-Networks" class="headerlink" title="Recurrent Neural Networks"></a>Recurrent Neural Networks</h1><p>递归神经网络<br>Humans don’t start their thinking from scratch every second. As you read this essay, you understand each word based on your understanding of previous words. You don’t throw everything away and start thinking from scratch again. Your thoughts have persistence.<br>人类不会每秒都从头开始思考。当你阅读这篇文章时，你会基于对之前词语的理解来理解每个词。你不会把所有东西都丢掉然后重新开始思考。你的思维是有连续性的。</p><p>Traditional neural networks can’t do this, and it seems like a major shortcoming. For example, imagine you want to classify what kind of event is happening at every point in a movie. It’s unclear how a traditional neural network could use its reasoning about previous events in the film to inform later ones.<br>传统的神经网络做不到这一点，这似乎是一个主要的缺陷。比如，想象一下你想对电影中每个时刻发生的事件类型进行分类。目前尚不清楚传统的神经网络如何利用其对电影中先前事件的推理来为后来的事件提供信息。</p><p>Recurrent neural networks address this issue. They are networks with loops in them, allowing information to persist.<br>循环神经网络解决了这个问题。它们是内部带有循环的网络，允许信息持续存在。</p><img src="/4579c6a3/1.png" class><p><strong>Recurrent Neural Networks have loops.  递归神经网络有循环。</strong></p><p>In the above diagram, a chunk of neural network, 𝐴, looks at some input $𝑥_𝑡$ and outputs a value $ℎ_𝑡$. A loop allows information to be passed from one step of the network to the next.<br>在上图中，神经网络的一部分 $A$ ,查看一些输入$x_t​$ 并输出一个值 $h_t$。一个循环允许信息从网络的一个步骤传递到下一个步骤。</p><p>These loops make recurrent neural networks seem kind of mysterious. However, if you think a bit more, it turns out that they aren’t all that different than a normal neural network. A recurrent neural network can be thought of as multiple copies of the same network, each passing a message to a successor. Consider what happens if we unroll the loop:<br>这些循环使循环神经网络看起来有点神秘。然而，如果你多想一点，就会发现它们与普通的神经网络并没有太大的不同。递归神经网络可以被认为是同一网络的多个副本，每个副本将消息传递给继任者。考虑一下如果我们展开循环会发生什么：<br><img src="/4579c6a3/2.png" class><br><strong>An unrolled recurrent neural network.  展开的循环神经网络。</strong></p><p>This chain-like nature reveals that recurrent neural networks are intimately related to sequences and lists. They’re the natural architecture of neural network to use for such data.<br>这种链式结构表明，递归神经网络与序列和列表密切相关。它们是处理此类数据的自然神经网络架构。</p><p>And they certainly are used! In the last few years, there have been incredible success applying RNNs to a variety of problems: speech recognition, language modeling, translation, image captioning… The list goes on. I’ll leave discussion of the amazing feats one can achieve with RNNs to Andrej Karpathy’s excellent blog post, <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">The Unreasonable Effectiveness of Recurrent Neural Networks</a>. But they really are pretty amazing.<br>他们当然被使用了！在过去的几年里，将RNN应用于各种问题取得了令人难以置信的成功：语音识别、语言建模、翻译、图像字幕……这样的例子不胜枚举。我将把关于RNN可以实现的惊人壮举的讨论留给Andrej Karpathy的优秀博客文章，递归神经网络的不合理有效性。但他们真的非常了不起。</p><p>Essential to these successes is the use of “LSTMs,” a very special kind of recurrent neural network which works, for many tasks, much much better than the standard version. Almost all exciting results based on recurrent neural networks are achieved with them. It’s these LSTMs that this essay will explore.<br>这些成功的关键是“LSTM”的使用，这是一种非常特殊的递归神经网络，对于许多任务，它比标准版本要好得多。几乎所有基于递归神经网络的令人兴奋的结果都是通过它们实现的。本文将探讨的正是这些 LSTM。</p><h1 id="the-problem-of-long-term-dependencies"><a href="#The-Problem-of-Long-Term-Dependencies" class="headerlink" title="The Problem of Long-Term Dependencies"></a>The Problem of Long-Term Dependencies</h1><p>长期依赖性问题</p><p>One of the appeals of RNNs is the idea that they might be able to connect previous information to the present task, such as using previous video frames might inform the understanding of the present frame. If RNNs could do this, they’d be extremely useful. But can they? It depends.<br>RNN的吸引力之一是，它们可能能够将先前的信息与当前任务联系起来，例如使用以前的视频帧可能会为理解当前帧提供信息。如果RNN可以做到这一点，它们将非常有用。但是他们能做到吗？这要视情况而定。</p><p>Sometimes, we only need to look at recent information to perform the present task. For example, consider a language model trying to predict the next word based on the previous ones. If we are trying to predict the last word in “the clouds are in the _sky_,” we don’t need any further context – it’s pretty obvious the next word is going to be sky. In such cases, where the gap between the relevant information and the place that it’s needed is small, RNNs can learn to use the past information.<br>有时候，我们只需要查看最近的信息就可以完成当前的任务。例如，考虑一个语言模型,它尝试基于前面的单词来预测下一个单词。如果我们试图预测“the clouds are in the sky”中的最后一个单词，我们不需要任何进一步的上下文——很明显下一个单词将是sky。在这种情况下，相关信息和所需位置之间的间隔较小，RNNs可以学习使用过去的信息。<br><img src="/4579c6a3/3.png" class></p><p>But there are also cases where we need more context. Consider trying to predict the last word in the text “I grew up in France… I speak fluent _French_.” Recent information suggests that the next word is probably the name of a language, but if we want to narrow down which language, we need the context of France, from further back. It’s entirely possible for the gap between the relevant information and the point where it is needed to become very large.<br>但在某些情况下，我们需要更多的背景信息。考虑试图预测文本“I grew up in France… I speak fluent French.”中的最后一个单词。最近的信息表明下一个单词可能是某种语言的名称，但如果我们想缩小语言范围，我们需要更早的法国这一背景信息。相关信息和需要使用该信息的点之间的间隔完全有可能变得非常大。</p><p>Unfortunately, as that gap grows, RNNs become unable to learn to connect the information.<br>不幸的是，随着这种间隔的扩大，RNN变得无法学习去连接信息。</p><img src="/4579c6a3/4.png" class><p>In theory, RNNs are absolutely capable of handling such “long-term dependencies.” A human could carefully pick parameters for them to solve toy problems of this form. Sadly, in practice, RNNs don’t seem to be able to learn them. The problem was explored in depth by <a href="http://people.idsia.ch/~juergen/SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf">Hochreiter (1991) [German]</a> and <a href="http://www-dsi.ing.unifi.it/~paolo/ps/tnn-94-gradient.pdf">Bengio, et al. (1994)</a>, who found some pretty fundamental reasons why it might be difficult.<br>理论上，RNNs完全有能力处理这种“长期依赖”。人类可以仔细挑选参数，使它们解决这种形式的玩具问题。遗憾的是，在实际应用中，RNNs似乎无法学会它们。这个问题在Hochreiter（1991）和Bengio等人（1994）的研究中得到了深入探讨，他们发现了一些可能导致这一困难的基本原因。</p><p>Thankfully, LSTMs don’t have this problem!<br>值得庆幸的是，LSTM 没有这个问题！</p><h1 id="lstm-networks"><a href="#LSTM-Networks" class="headerlink" title="LSTM Networks"></a>LSTM Networks</h1><p>Long Short Term Memory networks – usually just called “LSTMs” – are a special kind of RNN, capable of learning long-term dependencies. They were introduced by <a href="http://www.bioinf.jku.at/publications/older/2604.pdf">Hochreiter &amp; Schmidhuber (1997)</a>, and were refined and popularized by many people in following work.<a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/#fn1">1</a> They work tremendously well on a large variety of problems, and are now widely used.<br>长短期记忆网络——通常简称为“LSTMs”——是一种特殊的RNN，能够学习长期依赖。它们由Hochreiter和Schmidhuber（1997）引入，并在随后的工作中被许多人改进和推广。LSTMs在大量不同的问题上表现出色，现在被广泛使用。</p><p>LSTMs are explicitly designed to avoid the long-term dependency problem. Remembering information for long periods of time is practically their default behavior, not something they struggle to learn!<br>LSTMs被明确设计用于避免长期依赖问题。记住长时间的信息几乎是它们的默认行为，而不是它们需要努力学习的东西！</p><p>All recurrent neural networks have the form of a chain of repeating modules of neural network. In standard RNNs, this repeating module will have a very simple structure, such as a single tanh layer.<br>所有递归神经网络都具有神经网络重复模块链的形式。在标准 RNN 中，该重复模块将具有非常简单的结构，例如单个 tanh 层。</p><img src="/4579c6a3/5.png" class><p><strong>The repeating module in a standard RNN contains a single layer.<br>标准 RNN 中的重复模块包含单层。</strong></p><p>LSTMs also have this chain like structure, but the repeating module has a different structure. Instead of having a single neural network layer, there are four, interacting in a very special way.<br>LSTM 也具有这种链状结构，但重复模块具有不同的结构。不是只有一个神经网络层，而是有四个，以一种非常特殊的方式进行交互。</p><img src="/4579c6a3/6.png" class><p><strong>The repeating module in an LSTM contains four interacting layers.<br>LSTM 中的重复模块包含四个交互层。</strong></p><p>Don’t worry about the details of what’s going on. We’ll walk through the LSTM diagram step by step later. For now, let’s just try to get comfortable with the notation we’ll be using.<br>不用担心具体的细节。我们稍后会一步步讲解LSTM的图表。现在，让我们先熟悉一下我们将要使用的符号。<br><img src="/4579c6a3/7.png" class><br>In the above diagram, each line carries an entire vector, from the output of one node to the inputs of others. The pink circles represent pointwise operations, like vector addition, while the yellow boxes are learned neural network layers. Lines merging denote concatenation, while a line forking denote its content being copied and the copies going to different locations.<br>在上图中，每条线都带有一个完整的向量，从一个节点的输出到其他节点的输入。粉红色的圆圈代表逐点运算，如向量加法，而黄色框是学习的神经网络层。合并的行表示串联，而分叉的行表示正在复制其内容并将副本发送到不同的位置。</p><h1 id="the-core-idea-behind-lstms"><a href="#The-Core-Idea-Behind-LSTMs" class="headerlink" title="The Core Idea Behind LSTMs"></a>The Core Idea Behind LSTMs</h1><p>LSTM 背后的核心思想</p><p>The key to LSTMs is the cell state, the horizontal line running through the top of the diagram.<br>LSTMs的关键是单元状态，这条横线贯穿了图表的顶部。</p><p>The cell state is kind of like a conveyor belt. It runs straight down the entire chain, with only some minor linear interactions. It’s very easy for information to just flow along it unchanged.<br>单元状态有点像传送带。它直接沿着整个链条运行，只有一些小的线性交互。信息可以非常容易地沿着它不变地流动。</p><img src="/4579c6a3/8.png" class><p>The LSTM does have the ability to remove or add information to the cell state, carefully regulated by structures called gates.<br>LSTM确实具有从单元状态中移除或添加信息的能力，这些操作由称为门控的结构严格调控。</p><p>Gates are a way to optionally let information through. They are composed out of a sigmoid neural net layer and a pointwise multiplication operation.<br>门控是一种选择性地让信息通过的方式。它们由一个sigmoid神经网络层和一个逐点乘法操作组成。<br><img src="/4579c6a3/9.png" class><br>The sigmoid layer outputs numbers between zero and one, describing how much of each component should be let through. A value of zero means “let nothing through,” while a value of one means “let everything through!”<br>sigmoid 层输出介于 0 和 1 之间的数字，描述每个组件应通过多少。值为零表示“什么都不让通过”，而值为 1 表示“让所有东西都通过！”</p><p>An LSTM has three of these gates, to protect and control the cell state.<br>LSTM 有三个这样的门，用于保护和控制单元状态。</p><h1 id="step-by-step-lstm-walk-through"><a href="#Step-by-Step-LSTM-Walk-Through" class="headerlink" title="Step-by-Step LSTM Walk Through"></a>Step-by-Step LSTM Walk Through</h1><p>循序渐进的 LSTM 演练</p><h2 id="forget-gate-layer"><a href="#forget-gate-layer" class="headerlink" title="forget gate layer"></a>forget gate layer</h2><p>The first step in our LSTM is to decide what information we’re going to throw away from the cell state. This decision is made by a sigmoid layer called the “forget gate layer.” It looks at $h_{t−1}$ and $x_t$, and outputs a number between 00 and 11 for each number in the cell state $C_{t−1}$. A 1 represents “completely keep this” while a 0 represents “completely get rid of this.”<br>LSTM的第一步是决定要从单元状态中丢弃哪些信息。这个决策是由一个名为“遗忘门层”的sigmoid层做出的。它查看 $h_{t-1}​$ 和 $x_t$，并为单元状态 $C_{t-1}$ 中的每个数字输出一个介于0和1之间的数值。1表示“完全保留这个”，而0表示“完全丢弃这个”。</p><p>Let’s go back to our example of a language model trying to predict the next word based on all the previous ones. In such a problem, the cell state might include the gender of the present subject, so that the correct pronouns can be used. When we see a new subject, we want to forget the gender of the old subject.<br>让我们回到我们的例子，一个语言模型试图基于所有前面的单词来预测下一个单词。在这样的问题中，单元状态可能包含当前主语的性别，以便使用正确的代词。当我们看到一个新的主语时，我们想忘记旧主语的性别。<br><img src="/4579c6a3/10.png" class></p><h2 id="input-gate-layer"><a href="#input-gate-layer" class="headerlink" title="input gate layer"></a>input gate layer</h2><p>The next step is to decide what new information we’re going to store in the cell state. This has two parts. First, a sigmoid layer called the “input gate layer” decides which values we’ll update. Next, a tanh layer creates a vector of new candidate values, $\tilde{C}_t$, that could be added to the state. In the next step, we’ll combine these two to create an update to the state.<br>下一步是决定要在单元状态中存储哪些新信息。这包括两个部分。首先，一个名为“输入门层”的sigmoid层决定我们将更新哪些值。接下来，一个tanh层创建一个新的候选值向量$\tilde{C}_t$，这些候选值可以被添加到状态中。在下一步中，我们将结合这两部分来更新状态。</p><p>In the example of our language model, we’d want to add the gender of the new subject to the cell state, to replace the old one we’re forgetting.<br>在我们的语言模型示例中，我们希望将新主语的性别添加到单元格状态中，以替换我们忘记的旧主语。</p><img src="/4579c6a3/11.png" class><p>It’s now time to update the old cell state$C_{t-1}$, into the new cell state $C_t$​. The previous steps already decided what to do, we just need to actually do it.<br>现在是时候将旧的单元格状态$C_{t-1}$更新为新的单元格状态 $C_t$​了。前面的步骤已经决定了要做什么，我们只需要实际去执行它。</p><p>We multiply the old state by $𝑓_𝑡$, forgetting the things we decided to forget earlier. Then we add $i_t \ast \tilde{C}_t$. This is the new candidate values, scaled by how much we decided to update each state value.<br>我们将旧状态乘以 $f_t$，忘记我们之前决定忘记的内容。然后我们加上 $i_t \ast \tilde{C}_t$。这些是新的候选值，按我们决定更新每个状态值的程度进行缩放。</p><p>In the case of the language model, this is where we’d actually drop the information about the old subject’s gender and add the new information, as we decided in the previous steps.<br>在语言模型的情况下，正如我们在前面的步骤中决定的那样，我们实际上会删除有关旧主题性别的信息并添加新信息。</p><img src="/4579c6a3/12.png" class><h2 id="output-layer"><a href="#output-layer" class="headerlink" title="output  layer"></a>output  layer</h2><p>Finally, we need to decide what we’re going to output. This output will be based on our cell state, but will be a filtered version. First, we run a sigmoid layer which decides what parts of the cell state we’re going to output. Then, we put the cell state through tanh (to push the values to be between −1 and 1) and multiply it by the output of the sigmoid gate, so that we only output the parts we decided to.<br>最后，我们需要决定输出什么。这个输出将基于我们的单元状态，但会是一个过滤后的版本。首先，我们运行一个sigmoid层来决定要输出单元状态的哪些部分。然后，我们将单元状态通过tanh（将值压缩到-1到1之间），并将其与sigmoid门的输出相乘，这样我们只输出我们决定输出的部分。</p><p>For the language model example, since it just saw a subject, it might want to output information relevant to a verb, in case that’s what is coming next. For example, it might output whether the subject is singular or plural, so that we know what form a verb should be conjugated into if that’s what follows next.<br>对于语言模型的例子，由于它刚刚看到一个主语，它可能想输出与动词相关的信息，以防接下来需要动词。例如，它可能会输出主语是单数还是复数，这样我们就知道如果接下来是动词，该动词应该变成什么形式。</p><img src="/4579c6a3/13.png" class><h1 id="variants-on-long-short-term-memory"><a href="#Variants-on-Long-Short-Term-Memory" class="headerlink" title="Variants on Long Short Term Memory"></a>Variants on Long Short Term Memory</h1><p>长短期记忆的变体</p><p>What I’ve described so far is a pretty normal LSTM. But not all LSTMs are the same as the above. In fact, it seems like almost every paper involving LSTMs uses a slightly different version. The differences are minor, but it’s worth mentioning some of them.<br>我到目前为止描述的是一个相当普通的LSTM。但并不是所有的LSTM都与上述相同。实际上，几乎每篇涉及LSTM的论文都使用了稍微不同的版本。这些差异很小，但值得一提。</p><p>One popular LSTM variant, introduced by <a href="ftp://ftp.idsia.ch/pub/juergen/TimeCount-IJCNN2000.pdf">Gers &amp; Schmidhuber (2000)</a>, is adding “peephole connections.” This means that we let the gate layers look at the cell state.<br>一个由Gers和Schmidhuber（2000）引入的流行LSTM变体是添加“窥视连接”。这意味着我们让门控层查看单元状态。</p><img src="/4579c6a3/14.png" class><p>The above diagram adds peepholes to all the gates, but many papers will give some peepholes and not others.<br>上图为所有门控添加了窥视连接，但许多论文会只为部分门控添加窥视连接，而不是全部。</p><p>Another variation is to use coupled forget and input gates. Instead of separately deciding what to forget and what we should add new information to, we make those decisions together. We only forget when we’re going to input something in its place. We only input new values to the state when we forget something older.<br>另一种变体是使用耦合的遗忘门和输入门。我们不是分别决定要忘记什么以及要添加什么新信息，而是将这些决策结合在一起。我们只有在要输入新信息时才会忘记某些内容。只有在忘记旧信息时，我们才会将新值输入到状态中。</p><img src="/4579c6a3/15.png" class><p>A slightly more dramatic variation on the LSTM is the Gated Recurrent Unit, or GRU, introduced by <a href="http://arxiv.org/pdf/1406.1078v3.pdf">Cho, et al. (2014)</a>. It combines the forget and input gates into a single “update gate.” It also merges the cell state and hidden state, and makes some other changes. The resulting model is simpler than standard LSTM models, and has been growing increasingly popular.<br>LSTM的一个稍微更显著的变体是门控循环单元（GRU），由Cho等人（2014）引入。它将遗忘门和输入门组合成一个“更新门”。它还合并了单元状态和隐藏状态，并做了一些其他的改变。最终的模型比标准的LSTM模型更简单，并且越来越受欢迎。</p><img src="/4579c6a3/16.png" class><p>These are only a few of the most notable LSTM variants. There are lots of others, like Depth Gated RNNs by <a href="http://arxiv.org/pdf/1508.03790v2.pdf">Yao, et al. (2015)</a>. There’s also some completely different approach to tackling long-term dependencies, like Clockwork RNNs by <a href="http://arxiv.org/pdf/1402.3511v1.pdf">Koutnik, et al. (2014)</a>.<br>这些只是一些最著名的LSTM变体。还有许多其他变体，例如Yao等人（2015）提出的深度门控RNN。此外，还有一些完全不同的方法来解决长期依赖问题，例如Koutnik等人（2014）提出的时钟式RNN。</p><p>Which of these variants is best? Do the differences matter? <a href="http://arxiv.org/pdf/1503.04069.pdf">Greff, et al. (2015)</a> do a nice comparison of popular variants, finding that they’re all about the same. <a href="http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf">Jozefowicz, et al. (2015)</a> tested more than ten thousand RNN architectures, finding some that worked better than LSTMs on certain tasks.<br>这些变体中哪一个最好？差异重要吗？Greff等人（2015）对流行变体进行了很好的比较，发现它们的表现几乎相同。Jozefowicz等人（2015）测试了超过一万种RNN架构，发现其中一些在某些任务上的表现比LSTMs更好。</p><h1 id="conclusion-结论"><a href="#Conclusion-结论" class="headerlink" title="Conclusion 结论"></a>Conclusion 结论</h1><p>Earlier, I mentioned the remarkable results people are achieving with RNNs. Essentially all of these are achieved using LSTMs. They really work a lot better for most tasks!<br>前面，我提到了人们用递归神经网络（RNNs）取得的显著成果。基本上所有这些成果都是使用LSTMs实现的。对于大多数任务，LSTMs的效果确实要好得多！</p><p>Written down as a set of equations, LSTMs look pretty intimidating. Hopefully, walking through them step by step in this essay has made them a bit more approachable.<br>作为一组方程写下来，LSTMs看起来相当令人生畏。希望通过在本文中一步一步地讲解它们，使它们变得更容易理解。</p><p>LSTMs were a big step in what we can accomplish with RNNs. It’s natural to wonder: is there another big step? A common opinion among researchers is: “Yes! There is a next step and it’s attention!” The idea is to let every step of an RNN pick information to look at from some larger collection of information. For example, if you are using an RNN to create a caption describing an image, it might pick a part of the image to look at for every word it outputs. In fact, <a href="http://arxiv.org/pdf/1502.03044v2.pdf">Xu, _et al._ (2015)</a> do exactly this – it might be a fun starting point if you want to explore attention! There’s been a number of really exciting results using attention, and it seems like a lot more are around the corner…<br>LSTMs是我们用RNNs能实现的一个大进步。很自然地会有人问：还有另一个大进步吗？研究人员的一个普遍看法是：“是的！下一个进步是注意力机制！”这个想法是让RNN的每一步都从一些更大的信息集合中选择要看的信息。例如，如果你使用RNN来创建描述图像的标题，它可能会为它输出的每个单词选择图像的一部分。事实上，Xu等人（2015）正是这样做的——如果你想探索注意力机制，这可能是一个有趣的起点！使用注意力机制已经取得了许多非常令人兴奋的成果，似乎还会有更多的成果即将到来……</p><p>Attention isn’t the only exciting thread in RNN research. For example, Grid LSTMs by <a href="http://arxiv.org/pdf/1507.01526v1.pdf">Kalchbrenner, _et al._ (2015)</a> seem extremely promising. Work using RNNs in generative models – such as <a href="http://arxiv.org/pdf/1502.04623.pdf">Gregor, _et al._ (2015)</a>, <a href="http://arxiv.org/pdf/1506.02216v3.pdf">Chung, _et al._ (2015)</a>, or <a href="http://arxiv.org/pdf/1411.7610v3.pdf">Bayer &amp; Osendorfer (2015)</a> – also seems very interesting. The last few years have been an exciting time for recurrent neural networks, and the coming ones promise to only be more so!<br>注意力机制并不是RNN研究中唯一令人兴奋的方向。例如，Kalchbrenner等人（2015）的Grid LSTMs看起来非常有前途。在生成模型中使用RNN的工作——例如Gregor等人（2015）、Chung等人（2015）或Bayer和Osendorfer（2015）的工作——也非常有趣。过去几年是递归神经网络的激动人心的时期，未来几年只会更加激动人心！</p><h1 id="acknowledgments-确认"><a href="#Acknowledgments-确认" class="headerlink" title="Acknowledgments 确认"></a>Acknowledgments 确认</h1><p>I’m grateful to a number of people for helping me better understand LSTMs, commenting on the visualizations, and providing feedback on this post.<br>我感谢许多人帮助我更好地理解 LSTM，对可视化进行评论，并对这篇文章提供反馈。</p><p>I’m very grateful to my colleagues at Google for their helpful feedback, especially <a href="http://research.google.com/pubs/OriolVinyals.html">Oriol Vinyals</a>, <a href="http://research.google.com/pubs/GregCorrado.html">Greg Corrado</a>, <a href="http://research.google.com/pubs/JonathonShlens.html">Jon Shlens</a>, <a href="http://people.cs.umass.edu/~luke/">Luke Vilnis</a>, and <a href="http://www.cs.toronto.edu/~ilya/">Ilya Sutskever</a>. I’m also thankful to many other friends and colleagues for taking the time to help me, including <a href="https://www.linkedin.com/pub/dario-amodei/4/493/393">Dario Amodei</a>, and <a href="http://cs.stanford.edu/~jsteinhardt/">Jacob Steinhardt</a>. I’m especially thankful to <a href="http://www.kyunghyuncho.me/">Kyunghyun Cho</a> for extremely thoughtful correspondence about my diagrams.<br>我非常感谢 Google 同事提供的有益反馈，尤其是 Oriol Vinyals、Greg Corrado、Jon Shlens、Luke Villnis 和 Ilya Sutskever。我还要感谢许多其他朋友和同事抽出时间帮助我，包括 Dario Amodei 和 Jacob Steinhardt。我特别感谢 Kyunghyun Cho 对我的图表进行了非常周到的通信。</p><p>Before this post, I practiced explaining LSTMs during two seminar series I taught on neural networks. Thanks to everyone who participated in those for their patience with me, and for their feedback.<br>在这篇文章之前，我在我教授的关于神经网络的两个系列研讨会上练习了解释 LSTM。感谢所有参与活动的人对我的耐心和反馈。</p><h1 id="注释-如何理解门控结构的计算"><a href="#注释-如何理解门控结构的计算" class="headerlink" title="注释-如何理解门控结构的计算"></a>注释-如何理解门控结构的计算</h1><p>根据前面的文章， 我们已经知道基础 神经网络和 基础RNN 中，数据从输入层到隐藏层到输出层的计算，这里再复习一下</p><h2 id="基础神经网络"><a href="#基础神经网络" class="headerlink" title="基础神经网络"></a>基础神经网络</h2><p><strong>隐藏层</strong><br>$h_t​=f(W_{xh​}x_t​+b_h​)$</p><ul><li>$x_t$​：当前输入</li><li>$W_{xh}$：输入层到隐藏层的权重矩阵</li><li>$b_h$​：偏置</li><li>$f$：激活函数（如tanh或ReLU）</li></ul><p>计算隐藏状态分为2个步骤</p><ol><li>计算隐藏层的输入加权和：</li><li>应用激活函数，计算隐藏层的输出<h2 id="基础rnn"><a href="#基础RNN" class="headerlink" title="基础RNN"></a>基础RNN</h2></li></ol><p>RNN的隐藏层具有循环连接，即多了一个隐藏层到隐藏层的权重矩阵参与计算 ，使得每个隐藏状态依赖于前一时间步的隐藏状态和当前时间步的输入。公式如下：<br>$h_t​=f(W_{hh}​h_{t−1}​+W_{xh​}x_t​+b_h​)$</p><ul><li>$h_t​$：当前时间步的隐藏状态</li><li>$h_{t-1}$：前一时间步的隐藏状态</li><li>$x_t$​：当前时间步的输入</li><li>$W_{hh}$​：隐藏状态到隐藏状态的权重矩阵</li><li>$W_{xh}$：输入到隐藏状态的权重矩阵</li><li>$b_h$​：偏置</li><li>$f$：激活函数（如tanh或ReLU）</li></ul><p>从上面文章中可以看到， 不论计算过程在复杂，都是要根据输入求输出。。 而在LSTM 中， 复杂的点在于。隐藏层的计算由简单的隐藏层-隐藏层权重矩阵参与计算 拆分成了多个步骤</p><h2 id="lstm"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h2><h3 id="1-遗忘门forget-gate"><a href="#1-遗忘门（Forget-Gate）" class="headerlink" title="1. 遗忘门（Forget Gate）"></a>1. 遗忘门（Forget Gate）</h3><p>遗忘门控制单元状态中哪些信息需要被保留或丢弃。遗忘门接收当前时间步的输入 $x_t$和前一时间步的隐藏状态 $h_{t-1}$，通过一个$Sigmoid$函数计算得到一个介于0和1之间的标量（或向量），用于缩放前一时间步的细胞状态。</p><p>公式如下： $f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)$<br>如果把层级关系也在公式中体现出来，该公式可以细化成如下格式：<br>$f_t^l = \sigma(W_f \cdot [h_{t-1}^l, x_t^{l-1}] + b_f)$</p><p>其中 $x$  也可以替换成其他变量，只要是代表当前时间步的输入即可。<br>例如在 <a href="https://arxiv.org/pdf/1409.2329">RECURRENT NEURAL NETWORK REGULARIZATION</a>   该公式就表示成了 $f_t^l = \sigma(W_f \cdot [h_{t-1}^l, h_t^{l-1}] + b_f)$</p><ul><li>$[h_t, x_{t-1}]$或者$[h_t^{l-1}, h_{t-1}^l]$表示将当前输入和前一时间步的隐藏状态向量拼接成一个向量。</li><li>$W_f​$ 是该遗忘门的权重矩阵。</li><li>$b_f$​ 是偏置向量。</li><li>$\sigma$ 是$sigmoid$ 非线性激活函数，输出范围在0到1之间。</li></ul><h3 id="2-输入门input-gate"><a href="#2-输入门（Input-Gate）" class="headerlink" title="2. 输入门（Input Gate）"></a>2. 输入门（Input Gate）</h3><p>输入门控制新信息写入单元状态的过程。输入门同样接收当前时间步的输入 $x_t$和前一时间步的隐藏状态 $h_{t-1}$，并通过Sigmoid函数生成一个介于0和1之间的标量，表示允许多少新信息进入细胞状态。0表示完全不允许新信息进入，1表示完全允许新信息进入。<br>$tanh$层生成候选单元状态。</p><p>公式如下：<br>$i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)$</p><p>$\tilde{C}_t = \tanh(W_C \cdot [h_{t-1}, x_t] + b_C)$</p><p>$W_i​$：输入门的权重矩阵，用于将前一时间步的隐藏状态和当前时间步的输入进行线性变换。<br>$W_C​$：候选细胞状态的权重矩阵，用于将前一时间步的隐藏状态和当前时间步的输入进行线性变换。</p><h3 id="3-单元状态cell-state"><a href="#3-单元状态（Cell-State）" class="headerlink" title="3. 单元状态（Cell State）"></a>3. 单元状态（Cell State）</h3><p>单元状态 $C_t$​ 是LSTM单元内部的长期记忆，它在时间步之间几乎直接传递，通过遗忘门和输入门的调节进行更新。新的单元状态由前一时间步的单元状态乘以遗忘门的输出加上输入门输出和候选值的乘积得到。</p><p>公式如下：$C_t = f_t \cdot C_{t-1} + i_t \cdot \tilde{C}_t$</p><h3 id="4-输出门output-gate-得到隐藏状态"><a href="#4-输出门（Output-Gate）-得到隐藏状态" class="headerlink" title="4. 输出门（Output Gate）- 得到隐藏状态"></a>4. 输出门（Output Gate）- 得到隐藏状态</h3><p>输出门决定哪些信息从细胞状态传递到隐藏状态（LSTM单元的输出）。输出门通过Sigmoid函数决定哪些信息将被输出，并将细胞状态通过Tanh层处理后乘以该输出。</p><p>公式如下：<br>$o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)$<br>$h_t = o_t \cdot \tanh(C_t)$</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;a href=&quot;https://colah.github.io/posts/2015-08-Understanding-LSTMs/&quot;&gt;Understanding LSTM Networks&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;recurrent-neural-network</summary>
      
    
    
    
    
    <category term="AI" scheme="http://example.com/tags/AI/"/>
    
    <category term="神经网络" scheme="http://example.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    <category term="Ilya sutskever‘s 30  papers" scheme="http://example.com/tags/Ilya-sutskever%E2%80%98s-30-papers/"/>
    
  </entry>
  
  <entry>
    <title>RECURRENT NEURAL NETWORK REGULARIZATION</title>
    <link href="http://example.com/7057a5e3/"/>
    <id>http://example.com/7057a5e3/</id>
    <published>2024-06-12T10:10:58.000Z</published>
    <updated>2024-06-16T14:23:22.616Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://arxiv.org/pdf/1409.2329">RECURRENT NEURAL NETWORK REGULARIZATION</a></p><h1 id="abstract-摘要"><a href="#ABSTRACT-摘要" class="headerlink" title="ABSTRACT 摘要"></a>ABSTRACT 摘要</h1><p>We present a simple regularization technique for Recurrent Neural Networks (RNNs) with Long Short-Term Memory (LSTM) units. Dropout, the most suc- cessful technique for regularizing neural networks, does not work well with RNNs and LSTMs. In this paper, we show how to correctly apply dropout to LSTMs, and show that it substantially reduces overfitting on a variety of tasks. These tasks include language modeling, speech recognition, image caption generation, and machine translation.<br>我们提出了一种用于长短期记忆（LSTM）单元的循环神经网络（RNN）的简单正则化技术。最成功的正则化神经网络技术——Dropout，在RNN和LSTM上效果不好。在本文中，我们展示了如何正确地将Dropout应用于LSTM，并证明它在各种任务上显著减少了过拟合。这些任务包括语言建模、语音识别、图像描述生成和机器翻译。</p><h1 id="1-introduction-引言"><a href="#1-INTRODUCTION-引言" class="headerlink" title="1 INTRODUCTION  引言"></a>1 INTRODUCTION  引言</h1><p>The Recurrent Neural Network (RNN) is neural sequence model that achieves state of the art per- formance on important tasks that include language modeling Mikolov (2012), speech recognition Graves et al. (2013), and machine translation Kalchbrenner &amp; Blunsom (2013). It is known that successful applications of neural networks require good regularization. Unfortunately, dropout Srivastava (2013), the most powerful regularization method for feedforward neural networks, does not work well with RNNs. As a result, practical applications of RNNs often use models that are too small because large RNNs tend to overfit. Existing regularization methods give relatively small improvements for RNNs Graves (2013). In this work, we show that dropout, when correctly used, greatly reduces overfitting in LSTMs, and evaluate it on three different problems.</p><p>The code for this work can be found in <a href="https://github.com/wojzaremba/lstm">https://github.com/wojzaremba/lstm</a>.</p><p>递归神经网络（RNN）是一种神经序列模型，可在重要任务上实现最先进的性能，包括语言建模Mikolov（2012），语音识别Graves等人（2013）和机器翻译Kalchbrenner&amp;Blunsom（2013）。众所周知，神经网络的成功应用需要良好的正则化。不幸的是，dropout Srivastava （2013） 是前馈神经网络最强大的正则化方法，但不能很好地用于 RNN。因此，RNN 的实际应用通常使用太小的模型，因为大型 RNN 往往会过度拟合。现有的正则化方法对RNNs Graves（2013）进行了相对较小的改进。在这项工作中，我们表明，如果正确使用，压差可以大大减少LSTM中的过拟合，并在三个不同的问题上对其进行评估。</p><p>此工作的代码可以在<a href="https://github.com/wojzaremba/lstm%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/wojzaremba/lstm</a>找到。</p><h1 id="2-related-work"><a href="#2-RELATED-WORK" class="headerlink" title="2 RELATED WORK"></a>2 RELATED WORK</h1><p>Dropout Srivastava (2013) is a recently introduced regularization method that has been very suc- cessful with feed-forward neural networks. While much work has extended dropout in various ways Wang &amp; Manning (2013); Wan et al. (2013), there has been relatively little research in applying it to RNNs. The only paper on this topic is by Bayer et al. (2013), who focuses on “marginalized dropout” Wang &amp; Manning (2013), a noiseless deterministic approximation to standard dropout. Bayer et al. (2013) claim that conventional dropout does not work well with RNNs because the re- currence amplifies noise, which in turn hurts learning. In this work, we show that this problem can be fixed by applying dropout to a certain subset of the RNNs’ connections. As a result, RNNs can now also benefit from dropout.<br>Dropout Srivastava（2013）是一种最近引入的正则化方法，在前馈神经网络中非常成功。尽管很多工作以各种方式扩展了Dropout Wang &amp; Manning（2013）；Wan等（2013），但在RNN上应用它的研究相对较少。关于这个主题的唯一论文是Bayer等人（2013）的，他们专注于“边缘化Dropout” Wang &amp; Manning（2013），这是标准Dropout的一种无噪声确定性近似。Bayer等人（2013）认为传统的Dropout在RNN上效果不好，因为递归放大了噪声，进而影响了学习。在这项工作中，我们展示了通过将Dropout应用于RNN连接的某个子集可以解决这个问题。因此，RNN现在也可以受益于Dropout。</p><p>Independently of our work, Pham et al. (2013) developed the very same RNN regularization method and applied it to handwriting recognition. We rediscovered this method and demonstrated strong empirical results over a wide range of problems. Other work that applied dropout to LSTMs is Pachitariu &amp; Sahani (2013).<br>独立于我们的工作，Pham等人（2013）开发了完全相同的RNN正则化方法并将其应用于手写识别。我们重新发现了这种方法，并在广泛的问题上展示了强大的实证结果。其他将Dropout应用于LSTM的工作包括Pachitariu &amp; Sahani（2013）。</p><p>There have been a number of architectural variants of the RNN that perform better on problems with long term dependencies Hochreiter &amp; Schmidhuber (1997); Graves et al. (2009); Cho et al. (2014); Jaeger et al. (2007); Koutník et al. (2014); Sundermeyer et al. (2012). In this work, we show how to correctly apply dropout to LSTMs, the most commonly-used RNN variant; this way of applying dropout is likely to work well with other RNN architectures as well. In this paper, we consider the following tasks: language modeling, speech recognition, and machine translation. Language modeling is the first task where RNNs have achieved substantial success Mikolov et al. (2010; 2011); Pascanu et al. (2013). RNNs have also been successfully used for speech recognition Robinson et al. (1996); Graves et al. (2013) and have recently been applied to machine translation, where they are used for language modeling, re-ranking, or phrase modeling Devlin et al. (2014); Kalchbrenner &amp; Blunsom (2013); Cho et al. (2014); Chow et al. (1987); Mikolov et al. (2013).<br>已经有许多RNN的架构变体在处理长期依赖问题上表现更好： Hochreiter &amp; Schmidhuber (1997); Graves等（2009）；Cho等（2014）；Jaeger等（2007）；Koutník等（2014）；Sundermeyer等（2012）。在这项工作中，我们展示了如何正确地将dropout应用于LSTM，这是最常用的RNN变体；这种应用dropout的方法也可能适用于其他RNN架构。在本文中，我们考虑了以下任务：语言建模、语音识别和机器翻译。语言建模是RNN首次取得显著成功的任务 Mikolov等（2010；2011）；Pascanu等（2013）。RNN也已成功应用于语音识别 Robinson等（1996）；Graves等（2013），并且最近被应用于机器翻译，在那里它们被用于语言建模、重排序或短语建模 Devlin等（2014）；Kalchbrenner &amp; Blunsom（2013）；Cho等（2014）；Chow等（1987）；Mikolov等（2013）。</p><h1 id="3-regularizing-rnns-with-lstm-cells-使用lstm单元对rnn进行正则化"><a href="#3-REGULARIZING-RNNS-WITH-LSTM-CELLS-使用LSTM单元对RNN进行正则化" class="headerlink" title="3 REGULARIZING RNNS WITH LSTM CELLS 使用LSTM单元对RNN进行正则化"></a>3 REGULARIZING RNNS WITH LSTM CELLS 使用LSTM单元对RNN进行正则化</h1><p>In this section we describe the deep LSTM (Section 3.1). Next, we show how to regularize them (Section 3.2), and explain why our regularization scheme works.<br>在本节中，我们描述了深度LSTM（3.1节）。接下来，我们展示如何对它们进行正则化（3.2节），并解释我们的正则化方案为何有效。</p><p>We let subscripts denote timesteps and superscripts denote layers. All our states are n-dimensional. Let $h_t^l \in \mathbb{R}^n$ be a hidden state in layer$l$ in timestep $t$. Moreover, let $T_{n,m} : \mathbb{R}^n \to \mathbb{R}^m$be an affine transform ($Wx + b$ for some $W$ and $b$). Let $\odot$ be element-wise multiplication and let $h_t^0​$ be an input word vector at timestep $k$. We use the activations $h_t^L$​ to predict $y_t$​, since $L$ is the number of layers in our deep LSTM.<br>我们用下标表示时间步长，用上标表示层次。我们所有的状态都是n维的。令$h_t^l \in \mathbb{R}^n$ 为时间步$t$中层$l$的隐藏状态。此外，令$T_{n,m} : \mathbb{R}^n \to \mathbb{R}^m$为仿射变换（某些$W$和$b$,$Wx + b$）。令$\odot$为逐元素乘法，并令$h_t^0$​为时间步$k$的输入词向量。我们使用激活值$h_t^L$​来预测$y_t​$，因为$L$是我们深度LSTM的层数。</p><h2 id="31-long-short-term-memory-units-长短期记忆单元"><a href="#3-1-LONG-SHORT-TERM-MEMORY-UNITS-长短期记忆单元" class="headerlink" title="3.1 LONG-SHORT TERM MEMORY UNITS 长短期记忆单元"></a><strong>3.1 LONG-SHORT TERM MEMORY UNITS</strong> 长短期记忆单元</h2><p>The RNN dynamics can be described using deterministic transitions from previous to current hidden states. The deterministic state transition is a function<br>RNN的动态可以用从先前隐藏状态到当前隐藏状态的确定性转换来描述。确定性状态转换是一个函数</p><p>RNN : $h_t^{l-1}​$, $h_{t-1}^l \rightarrow h_t^l$</p><p>For classical RNNs, this function is given by<br>$h_t^l = f(T_{n,n} h_t^{l-1} + T_{n,n} h_{t-1}^l), where f \in \{\text{sigm, tanh}\}$</p><p>The LSTM has complicated dynamics that allow it to easily “memorize” information for an extended number of timesteps. The “long term” memory is stored in a vector of memory cells $c_t^l \in \mathbb{R}^n$. Although many LSTM architectures that differ in their connectivity structure and activation functions, all LSTM architectures have explicit memory cells for storing information for long periods of time. The LSTM can decide to overwrite the memory cell, retrieve it, or keep it for the next time step. The LSTM architecture used in our experiments is given by the following equations Graves et al. (2013):<br>LSTM具有复杂的动态，允许它轻松地“记住”多个时间步长的信息。“长期”记忆存储在记忆单元向量$c_t^l \in \mathbb{R}^n$中。尽管许多LSTM架构在连接结构和激活函数上有所不同，但所有LSTM架构都有明确的记忆单元用于长时间存储信息。LSTM可以决定覆盖记忆单元、检索或者在下一个时间步中保留它。我们实验中使用的LSTM架构由以下方程给出 Graves等（2013）：</p><p>LSTM : $h_t^{l-1}$, $h_{t-1}^l$, $c_{t-1}^l \rightarrow h_t^l$​, $c_t^l$</p><p>$\left( \begin{array}{c} i \ f \ o \ g \end{array} \right) = \left( \begin{array}{c} \text{sigm} \ \text{sigm} \ \text{sigm} \ \text{tanh} \end{array} \right) T_{2n,4n} \left( \begin{array}{c} h_{t}^{l-1} \ h_{t-1}^{l} \end{array} \right)​$</p><p>$c_t^l = f \odot c_{t-1}^l + i \odot g$</p><p>$h_t^l = o \odot \text{tanh}(c_t^l)$<br>In these equations, sigm and tanh are applied element-wise. Figure 1 illustrates the LSTM equations.<br>在这些方程中，sigm和tanh逐元素应用。图1展示了LSTM方程</p><h2 id="32-regularization-with-dropout"><a href="#3-2-REGULARIZATION-WITH-DROPOUT" class="headerlink" title="3.2 REGULARIZATION WITH DROPOUT"></a>3.2 REGULARIZATION WITH DROPOUT</h2><p>The main contribution of this paper is a recipe for applying dropout to LSTMs in a way that success-fully reduces overfitting. The main idea is to apply the dropout operator only to the non-recurrent<br>本文的主要贡献是提供了一种将dropout应用于LSTM的方法，从而成功地减少了过拟合。主要思想是仅将dropout操作符应用于非递归连接。<br><img src="/7057a5e3/1.png" class><br>Figure 1: A graphical representation of LSTM memory cells used in this paper (there are minor differences in comparison to Graves (2013)).<br>图1：本文中使用的LSTM记忆单元的图形表示（与Graves（2013）相比有细微差别）。<br><img src="/7057a5e3/2.png" class></p><p>Figure 2: Regularized multilayer RNN. The dashed arrows indicate connections where dropout is applied, and the solid lines indicate connections where dropout is not applied.<br>图2：正则化的多层RNN。虚线箭头表示应用了dropout的连接，实线表示未应用dropout的连接。<br>⚠️： x 表示输入层， y 表示输出层</p><p>connections (Figure 2). The following equation describes it more precisely, where D is the dropoutoperator that sets a random subset of its argument to zero:<br>连接（图2）。以下方程更准确地描述了这一点，其中 $D$ 是将其参数的随机子集设置为零的dropout操作符：</p><p>$\left( \begin{array}{c} i \ f \ o \ g \end{array} \right) = \left( \begin{array}{c} \text{sigm} \ \text{sigm} \ \text{sigm} \ \text{tanh} \end{array} \right) T_{2n,4n} \left( \begin{array}{c} {D}(h_{t}^{l-1}) \ h_{t-1}^{l} \end{array} \right)​$</p><p>$c_t^l = f \odot c_{t-1}^l + i \odot g$</p><p>$h_t^l = o \odot \text{tanh}(c_t^l)$</p><p>Our method works as follows. The dropout operator corrupts the information carried by the units,forcing them to perform their intermediate computations more robustly. At the same time, we do not want to erase all the information from the units. It is especially important that the units remember<br>events that occurred many timesteps in the past. Figure 3 shows how information could flow from an event that occurred at timestep t − 2 to the prediction in timestep t + 2 in our implementation of dropout. We can see that the information is corrupted by the dropout operator exactly L + 1 times,<br>我们的方法如下。dropout 运算符会破坏单元携带的信息，迫使它们更稳健地执行中间计算。同时，我们不想抹去单元的所有信息。特别重要的是，单元需要记住许多时间步长之前发生的事件。图3显示了在我们实现的dropout中，信息如何从时间步 $t-2$ 传递到时间步 $t+2$ 的预测。我们可以看到，信息恰好被dropout操作符破坏了 $L+1$ 次。</p><img src="/7057a5e3/3.png" class><p>Figure 3: The thick line shows a typical path of information flow in the LSTM. The information is affected by dropout L + 1 times, where L is depth of network.<br>图 3：粗线显示了 LSTM 中信息流的典型路径。信息受 L + 1 次的dropout影响，其中 L 是网络深度。</p><img src="/7057a5e3/4.png" class><p>Figure 4: Some interesting samples drawn from a large regularized model conditioned on “The meaning of life is”. We have removed “unk”, “N”, “$” from the set of permissible words.<br>图4：从一个以“The meaning of life is”为条件的大型正则化模型中抽取的一些有趣样本。我们已经从允许的单词集中移除了“unk”、“N”、“$”。</p><p>and this number is independent of the number of timesteps traversed by the information. Standard dropout perturbs the recurrent connections, which makes it difficult for the LSTM to learn to store information for long periods of time. By not using dropout on the recurrent connections, the LSTM can benefit from dropout regularization without sacrificing its valuable memorization ability.<br>这个数字与信息经过的时间步数无关。标准的dropout会扰乱递归连接，这使得LSTM难以学习长时间存储信息。通过不在递归连接上使用dropout，LSTM可以从dropout正则化中受益，而不牺牲其宝贵的记忆能力。</p><h1 id="4-experiments-实验"><a href="#4-EXPERIMENTS-实验" class="headerlink" title="4 EXPERIMENTS   实验"></a>4 EXPERIMENTS   实验</h1><p>We present results in three domains: language modeling (Section 4.1), speech recognition (Section 4.2), machine translation (Section 4.3), and image caption generation (Section 4.4).<br>我们在三个领域中展示了结果：语言建模（第4.1节）、语音识别（第4.2节）、机器翻译（第4.3节）和图像描述生成（第4.4节）。</p><h2 id="41-language-modeling-语言建模"><a href="#4-1-LANGUAGE-MODELING-语言建模" class="headerlink" title="4.1 LANGUAGE MODELING   语言建模"></a>4.1 LANGUAGE MODELING   语言建模</h2><p>We conducted word-level prediction experiments on the Penn Tree Bank (PTB) dataset Marcus et al. (1993), which consists of 929k training words, 73k validation words, and 82k test words. It has 10k words in its vocabulary. We downloaded it from Tomas Mikolov’s webpage†. We trained regularized LSTMs of two sizes; these are denoted the medium LSTM and large LSTM. Both LSTMs have two layers and are unrolled for 35 steps. We initialize the hidden states to zero. We then use the final hidden states of the current minibatch as the initial hidden state of the subsequent minibatch (successive minibatches sequentially traverse the training set). The size of each minibatch is 20.<br>我们在Penn Tree Bank (PTB)数据集上进行了词级预测实验，该数据集包括92.9万个训练词、7.3万个验证词和8.2万个测试词。其词汇表有1万个单词。我们从Tomas Mikolov的网页下载了该数据集。我们训练了两种规模的正则化LSTM；它们分别被称为中型LSTM和大型LSTM。两个LSTM都有两层，展开35步。我们将隐藏状态初始化为零。然后我们使用当前小批量的最终隐藏状态作为后续小批量的初始隐藏状态（连续的小批量依次遍历训练集）。每个小批量的大小为20。<br><img src="/7057a5e3/5.png" class></p><p>The medium LSTM has 650 units per layer and its parameters are initialized uniformly in [−0.05, 0.05]. As described earlier, we apply 50% dropout on the non-recurrent connections. We train the LSTM for 39 epochs with a learning rate of 1, and after 6 epochs we decrease it by a factor of 1.2 after each epoch. We clip the norm of the gradients (normalized by minibatch size) at 5. Training this network takes about half a day on an NVIDIA K20 GPU.<br>中型LSTM每层有650个单元，其参数在[−0.05, 0.05]范围内均匀初始化。如前所述，我们在非递归连接上应用50%的dropout。我们用学习率为1训练LSTM共39个周期，在第6个周期后，每个周期将学习率按1.2的因子递减。我们将梯度的范数（按小批量大小归一化）剪裁到5。训练该网络在NVIDIA K20 GPU上大约需要半天时间。</p><p>The large LSTM has 1500 units per layer and its parameters are initialized uniformly in [−0.04, 0.04]. We apply 65% dropout on the non-recurrent connections. We train the model for 55 epochs with a learning rate of 1; after 14 epochs we start to reduce the learning rate by a factor of 1.15 after each epoch. We clip the norm of the gradients (normalized by minibatch size) at 10 Mikolov et al. (2010). Training this network takes an entire day on an NVIDIA K20 GPU.<br>大型LSTM每层有1500个单元，其参数在[−0.04, 0.04]范围内均匀初始化。我们在非递归连接上应用65%的dropout。我们用学习率为1训练模型共55个周期；在第14个周期后，每个周期开始按1.15的因子递减学习率。我们将梯度的范数（按小批量大小归一化）剪裁到10 Mikolov等（2010）。训练该网络在NVIDIA K20 GPU上需要整整一天时间。</p><p>For comparison, we trained a non-regularized network. We optimized its parameters to get the best validation performance. The lack of regularization effectively constrains size of the network, forcing us to use small network because larger networks overfit. Our best performing non-regularized LSTM has two hidden layers with 200 units per layer, and its weights are initialized uniformly in [−0.1, 0.1]. We train it for 4 epochs with a learning rate of 1 and then we decrease the learning rate by a factor of 2 after each epoch, for a total of 13 training epochs. The size of each minibatch is 20, and we unroll the network for 20 steps. Training this network takes 2-3 hours on an NVIDIA K20 GPU.<br>为了比较，我们训练了一个未正则化的网络。我们优化其参数以获得最佳验证性能。缺乏正则化有效地限制了网络的大小，迫使我们使用小型网络，因为较大的网络会过拟合。我们表现最好的未正则化LSTM有两层隐藏层，每层200个单元，其权重在[−0.1, 0.1]范围内均匀初始化。我们用学习率为1训练了4个周期，然后每个周期将学习率按2的因子递减，总共训练13个周期。每个小批量的大小为20，我们展开网络20步。训练该网络在NVIDIA K20 GPU上需要2-3小时。</p><p>Table 1 compares previous results with our LSTMs, and Figure 4 shows samples drawn from a single large regularized LSTM.<br>表1比较了以前的结果和我们的LSTM，图4显示了从单个大型正则化LSTM中抽取的样本。</p><h2 id="42-speech-recognition-语音识别"><a href="#4-2-SPEECH-RECOGNITION-语音识别" class="headerlink" title="4.2 SPEECH RECOGNITION   语音识别"></a>4.2 SPEECH RECOGNITION   语音识别</h2><p>Deep Neural Networks have been used for acoustic modeling for over half a century (see Bourlard &amp; Morgan (1993) for a good review). Acoustic modeling is a key component in mapping acoustic signals to sequences of words, as it models $p(s_t|X)$ where $s_t$​ is the phonetic state at time $t$ and $X$ is the acoustic observation. Recent work has shown that LSTMs can achieve excellent performance on acoustic modeling Sak et al. (2014), yet relatively small LSTMs (in terms of the number of their parameters) can easily overfit the training set. A useful metric for measuring the performance of acoustic models is frame accuracy, which is measured at each sts_tst​ for all timesteps ttt. Generally, this metric correlates with the actual metric of interest, the Word Error Rate (WER).<br>深度神经网络已经被用于声学建模超过半个世纪（参见Bourlard &amp; Morgan (1993)的良好综述）。声学建模是将声学信号映射到单词序列中的关键组成部分，因为它对p(st∣X)p(s_t|X)p(st​∣X)建模，其中sts_tst​是时间ttt的语音状态，XXX是声学观测。最近的工作表明，LSTM在声学建模上可以取得优异的性能 Sak等（2014），但相对较小的LSTM（就参数数量而言）很容易对训练集过拟合。衡量声学模型性能的一个有用指标是帧准确率，它在所有时间步长ttt处测量每个sts_tst​的准确率。通常，这个指标与实际感兴趣的指标，即单词错误率（WER）相关。</p><img src="/7057a5e3/6.png" class><p>Since computing the WER involves using a language model and tuning the decoding parameters for every change in the acoustic model, we decided to focus on frame accuracy in these experiments. Table 2 shows that dropout improves the frame accuracy of the LSTM. Not surprisingly, the training frame accuracy drops due to the noise added during training, but as is often the case with dropout, this yields models that generalize better to unseen data. Note that the test set is easier than the training set, as its accuracy is higher. We report the performance of an LSTM on an internal Google Icelandic Speech dataset, which is relatively small (93k utterances), so overfitting is a great concern.<br>由于计算WER涉及使用语言模型并调整声学模型每次变化的解码参数，我们决定在这些实验中专注于帧准确率。表2显示了dropout提高了LSTM的帧准确率。不出所料，由于训练过程中加入的噪声，训练帧准确率下降了，但与dropout经常出现的情况一样，这使得模型在未见数据上的泛化能力更强。请注意，测试集比训练集更容易，因为它的准确率更高。我们报告了LSTM在Google内部冰岛语语音数据集上的性能，该数据集相对较小（93k句子），因此过拟合是一个很大的问题。</p><h2 id="43-machine-translation-机器翻译"><a href="#4-3-MACHINE-TRANSLATION-机器翻译" class="headerlink" title="4.3 MACHINE TRANSLATION  机器翻译"></a>4.3 MACHINE TRANSLATION  机器翻译</h2><p>We formulate a machine translation problem as a language modelling task, where an LSTM is trained to assign high probability to a correct translation of a source sentence. Thus, the LSTM is trained on concatenations of source sentences and their translations Sutskever et al. (2014) (see also Cho et al. (2014)). We compute a translation by approximating the most probable sequence of words using a simple beam search with a beam of size 12. We ran an LSTM on the WMT’14 English to French dataset, on the “selected” subset from Schwenk (2014) which has 340M French words and 304M English words. Our LSTM has 4 hidden layers, and both its layers and word embeddings have 1000 units. Its English vocabulary has 160,000 words and its French vocabulary has 80,000 words. The optimal dropout probability was 0.2. Table 3 shows the performance of an LSTM trained with and without dropout. While our LSTM does not beat the phrase-based LIUM SMT system Schwenk et al. (2011), our results show that dropout improves the translation performance of the LSTM.<br>我们将机器翻译问题表述为一个语言建模任务，其中LSTM被训练为对源句子的正确翻译赋予高概率。因此，LSTM在源句子及其翻译的串联上进行训练 Sutskever等（2014）（另见Cho等（2014））。我们通过使用大小为12的简单束搜索来近似最可能的单词序列来计算翻译。我们在WMT’14英法数据集上的“selected”子集（来自Schwenk（2014），包含3.4亿个法语单词和3.04亿个英语单词）上运行了一个LSTM。我们的LSTM有4个隐藏层，其层和词嵌入都有1000个单元。它的英语词汇量有160,000个单词，法语词汇量有80,000个单词。最佳的dropout概率是0.2。表3显示了使用和不使用dropout训练的LSTM的性能。虽然我们的LSTM没有击败基于短语的LIUM SMT系统 Schwenk等（2011），但我们的结果表明dropout提高了LSTM的翻译性能。</p><h2 id="44-image-caption-generation图像描述生成"><a href="#4-4-IMAGE-CAPTION-GENERATION图像描述生成" class="headerlink" title="4.4 IMAGE CAPTION GENERATION图像描述生成"></a>4.4 IMAGE CAPTION GENERATION图像描述生成</h2><p>We applied the dropout variant to the image caption generation model of Vinyals et al. (2014). The image caption generation is similar to the sequence-to-sequence model of Sutskever et al. (2014), but where the input image is mapped onto a vector with a highly-accurate pre-trained convolutional neural network (Szegedy et al., 2014), which is converted into a caption with a single-layer LSTM (see Vinyals et al. (2014) for the details on the architecture). We test our dropout scheme on LSTM as the convolutional neural network is not trained on the image caption dataset because it is not large (MSCOCO (Lin et al., 2014)).<br>我们将dropout变体应用于Vinyals等人（2014）的图像描述生成模型。图像描述生成类似于Sutskever等人（2014）的序列到序列模型，但输入图像被映射到一个具有高精度的预训练卷积神经网络（Szegedy等人，2014）的向量，该向量通过单层LSTM转换为描述（有关架构的详细信息，请参见Vinyals等人，2014）。我们在LSTM上测试了我们的dropout方案，因为卷积神经网络并未在图像描述数据集上进行训练，因为它不是很大（MSCOCO（Lin等人，2014））。</p><p>Our results are summarized in the following Table 4. In brief, dropout helps relative to not using dropout, but using an ensemble eliminates the gains attained by dropout. Thus, in this setting, the main effect of dropout is to produce a single model that is as good as an ensemble, which is a reasonable improvement given the simplicity of the technique.<br>我们的结果总结在以下表4中。简而言之，dropout相对于不使用dropout有帮助，但使用集成方法消除了通过dropout获得的收益。因此，在这种情况下，dropout的主要作用是产生一个与集成一样好的单一模型，考虑到该技术的简单性，这是一个合理的改进。</p><h1 id="5-conclusion"><a href="#5-CONCLUSION" class="headerlink" title="5 CONCLUSION"></a>5 CONCLUSION</h1><p>We presented a simple way of applying dropout to LSTMs that results in large performance increases on several problems in different domains. Our work makes dropout useful for RNNs, and our results suggest that our implementation of dropout could improve performance on a wide variety of applications.<br>我们提出了一种将dropout应用于LSTM的简单方法，这在不同领域的几个问题上导致了性能的大幅提升。我们的工作使dropout对RNN有用，并且我们的结果表明，我们实现的dropout可以提高各种应用的性能。<br><img src="/7057a5e3/7.png" class></p><h1 id="6-acknowledgments"><a href="#6-ACKNOWLEDGMENTS" class="headerlink" title="6 ACKNOWLEDGMENTS"></a>6 ACKNOWLEDGMENTS</h1><p>We wish to acknowledge Tomas Mikolov for useful comments on the first version of the paper.<br>我们希望感谢Tomas Mikolov对论文第一版提出的有益意见。</p><h1 id="注释"><a href="#注释" class="headerlink" title="注释"></a>注释</h1><h2 id="1-元素乘法"><a href="#1-元素乘法" class="headerlink" title="1. 元素乘法"></a>1. 元素乘法</h2><p><strong>元素乘法</strong>（Element-wise multiplication），也称为Hadamard乘积（Hadamard product），是对两个同形矩阵或向量的对应元素进行逐一相乘的操作，广泛应用于各种线性代数和神经网络计算中。 用符号“⊙”表示。</p><h3 id="公式表示"><a href="#公式表示" class="headerlink" title="公式表示"></a>公式表示</h3><p>给定两个相同大小的矩阵或向量 $A$ 和 $B$，其元素乘法 $C$ 计算如下：<br>$C = A \odot B$</p><p>其中：</p><ul><li>$A = [a_1, a_2, …, a_n]$</li><li>$B = [b_1, b_2, …, b_n]$</li><li>$C = [c_1, c_2, …, c_n]$</li><li>$c_i = a_i \cdot b_i$<h3 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h3></li></ul><p>假设有两个向量 $A$ 和 $B$：</p><p>$A=[1,2,3]$<br>$B=[4,5,6]$</p><p>它们的元素乘法 $C$ 为：<br>$C = A \odot B = [1 \cdot 4, 2 \cdot 5, 3 \cdot 6] = [4, 10, 18]$</p><h3 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h3><ol><li><strong>神经网络中的LSTM</strong>：<ul><li>用于更新单元状态，如公式 $c_t = f \odot c_{t-1} + i \odot \tilde{c}_t$中。</li></ul></li><li><strong>图像处理</strong>：<ul><li>用于图像滤波，将滤波器应用于图像的每个像素。</li></ul></li><li><strong>数据处理</strong>：<ul><li>在数据预处理中，用于按元素缩放或调整数据。</li></ul></li></ol><h2 id="2-公式拆解rnn"><a href="#2-公式拆解-RNN" class="headerlink" title="2. 公式拆解:RNN"></a>2. 公式拆解:RNN</h2><p>RNN : $h_t^{l-1}​$, $h_{t-1}^l \rightarrow h_t^l$</p><p>表明在RNN中， 隐藏状态的计算结果依赖于当前时间步的输入 $h_t$和前一时间步的隐藏状态 $h_{t-1}$。 再细化一点，</p><ol><li>当前时间步的输入 $h_t$ 应该来源于上一层，所以是 $h_t^{l-1}​$</li><li>前一时间步的隐藏状态 $h_{t-1}$ ，应该是同一层的前一个时间步， 所以是$h_{t-1}^l$</li></ol><p>该状态转移过程，如果用具体的数学公式表示，可以如下所示<br>$h_t^l = f(T_{n,n} h_t^{l-1} + T_{n,n} h_{t-1}^l), where f \in \{\text{sigm, tanh}\}$</p><h2 id="3-公式拆解lstm-状态更新"><a href="#3-公式拆解-LSTM-状态更新" class="headerlink" title="3. 公式拆解:LSTM  状态更新"></a>3. 公式拆解:LSTM  状态更新</h2><p>LSTM : $h_t^{l-1}$, $h_{t-1}^l$, $c_{t-1}^l \rightarrow h_t^l$​, $c_t^l$<br>描述了LSTM如何通过当前层的输入向量 $h_t^{l-1}$、前一时间步的隐藏状态 $h_{t-1}^l$和单元状态 $c_{t-1}^l$ 来生成新的隐藏状态 $h_t^l$ 和单元状态 $c_t^l$。</p><ol><li>$h_t^{l-1}$：表示第 $l−1$ 层在时间步 $t$ 的隐藏状态向量。这是第 $l$ 层的当前输入。</li><li>$h_{t-1}^l$：表示第 $l$ 层在时间步 $t−1$ 的隐藏状态向量。这是第 $l$ 层的前一个时间步的状态。</li><li>$c_{t-1}^l$：表示第 $l$ 层在时间步 $t−1$的单元状态向量。这是第 $l$ 层的前一个时间步的单元状态。</li><li>$h_t^l$：表示第 $l$ 层在时间步 $t$ 的隐藏状态向量。这是经过第 $l$ 层计算后的新状态。</li><li>$c_t^l$：表示第 $l$ 层在时间步 $t$ 的新的单元状态向量。这是更新后的单元状态。</li></ol><p>$\left( \begin{array}{c} i \ f \ o \ g \end{array} \right) = \left( \begin{array}{c} \text{sigm} \ \text{sigm} \ \text{sigm} \ \text{tanh} \end{array} \right) T_{2n,4n} \left( \begin{array}{c} h_{t}^{l-1} \ h_{t-1}^{l} \end{array} \right)​$</p><p>描述了输入门 $i$、遗忘门 $f$、输出门 $o$ 和候选记忆单元 $g$ 的计算。这里，矩阵 $T_{2n,4n}$​ 包含了相应的权重，输入包括当前输入 $h_t^{l-1}$和前一时间步的隐藏状态 $h_{t-1}^l$​</p><ul><li><strong>输入门 $i$</strong> 和 <strong>遗忘门 $f$</strong> 控制信息的更新和遗忘，使用$sigmoid$激活函数。</li><li><strong>输出门 $o$</strong> 控制输出信息，使用$sigmoid$激活函数。</li><li><strong>候选记忆单元 $g$</strong> 提供新的信息内容，使用$tanh$激活函数。</li><li><strong>权重矩阵 $T_{2n,4n}$</strong>  一个大小为 $2n \times 4n$ 的矩阵，其中 $n$ 是隐藏状态向量的维度。将输入向量 $h_t^{l-1}$​ 和隐藏状态向量 $h_{t-1}^l$ 拼接起来（向量长度为 $2n$），并通过矩阵 $T_{2n,4n}$​ 进行线性变换，生成一个长度为 $4n$ 的输出向量,即 $i, f, o, g$ 四个部分<h3 id="1-遗忘门forget-gate"><a href="#1-遗忘门（Forget-Gate）" class="headerlink" title="1. 遗忘门（Forget Gate）"></a>1. 遗忘门（Forget Gate）</h3></li></ul><p>$f_t^l = \sigma(W_f \cdot [h_{t-1}^l, h_t^{l-1}] + b_f)$</p><ul><li>$[h_t^{l-1}, h_{t-1}^l]$表示将当前输入和前一时间步的隐藏状态向量拼接成一个向量。</li><li>$W_f​$ 是该拼接向量的权重矩阵。</li><li>$b_f$​ 是偏置向量。</li><li>$\sigma$ 是$sigmoid$激活函数，输出范围在0到1之间。</li></ul><p>假设 $n = 4$：</p><ul><li>当前输入向量 $h_t^{l-1}$为 $[h_1​,h_2​,h_3​,h_4​]$。</li><li>前一时间步的隐藏状态 $h_{t-1}^l$ 为 $[h_5, h_6, h_7, h_8]$</li></ul><p>拼接后的向量为：$[h_1, h_2, h_3, h_4, h_5, h_6, h_7, h_8]$</p><p>权重矩阵 $W_f​$ 将此向量进行线性变换，生成一个长度为 $n$ 的向量。</p><h4 id="线性变换"><a href="#线性变换" class="headerlink" title="线性变换"></a>线性变换</h4><p>进行线性变换的公式为：<br>$z_f = W_f \cdot [h_t^{l-1}, h_{t-1}^l] + b_f$</p><p>具体步骤：</p><ol><li><strong>矩阵乘法</strong>：<ul><li>$W_f$是一个 $n \times 2n$ 的矩阵，拼接向量是一个长度为 $2n$ 的向量。</li><li>通过矩阵乘法，结果是一个长度为 $n$ 的向量。</li></ul></li><li><strong>加偏置</strong>：<ul><li>将得到的向量与偏置向量 $b_f$ 相加，仍然是一个长度为 $n$ 的向量。</li></ul></li></ol><p>例如，假设 $W_f$​ 和 $b_f$​ 为：<br>$W_f = \begin{pmatrix} w_{11} &amp; w_{12} &amp; w_{13} &amp; w_{14} &amp; w_{15} &amp; w_{16} &amp; w_{17} &amp; w_{18} \ w_{21} &amp; w_{22} &amp; w_{23} &amp; w_{24} &amp; w_{25} &amp; w_{26} &amp; w_{27} &amp; w_{28} \ w_{31} &amp; w_{32} &amp; w_{33} &amp; w_{34} &amp; w_{35} &amp; w_{36} &amp; w_{37} &amp; w_{38} \ w_{41} &amp; w_{42} &amp; w_{43} &amp; w_{44} &amp; w_{45} &amp; w_{46} &amp; w_{47} &amp; w_{48} \end{pmatrix}$</p><p>拼接向量为：<br>$[h_1, h_2, h_3, h_4, h_5, h_6, h_7, h_8]$</p><p>矩阵乘法：<br>$z_f = W_f \cdot [h_1, h_2, h_3, h_4, h_5, h_6, h_7, h_8]$</p><p>计算每个元素：<br>$\begin{aligned} z_{f1} &amp;= w_{11}h_1 + w_{12}h_2 + w_{13}h_3 + w_{14}h_4 + w_{15}h_5 + w_{16}h_6 + w_{17}h_7 + w_{18}h_8 \ z_{f2} &amp;= w_{21}h_1 + w_{22}h_2 + w_{23}h_3 + w_{24}h_4 + w_{25}h_5 + w_{26}h_6 + w_{27}h_7 + w_{28}h_8 \ z_{f3} &amp;= w_{31}h_1 + w_{32}h_2 + w_{33}h_3 + w_{34}h_4 + w_{35}h_5 + w_{36}h_6 + w_{37}h_7 + w_{38}h_8 \ z_{f4} &amp;= w_{41}h_1 + w_{42}h_2 + w_{43}h_3 + w_{44}h_4 + w_{45}h_5 + w_{46}h_6 + w_{47}h_7 + w_{48}h_8 \end{aligned}$</p><p>加偏置：<br>$z_f = \begin{pmatrix} z_{f1} + b_1 \ z_{f2} + b_2 \ z_{f3} + b_3 \ z_{f4} + b_4 \end{pmatrix}$</p><h4 id="sigmoid-非线性激活"><a href="#sigmoid-非线性激活" class="headerlink" title="sigmoid 非线性激活"></a>sigmoid 非线性激活</h4><p>通过$sigmoid$激活函数得到遗忘门的激活值：<br>$f_t^l = \sigma(z_f)$<br>通过 $sigmoid$ 非线性激活函数，得到遗忘门的激活值。</p><h3 id="2-输入门input-gate"><a href="#2-输入门（Input-Gate）" class="headerlink" title="2. 输入门（Input Gate）"></a>2. 输入门（Input Gate）</h3><p>计算输入门的激活值，决定新的输入信息的哪些部分将更新单元状态： $i_t^l = \sigma(W_i \cdot [h_{t-1}^l, h_t^{l-1}] + b_i)$</p><p>输入调制门（Input Modulation Gate）输入调制门产生新的候选记忆内容，通过 tanh 函数进行激活。<br>它的数学表示为：$g_t = \tanh(W_g \cdot [h_{t-1}, h_t] + b_g)$</p><h3 id="3-单元状态"><a href="#3-单元状态" class="headerlink" title="3. 单元状态"></a>3. 单元状态</h3><p>结合遗忘门和输入门的信息，更新单元状态：</p><p>$c_t^l = f \odot c_{t-1}^l + i \odot g$</p><p>描述了如何更新单元状态。这里，$\odot$ 表示元素乘法（Hadamard乘积）。</p><ul><li>遗忘门$f$决定了前一时间步的单元状态 $c_{t-1}^l$有多少被保留。遗忘门的输出值在0和1之间：<ul><li>当 $f$​ 接近1时，表示大部分单元状态被保留。</li><li>当 $f$接近0时，表示大部分单元状态被遗忘。</li></ul></li><li>输入门 $i$ 和候选记忆单元 $g$ 决定了多少新的信息被添加到当前单元状态 $c_t^l$。</li></ul><h3 id="4-输出门"><a href="#4-输出门" class="headerlink" title="4. 输出门"></a>4. 输出门</h3><p>计算输出门的激活值，决定隐藏状态的更新：<br>输出门：$o_t^l = \sigma(W_o \cdot [h_{t-1}^l, h_t^{l-1}] + b_o)$<br>隐藏状态：$h_t^l = o_t^l * \tanh(c_t^l)$<br>输出门 $o$ 控制了从单元状态 $c_t^l$ 传递到隐藏状态 $h_t^l$​ 的信息，通过$tanh$函数进行非线性变换。</p><p>LSTM通过输入门、遗忘门、输出门和候选记忆单元的协同作用，有效地捕捉序列数据中的长短期依赖关系，解决了传统RNN中梯度消失和梯度爆炸的问题。这个更新机制使得LSTM在处理长序列数据时表现出色，能够有效地保留重要信息并过滤无关信息。</p><h3 id="5-更新隐藏状态hidden-state-update"><a href="#5-更新隐藏状态（Hidden-State-Update）" class="headerlink" title="5. 更新隐藏状态（Hidden State Update）"></a>5. 更新隐藏状态（Hidden State Update）</h3><p>结合新的单元状态和输出门的激活值，更新隐藏状态： $h_t^l = o_t^l * \tanh(c_t^l)$</p><h2 id="4-应用了dropout-的lstm"><a href="#4-应用了dropout-的LSTM" class="headerlink" title="4. 应用了dropout 的LSTM"></a>4. 应用了dropout 的LSTM</h2><p>从正文中可以看出，和标准的LSTM 状态更新过程相比， 其变化只是增加了一个$D$。<br>$D$ 是将其参数的随机子集设置为零的dropout操作符。</p><p>$\left( \begin{array}{c} i \ f \ o \ g \end{array} \right) = \left( \begin{array}{c} \text{sigm} \ \text{sigm} \ \text{sigm} \ \text{tanh} \end{array} \right) T_{2n,4n} \left( \begin{array}{c} {D}(h_{t}^{l-1}) \ h_{t-1}^{l} \end{array} \right)​$</p><p>如何理解其主要思想是仅将dropout操作符应用于非递归连接。<br>由于：</p><ol><li>$h_t^{l-1}$：表示第 $l−1$ 层在时间步 $t$ 的隐藏状态向量。这是第 $l$ 层的当前输入。</li><li>$h_{t-1}^l$：表示第 $l$ 层在时间步 $t−1$ 的隐藏状态向量。这是第 $l$ 层的前一个时间步的状态。<br>同一层前后时间步之间的数据流转 就是递归操作， 不同层之间的数据流转是非递归操作， 根据公式，$D$ 应用在了$h_t^{l-1}$上， 所以说$D$ 是应用在非递归连接上的操作符合</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1409.2329&quot;&gt;RECURRENT NEURAL NETWORK REGULARIZATION&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;abstract-摘要&quot;&gt;&lt;a href=&quot;#ABSTRACT-摘要&quot; cla</summary>
      
    
    
    
    
    <category term="AI" scheme="http://example.com/tags/AI/"/>
    
    <category term="神经网络" scheme="http://example.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    <category term="Ilya sutskever‘s 30  papers" scheme="http://example.com/tags/Ilya-sutskever%E2%80%98s-30-papers/"/>
    
  </entry>
  
  <entry>
    <title>The Unreasonable Effectiveness of Recurrent Neural Networks</title>
    <link href="http://example.com/2472be8a/"/>
    <id>http://example.com/2472be8a/</id>
    <published>2024-06-11T09:59:43.000Z</published>
    <updated>2024-06-16T14:23:22.626Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://karpathy.github.io/2015/05/21/rnn-effectiveness/">Andrej Karpathy blog:# The Unreasonable Effectiveness of Recurrent Neural Networks</a></p><p>There’s something magical about Recurrent Neural Networks (RNNs). I still remember when I trained my first recurrent network for <a href="http://cs.stanford.edu/people/karpathy/deepimagesent/">Image Captioning</a>. Within a few dozen minutes of training my first baby model (with rather arbitrarily-chosen hyperparameters) started to generate very nice looking descriptions of images that were on the edge of making sense. Sometimes the ratio of how simple your model is to the quality of the results you get out of it blows past your expectations, and this was one of those times. What made this result so shocking at the time was that the common wisdom was that RNNs were supposed to be difficult to train (with more experience I’ve in fact reached the opposite conclusion). Fast forward about a year: I’m training RNNs all the time and I’ve witnessed their power and robustness many times, and yet their magical outputs still find ways of amusing me. This post is about sharing some of that magic with you.<br>循环神经网络（RNN）有其独特的魅力。我还记得第一次训练用于图像描述生成的循环神经网络时的情景。只用了短短几十分钟，即使是随意选择的超参数，这个初步模型已经开始生成看起来非常不错的图像描述，尽管这些描述有时只是勉强合理。有时，模型的简单程度与其输出结果的质量之间的比例会远远超出预期，这次就是一个典型的例子。这次结果如此令人震惊的原因在于，当时的普遍认知是，RNN很难训练（随着经验的增加，我实际上得出了相反的结论）。时间快进大约一年：我一直在训练RNN，目睹了它们的强大和稳健，尽管如此，它们神奇的输出依然能不断带给我惊喜。这篇文章旨在与大家分享这种魔力。</p><p>We’ll train RNNs to generate text character by character and ponder the question “how is that even possible?”<br>我们将训练循环神经网络（RNN）逐字符地生成文本，并思考这个问题：“这到底是怎么做到的？”</p><p>By the way, together with this post I am also releasing <a href="https://github.com/karpathy/char-rnn">code on Github</a> that allows you to train character-level language models based on multi-layer LSTMs. You give it a large chunk of text and it will learn to generate text like it one character at a time. You can also use it to reproduce my experiments below. But we’re getting ahead of ourselves; What are RNNs anyway?<br>顺便提一下，与这篇文章一起，我还在Github上发布了代码，这些代码可以用来训练基于多层LSTM的字符级语言模型。你只需提供一大段文本，它就会逐字符地学习生成类似的文本。你还可以使用它来重现我下面的实验。但在此之前，我们还是先回到正题上来：我们先来了解一下RNN到底是什么？</p><h1 id="recurrent-neural-networks-递归神经网络"><a href="#Recurrent-Neural-Networks-递归神经网络" class="headerlink" title="Recurrent Neural Networks  递归神经网络"></a>Recurrent Neural Networks  递归神经网络</h1><h2 id="sequences"><a href="#Sequences" class="headerlink" title="Sequences"></a>Sequences</h2><p><strong>Sequences</strong>. Depending on your background you might be wondering: _What makes Recurrent Networks so special_? A glaring limitation of Vanilla Neural Networks (and also Convolutional Networks) is that their API is too constrained: they accept a fixed-sized vector as input (e.g. an image) and produce a fixed-sized vector as output (e.g. probabilities of different classes). Not only that: These models perform this mapping using a fixed amount of computational steps (e.g. the number of layers in the model). The core reason that recurrent nets are more exciting is that they allow us to operate over _sequences_ of vectors: Sequences in the input, the output, or in the most general case both. A few examples may make this more concrete:<br>序列。根据你的背景，你可能会问：循环神经网络有什么特别之处？一个显而易见的限制是Vanilla 神经网络（以及卷积神经网络）的API过于受限：它们接受固定大小的向量作为输入（例如，一张图片），并产生固定大小的向量作为输出（例如，不同类别的概率）。不仅如此，这些模型使用固定数量的计算步骤来完成这个映射（例如，模型中的层数）。循环神经网络更令人兴奋的核心原因在于它们允许我们对向量序列进行操作：输入中的序列，输出中的序列，或者在最一般的情况下，两者都是序列。几个例子可以让这一点更加具体：</p><img src="/2472be8a/1.png" class><p>Each rectangle is a vector and arrows represent functions (e.g. matrix multiply). Input vectors are in red, output vectors are in blue and green vectors hold the RNN’s state (more on this soon). From left to right: <strong>(1)</strong> Vanilla mode of processing without RNN, from fixed-sized input to fixed-sized output (e.g. image classification). <strong>(2)</strong> Sequence output (e.g. image captioning takes an image and outputs a sentence of words). <strong>(3)</strong> Sequence input (e.g. sentiment analysis where a given sentence is classified as expressing positive or negative sentiment). <strong>(4)</strong> Sequence input and sequence output (e.g. Machine Translation: an RNN reads a sentence in English and then outputs a sentence in French). <strong>(5)</strong> Synced sequence input and output (e.g. video classification where we wish to label each frame of the video). Notice that in every case are no pre-specified constraints on the lengths sequences because the recurrent transformation (green) is fixed and can be applied as many times as we like.<br>每个矩形代表一个向量，箭头代表函数（例如矩阵乘法）。输入向量用红色表示，输出向量用蓝色表示，绿色向量表示RNN的状态（稍后会详细说明）。从左到右依次是：</p><p><strong>(1)</strong> 普通模式的处理，没有使用RNN，从固定大小的输入到固定大小的输出（例如图像分类）。</p><p><strong>(2)</strong> 序列输出（例如图像描述生成，输入一张图片，输出一个单词句子）。</p><p><strong>(3)</strong> 序列输入（例如情感分析，将给定的句子分类为表达正面或负面情感）。</p><p><strong>(4)</strong> 序列输入和序列输出（例如机器翻译：RNN读取一段英文句子，然后输出一段法文句子）。</p><p><strong>(5)</strong> 同步的序列输入和输出（例如视频分类，我们希望对视频的每一帧进行标签）。</p><p>注意，在每种情况下，序列长度都没有预先指定的限制，因为循环变换（绿色）是固定的，可以根据需要应用多次。</p><p>As you might expect, the sequence regime of operation is much more powerful compared to fixed networks that are doomed from the get-go by a fixed number of computational steps, and hence also much more appealing for those of us who aspire to build more intelligent systems. Moreover, as we’ll see in a bit, RNNs combine the input vector with their state vector with a fixed (but learned) function to produce a new state vector. This can in programming terms be interpreted as running a fixed program with certain inputs and some internal variables. Viewed this way, RNNs essentially describe programs. In fact, it is known that <a href="http://binds.cs.umass.edu/papers/1995_Siegelmann_Science.pdf">RNNs are Turing-Complete</a> in the sense that they can to simulate arbitrary programs (with proper weights). But similar to universal approximation theorems for neural nets you shouldn’t read too much into this. In fact, forget I said anything.<br>正如你所预料的那样，相较于受限于固定计算步骤的固定网络，序列操作模式要强大得多，因此对于那些希望构建更智能系统的人来说也更具吸引力。此外，正如我们稍后会看到的，RNN通过固定（但可学习）的函数将输入向量与其状态向量结合，生成一个新的状态向量。这在编程术语中可以理解为运行一个具有特定输入和一些内部变量的固定程序。从这个角度来看，RNN本质上是在描述程序。事实上，RNN被认为是图灵完备的，这意味着它们可以模拟任意程序（在适当的权重下）。但是，与神经网络的通用近似定理类似，你不应该对此过于解读。实际上，忘掉我刚才说的话吧。</p><blockquote><p>If training vanilla neural nets is optimization over functions, training recurrent nets is optimization over programs.<br>如果说训练普通神经网络是对函数的优化，那么训练循环网络就是对程序的优化。</p></blockquote><p><strong>Sequential processing in absence of sequences</strong>. You might be thinking that having sequences as inputs or outputs could be relatively rare, but an important point to realize is that even if your inputs/outputs are fixed vectors, it is still possible to use this powerful formalism to _process_ them in a sequential manner. For instance, the figure below shows results from two very nice papers from <a href="http://deepmind.com/">DeepMind</a>. On the left, an algorithm learns a recurrent network policy that steers its attention around an image; In particular, it learns to read out house numbers from left to right (<a href="http://arxiv.org/abs/1412.7755">Ba et al.</a>). On the right, a recurrent network _generates_ images of digits by learning to sequentially add color to a canvas (<a href="http://arxiv.org/abs/1502.04623">Gregor et al.</a>):<br>在没有序列的情况下进行顺序处理。您可能认为将序列作为输入或输出可能相对罕见，但需要意识到的重要一点是，即使你的输入/输出是固定向量，仍然可以使用这种强大的形式主义以顺序方式处理它们。例如，下图显示了 DeepMind 的两篇非常好的论文的结果。在左边，算法学习一个循环网络策略，将其注意力引导到图像周围;特别是，它学会了从左到右读出门牌号（Ba等人）。在右边，一个循环网络通过学习依次向画布添加颜色来生成数字图像：<br><img src="https://karpathy.github.io/assets/rnn/house_read.gif" alt> <img src="https://karpathy.github.io/assets/rnn/house_generate.gif" alt></p><p>The takeaway is that even if your data is not in form of sequences, you can still formulate and train powerful models that learn to process it sequentially. You’re learning stateful programs that process your fixed-sized data.<br>要点是，即使你的数据不是以序列形式存在，你仍然可以设计和训练强大的模型，使其学会以顺序方式处理这些数据。你正在学习的是处理固定大小数据的有状态程序。</p><h2 id="rnn-computation"><a href="#RNN-computation" class="headerlink" title="RNN computation"></a>RNN computation</h2><p><strong>RNN computation.</strong> So how do these things work? At the core, RNNs have a deceptively simple API: They accept an input vector <code>x</code> and give you an output vector <code>y</code>. However, crucially this output vector’s contents are influenced not only by the input you just fed in, but also on the entire history of inputs you’ve fed in in the past. Written as a class, the RNN’s API consists of a single <code>step</code> function:<br>RNN 计算。那么这些东西是如何工作的呢？在核心上，RNN 有一个看似简单的 API：它们接受一个输入向量 <code>x</code> 并给你一个输出向量 <code>y</code> 。然而，至关重要的是，这个输出向量的内容不仅受到你刚刚输入的输入的影响，还受到你过去输入的整个输入历史的影响。RNN 的 API 编写为一个类，由一个 <code>step</code> 函数组成：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rnn = RNN()</span><br><span class="line">y = rnn.step(x) # x is an input vector, y is the RNN&#x27;s output vector</span><br></pre></td></tr></table></figure><br>The RNN class has some internal state that it gets to update every time <code>step</code> is called. In the simplest case this state consists of a single _hidden_ vector <code>h</code>. Here is an implementation of the step function in a Vanilla RNN:<br>RNN 类具有一些内部状态，每次调用时 <code>step</code> 都会更新。在最简单的情况下，此状态由单个隐藏向量组成 <code>h</code> 。以下是 Vanilla RNN 中 step 函数的实现：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">class RNN:</span><br><span class="line">  # ...</span><br><span class="line">  def step(self, x):</span><br><span class="line">    # update the hidden state</span><br><span class="line">    self.h = np.tanh(np.dot(self.W_hh, self.h) + np.dot(self.W_xh, x))</span><br><span class="line">    # compute the output vector</span><br><span class="line">    y = np.dot(self.W_hy, self.h)</span><br><span class="line">    return y</span><br></pre></td></tr></table></figure><p>The above specifies the forward pass of a vanilla RNN. This RNN’s parameters are the three matrices <code>W_hh, W_xh, W_hy</code>. The hidden state <code>self.h</code> is initialized with the zero vector. The <code>np.tanh</code> function implements a non-linearity that squashes the activations to the range <code>[-1, 1]</code>. Notice briefly how this works: There are two terms inside of the tanh: one is based on the previous hidden state and one is based on the current input. In numpy <code>np.dot</code> is matrix multiplication. The two intermediates interact with addition, and then get squashed by the tanh into the new state vector. If you’re more comfortable with math notation, we can also write the hidden state update as $ℎ_𝑡=tanh⁡(𝑊_{ℎℎ}ℎ_{𝑡−1}+𝑊_{𝑥ℎ}𝑥_𝑡)$, where tanh is applied elementwise.<br>上述内容描述了一个基础RNN的前向传播过程。这个RNN的参数是三个矩阵W_hh、W_xh和W_hy。隐藏状态self.h初始化为零向量。np.tanh函数实现了一种非线性激活函数，将激活值压缩到[-1, 1]范围内。简要说明其工作原理：tanh内部有两个项，一个基于前一个时间步隐藏状态，另一个基于当前时间步输入。在numpy中，np.dot表示矩阵乘法。这两个中间结果通过加法相互作用，然后通过tanh函数压缩为新的状态向量。如果你对数学表示法更熟悉，我们也可以将隐藏状态的更新写成 $ℎ_𝑡=tanh⁡(𝑊_{ℎℎ}ℎ_{𝑡−1}+𝑊_{𝑥ℎ}𝑥_𝑡)$，其中tanh逐元素应用。</p><p>⚠️：numpy是Python中一个非常流行的数值计算库。np.tanh函数和np.dot函数都是numpy库中的函数。np.tanh函数用于计算元素级的双曲正切，而np.dot函数用于执行矩阵乘法。</p><p>We initialize the matrices of the RNN with random numbers and the bulk of work during training goes into finding the matrices that give rise to desirable behavior, as measured with some loss function that expresses your preference to what kinds of outputs $y$ you’d like to see in response to your input sequences $x$.<br>我们用随机数初始化RNN的矩阵，在训练过程中，大部分工作是找到能够产生理想行为的矩阵，这通过某种损失函数来衡量，该损失函数表达了你对输入序列$x$对应输出$y$的期望。</p><h2 id="going-deep"><a href="#Going-deep" class="headerlink" title="Going deep"></a>Going deep</h2><p><strong>Going deep</strong>. RNNs are neural networks and everything works monotonically better (if done right) if you put on your deep learning hat and start stacking models up like pancakes. For instance, we can form a 2-layer recurrent network as follows:<br>深入研究。RNN是神经网络的一种，如果方法得当，采用深度学习的方法并像叠煎饼一样将模型堆叠起来，一切都会单调地变得更好。例如，我们可以如下构建一个两层的循环神经网络：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y1 = rnn1.step(x)</span><br><span class="line">y = rnn2.step(y1)</span><br></pre></td></tr></table></figure></p><p>In other words we have two separate RNNs: One RNN is receiving the input vectors and the second RNN is receiving the output of the first RNN as its input. Except neither of these RNNs know or care - it’s all just vectors coming in and going out, and some gradients flowing through each module during backpropagation.<br>换句话说，我们有两个独立的 RNN：一个 RNN 接收输入向量，第二个 RNN 接收第一个 RNN 的输出作为其输入。除了这些 RNN 都不知道或不关心之外——它们都只是进出的向量，以及在反向传播过程中流过每个模块的一些梯度。</p><h2 id="getting-fancy"><a href="#Getting-fancy" class="headerlink" title="Getting fancy"></a>Getting fancy</h2><p><strong>Getting fancy</strong>. I’d like to briefly mention that in practice most of us use a slightly different formulation than what I presented above called a _Long Short-Term Memory_ (LSTM) network. The LSTM is a particular type of recurrent network that works slightly better in practice, owing to its more powerful update equation and some appealing backpropagation dynamics. I won’t go into details, but everything I’ve said about RNNs stays exactly the same, except the mathematical form for computing the update (the line <code>self.h = ...</code> ) gets a little more complicated. From here on I will use the terms “RNN/LSTM” interchangeably but all experiments in this post use an LSTM.<br>更复杂的模型。在实践中，我们大多数人使用的公式与我上面提到的稍有不同，被称为长短期记忆网络（LSTM）。LSTM是一种特定类型的循环神经网络，实际上效果更好，因为它具有更强大的更新方程和一些更具吸引力的反向传播动态。我不会深入讨论细节，但我所说的关于RNN的一切都完全相同，除了计算更新的数学形式（即self.h = …这一行）变得稍微复杂了一些。从现在开始，我会交替使用“RNN/LSTM”这两个术语，但本文中的所有实验都使用LSTM。</p><h1 id="character-level-language-models"><a href="#Character-Level-Language-Models" class="headerlink" title="Character-Level Language Models"></a>Character-Level Language Models</h1><p>字符级语言模型</p><p>Okay, so we have an idea about what RNNs are, why they are super exciting, and how they work. We’ll now ground this in a fun application: We’ll train RNN character-level language models. That is, we’ll give the RNN a huge chunk of text and ask it to model the probability distribution of the next character in the sequence given a sequence of previous characters. This will then allow us to generate new text one character at a time.<br>好的，所以我们已经对RNN是什么、为什么它们非常令人兴奋以及它们如何工作有了一定的了解。现在，我们将把这些知识应用到一个有趣的实际应用中：我们将训练RNN字符级别的语言模型。也就是说，我们会给RNN提供一大段文本，并让它根据前面字符的序列来建模下一个字符的概率分布。这样一来，我们就可以一次生成一个字符的新文本。</p><p>As a working example, suppose we only had a vocabulary of four possible letters “helo”, and wanted to train an RNN on the training sequence “hello”. This training sequence is in fact a source of 4 separate training examples: 1. The probability of “e” should be likely given the context of “h”, 2. “l” should be likely in the context of “he”, 3. “l” should also be likely given the context of “hel”, and finally 4. “o” should be likely given the context of “hell”.<br>作为一个实际例子，假设我们只有四个可能的字母“helo”的词汇表，并且想要在训练序列“hello”上训练一个RNN。这个训练序列实际上是四个独立的训练示例的来源：</p><ol><li>在“h”的上下文中，“e”的概率应该很大。</li><li>在“he”的上下文中，“l”的概率应该很大。</li><li>在“hel”的上下文中，“l”的概率也应该很大。</li><li>最后，在“hell”的上下文中，“o”的概率应该很大。</li></ol><p>Concretely, we will encode each character into a vector using 1-of-k encoding (i.e. all zero except for a single one at the index of the character in the vocabulary), and feed them into the RNN one at a time with the <code>step</code> function. We will then observe a sequence of 4-dimensional output vectors (one dimension per character), which we interpret as the confidence the RNN currently assigns to each character coming next in the sequence. Here’s a diagram:<br>具体来说，我们将使用1-of-k编码将每个字符编码成一个向量（即，除了在词汇表中字符索引处为1，其余全为0），然后用step函数将它们逐一输入RNN。随后，我们会得到一系列4维输出向量（每个字符一个维度），我们将这些输出向量解释为RNN当前对序列中下一个字符的置信度。以下是一个示意图：</p><img src="/2472be8a/2.png" class><p>An example RNN with 4-dimensional input and output layers, and a hidden layer of 3 units (neurons). This diagram shows the activations in the forward pass when the RNN is fed the characters “hell” as input. The output layer contains confidences the RNN assigns for the next character (vocabulary is “h,e,l,o”); We want the green numbers to be high and red numbers to be low.<br>具有 4 维输入和输出层的示例 RNN，以及 3 个单元（神经元）的隐藏层。此图显示了将字符“hell”作为输入馈送 RNN 时前向传递中的激活。输出层包含 RNN 为下一个字符分配的置信度（词汇为“h，e，l，o”）;我们希望绿色数字高，红色数字低。</p><p>⚠️： W_xh 指输入层和隐藏层之间的权重矩阵， W_hh 指隐藏层之间的权重矩阵， W_hy 指隐藏层和输出层之间的权重矩阵</p><p>For example, we see that in the first time step when the RNN saw the character “h” it assigned confidence of 1.0 to the next letter being “h”, 2.2 to letter “e”, -3.0 to “l”, and 4.1 to “o”. Since in our training data (the string “hello”) the next correct character is “e”, we would like to increase its confidence (green) and decrease the confidence of all other letters (red). Similarly, we have a desired target character at every one of the 4 time steps that we’d like the network to assign a greater confidence to. Since the RNN consists entirely of differentiable operations we can run the backpropagation algorithm (this is just a recursive application of the chain rule from calculus) to figure out in what direction we should adjust every one of its weights to increase the scores of the correct targets (green bold numbers). We can then perform a _parameter update_, which nudges every weight a tiny amount in this gradient direction. If we were to feed the same inputs to the RNN after the parameter update we would find that the scores of the correct characters (e.g. “e” in the first time step) would be slightly higher (e.g. 2.3 instead of 2.2), and the scores of incorrect characters would be slightly lower. We then repeat this process over and over many times until the network converges and its predictions are eventually consistent with the training data in that correct characters are always predicted next.<br>例如，我们看到在第一个时间步中，当RNN看到字符“h”时，它对下一个字符的置信度分配为：字符“h”是1.0，字符“e”是2.2，字符“l”是-3.0，字符“o”是4.1。由于在我们的训练数据（字符串“hello”）中，下一个正确字符是“e”，我们希望增加“e”的置信度（用绿色表示），并降低所有其他字符的置信度（用红色表示）。类似地，在每一个时间步上，我们都有一个期望的目标字符，希望网络能对其分配更高的置信度。由于RNN完全由可微操作组成，我们可以运行反向传播算法（这只是微积分中链式法则的递归应用）来确定应调整每个权重的方向，以提高正确目标的得分（绿色加粗数字）。然后，我们可以执行参数更新，将每个权重在该梯度方向上微调一个小量。如果在参数更新后再次将相同的输入提供给RNN，我们会发现正确字符的得分（例如，第一个时间步中的“e”）会略有提高（例如，从2.2提高到2.3），而错误字符的得分会略有降低。然后，我们反复进行这个过程多次，直到网络收敛，其预测最终与训练数据一致，即总是预测出正确的下一个字符。</p><p>⚠️：图中output 层 数字并不是置信度，而是logits, 这些logits并不直接表示概率/置信度,要将这些logits转化为概率（置信度），我们通常使用Softmax函数。</p><p>A more technical explanation is that we use the standard Softmax classifier (also commonly referred to as the cross-entropy loss) on every output vector simultaneously. The RNN is trained with mini-batch Stochastic Gradient Descent and I like to use <a href="http://arxiv.org/abs/1502.04390">RMSProp</a> or Adam (per-parameter adaptive learning rate methods) to stablilize the updates.<br>更技术性的解释是，我们在每个输出向量上 同时使用标准的Softmax分类器（也常被称为交叉熵损失）。RNN使用小批量随机梯度下降法进行训练，我喜欢使用RMSProp或Adam（每个参数的自适应学习率方法）来稳定更新。</p><p>Notice also that the first time the character “l” is input, the target is “l”, but the second time the target is “o”. The RNN therefore cannot rely on the input alone and must use its recurrent connection to keep track of the context to achieve this task.<br>另请注意，第一次输入字符“l”时，目标是“l”，但第二次目标是“o”。因此，RNN 不能单独依赖输入，必须使用其循环连接来跟踪上下文以实现此任务。</p><p>At <strong>test time</strong>, we feed a character into the RNN and get a distribution over what characters are likely to come next. We sample from this distribution, and feed it right back in to get the next letter. Repeat this process and you’re sampling text! Lets now train an RNN on different datasets and see what happens.<br>在测试时，我们将一个字符输入RNN，并获得下一个字符可能出现的概率分布。我们从这个分布中采样，并将采样得到的字符再次输入RNN以获取下一个字符。重复这个过程，就可以生成文本了！现在，让我们在不同的数据集上训练一个RNN，看看会发生什么。</p><p>To further clarify, for educational purposes I also wrote a <a href="https://gist.github.com/karpathy/d4dee566867f8291f086">minimal character-level RNN language model in Python/numpy</a>. It is only about 100 lines long and hopefully it gives a concise, concrete and useful summary of the above if you’re better at reading code than text. We’ll now dive into example results, produced with the much more efficient Lua/Torch codebase.<br>为了进一步说明，我还用Python和numpy编写了一个最小的字符级RNN语言模型。它只有大约100行代码，希望能为你提供一个简洁、具体且有用的总结，如果你更擅长阅读代码而不是文字。现在，我们将深入探讨使用更加高效的Lua/Torch代码库生成的示例结果。</p><h1 id="fun-with-rnns"><a href="#Fun-with-RNNs" class="headerlink" title="Fun with RNNs"></a>Fun with RNNs</h1><p>All 5 example character models below were trained with the <a href="https://github.com/karpathy/char-rnn">code</a> I’m releasing on Github. The input in each case is a single file with some text, and we’re training an RNN to predict the next character in the sequence.<br>下面的所有 5 个示例字符模型都是使用我在 Github 上发布的代码训练的。每种情况下的输入都是一个包含一些文本的单个文件，我们正在训练一个 RNN 来预测序列中的下一个字符。</p><h3 id="paul-graham-generator-保罗格雷厄姆发电机"><a href="#Paul-Graham-generator-保罗·格雷厄姆发电机" class="headerlink" title="Paul Graham generator 保罗·格雷厄姆发电机"></a>Paul Graham generator 保罗·格雷厄姆发电机</h3><p>Lets first try a small dataset of English as a sanity check. My favorite fun dataset is the concatenation of <a href="http://www.paulgraham.com/articles.html">Paul Graham’s essays</a>. The basic idea is that there’s a lot of wisdom in these essays, but unfortunately Paul Graham is a relatively slow generator. Wouldn’t it be great if we could sample startup wisdom on demand? That’s where an RNN comes in.<br>让我们首先尝试一个小型的英文数据集来进行基本检查。我最喜欢的有趣数据集是保罗·格雷厄姆（Paul Graham）的文章合集。基本想法是，这些文章中有很多智慧，但遗憾的是，保罗·格雷厄姆的写作速度相对较慢。如果我们能按需采样创业智慧，那不是很棒吗？这正是RNN的用武之地。</p><p>Concatenating all pg essays over the last ~5 years we get approximately 1MB text file, or about 1 million characters (this is considered a very small dataset by the way). _Technical:_ Lets train a 2-layer LSTM with 512 hidden nodes (approx. 3.5 million parameters), and with dropout of 0.5 after each layer. We’ll train with batches of 100 examples and truncated backpropagation through time of length 100 characters. With these settings one batch on a TITAN Z GPU takes about 0.46 seconds (this can be cut in half with 50 character BPTT at negligible cost in performance). Without further ado, lets see a sample from the RNN:<br>将过去大约5年间的所有保罗·格雷厄姆的文章合并起来，我们得到了一个大约1MB的文本文件，约100万个字符（顺便说一下，这被认为是一个非常小的数据集）。技术细节：让我们训练一个具有2层、每层512个隐藏节点的LSTM（大约350万个参数），并在每层之后使用0.5的dropout。我们将使用100个样本的批次和长度为100字符的截断时间反向传播进行训练。在这些设置下，在TITAN Z GPU上处理一个批次大约需要0.46秒（使用50字符的截断时间反向传播，几乎不会影响性能，可以将时间减半）。事不宜迟，让我们看看RNN生成的一个样本：</p><p>_“The surprised in investors weren’t going to raise money. I’m not the company with the time there are all interesting quickly, don’t have to get off the same programmers. There’s a super-angel round fundraising, why do you can do. If you have a different physical investment are become in people who reduced in a startup with the way to argument the acquirer could see them just that you’re also the founders will part of users’ affords that and an alternation to the idea. [2] Don’t work at first member to see the way kids will seem in advance of a bad successful startup. And if you have to act the big company too.”_<br>_“投资者的惊讶是，他们并不打算筹集资金。我不是那个有时间的公司，有趣的事情很快就会出现，不需要让相同的程序员离开。有一个超级天使轮的融资，你为什么要这样做。如果你有不同的实体投资，会成为那些在初创公司里减少的人中争论的方式，收购者可能会看到他们只是创始人将成为用户努力的一部分，这是对想法的一种替代。[2] 一开始不要在成员身上工作，看孩子们将如何提前在一个失败的成功初创公司中表现出来。而且，如果你必须行动，那么大公司也一样。”_</p><p>Okay, clearly the above is unfortunately not going to replace Paul Graham anytime soon, but remember that the RNN had to learn English completely from scratch and with a small dataset (including where you put commas, apostrophes and spaces). I also like that it learns to support its own arguments (e.g. [2], above). Sometimes it says something that offers a glimmer of insight, such as _“a company is a meeting to think to investors”_. <a href="http://cs.stanford.edu/people/karpathy/char-rnn/pg.txt">Here’s</a> a link to 50K character sample if you’d like to see more.<br>好的，很明显，以上内容暂时还无法替代保罗·格雷厄姆，但请记住，RNN必须从零开始学习英语，而且是用一个小数据集（包括逗号、撇号和空格的位置）。我也喜欢它学会了支持自己的论点（例如，上文中的[2]）。有时，它会说出一些略带启发性的话，比如“a company is a meeting to think to investors”（公司是与投资者思考的会议）。如果你想查看更多，这里有一个50K字符的样本链接。</p><p><strong>Temperature.</strong> We can also play with the temperature of the Softmax during sampling. Decreasing the temperature from 1 to some lower number (e.g. 0.5) makes the RNN more confident, but also more conservative in its samples. Conversely, higher temperatures will give more diversity but at cost of more mistakes (e.g. spelling mistakes, etc). In particular, setting temperature very near zero will give the most likely thing that Paul Graham might say:<br>温度。我们也可以在采样过程中调整Softmax的温度。将温度从1降低到某个较低的数值（例如0.5），会使RNN更有信心，但也更保守于其采样结果。相反，较高的温度会带来更多的多样性，但代价是会有更多的错误（例如拼写错误等）。特别是，将温度设置得非常接近零，会产生最有可能是保罗·格雷厄姆会说的话：</p><p>_“is that they were all the same thing that was a startup is that they were all the same thing that was a startup is that they were all the same thing that was a startup is that they were all the same”<br>“他们都是一样的，创业公司是，他们都是创业公司，他们都是一样的创业公司，他们都是一样的，创业公司是一样的”_</p><p>looks like we’ve reached an infinite loop about startups.<br>看起来我们已经达到了一个关于初创公司的无限循环。</p><h3 id="shakespeare-莎士比亚"><a href="#Shakespeare-莎士比亚" class="headerlink" title="Shakespeare 莎士比亚"></a>Shakespeare 莎士比亚</h3><p>It looks like we can learn to spell English words. But how about if there is more structure and style in the data? To examine this I downloaded all the works of Shakespeare and concatenated them into a single (4.4MB) file. We can now afford to train a larger network, in this case lets try a 3-layer RNN with 512 hidden nodes on each layer. After we train the network for a few hours we obtain samples such as:<br>看起来我们可以学会拼写英语单词。但如果数据中有更多的结构和风格呢？为此，我下载了莎士比亚的所有作品，并将它们合并成一个单一的（4.4MB）文件。我们现在有能力训练一个更大的网络，在这种情况下，让我们尝试一个3层的RNN，每层有512个隐藏节点。经过几个小时的训练，我们得到如下样本：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">PANDARUS:</span><br><span class="line">Alas, I think he shall be come approached and the day</span><br><span class="line">When little srain would be attain&#x27;d into being never fed,</span><br><span class="line">And who is but a chain and subjects of his death,</span><br><span class="line">I should not sleep.</span><br><span class="line"></span><br><span class="line">Second Senator:</span><br><span class="line">They are away this miseries, produced upon my soul,</span><br><span class="line">Breaking and strongly should be buried, when I perish</span><br><span class="line">The earth and thoughts of many states.</span><br><span class="line"></span><br><span class="line">DUKE VINCENTIO:</span><br><span class="line">Well, your wit is in the care of side and that.</span><br><span class="line"></span><br><span class="line">Second Lord:</span><br><span class="line">They would be ruled after this chamber, and</span><br><span class="line">my fair nues begun out of the fact, to be conveyed,</span><br><span class="line">Whose noble souls I&#x27;ll have the heart of the wars.</span><br><span class="line"></span><br><span class="line">Clown:</span><br><span class="line">Come, sir, I will make did behold your worship.</span><br><span class="line"></span><br><span class="line">VIOLA:</span><br><span class="line">I&#x27;ll drink it.</span><br></pre></td></tr></table></figure></p><p>Remember, all the RNN knows are characters, so in particular it samples both speaker’s names and the contents. Sometimes we also get relatively extented monologue passages, such as:<br>请记住，RNN所知道的只有字符，因此它既采样说话者的名字，也采样内容。有时我们还会得到相对较长的独白段落，例如：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">VIOLA:</span><br><span class="line">Why, Salisbury must find his flesh and thought</span><br><span class="line">That which I am not aps, not a man and in fire,</span><br><span class="line">To show the reining of the raven and the wars</span><br><span class="line">To grace my hand reproach within, and not a fair are hand,</span><br><span class="line">That Caesar and my goodly father&#x27;s world;</span><br><span class="line">When I was heaven of presence and our fleets,</span><br><span class="line">We spare with hours, but cut thy council I am great,</span><br><span class="line">Murdered and by thy master&#x27;s ready there</span><br><span class="line">My power to give thee but so much as hell:</span><br><span class="line">Some service in the noble bondman here,</span><br><span class="line">Would show him to her wine.</span><br><span class="line"></span><br><span class="line">KING LEAR:</span><br><span class="line">O, if you were a feeble sight, the courtesy of your law,</span><br><span class="line">Your sight and several breath, will wear the gods</span><br><span class="line">With his heads, and my hands are wonder&#x27;d at the deeds,</span><br><span class="line">So drop upon your lordship&#x27;s head, and your opinion</span><br><span class="line">Shall be against your honour.</span><br></pre></td></tr></table></figure></p><p>I can barely recognize these samples from actual Shakespeare :) If you like Shakespeare, you might appreciate this 100,000 character sample. Of course, you can also generate an infinite amount of your own samples at different temperatures with the provided code.<br>我几乎无法辨认这些样本是否真的是莎士比亚的作品 :) 如果你喜欢莎士比亚，你可能会喜欢这个10万字符的样本。当然，你也可以使用提供的代码在不同温度下生成无限量的样本。</p><h3 id="wikipedia-维基百科"><a href="#Wikipedia-维基百科" class="headerlink" title="Wikipedia 维基百科"></a>Wikipedia 维基百科</h3><p>We saw that the LSTM can learn to spell words and copy general syntactic structures. Lets further increase the difficulty and train on structured markdown. In particular, lets take the <a href="http://prize.hutter1.net/">Hutter Prize</a> 100MB dataset of raw Wikipedia and train an LSTM. Following <a href="http://arxiv.org/abs/1308.0850">Graves et al.</a>, I used the first 96MB for training, the rest for validation and ran a few models overnight. We can now sample Wikipedia articles! Below are a few fun excerpts. First, some basic markdown output:<br>我们已经看到LSTM可以学会拼写单词和复制一般的句法结构。让我们进一步增加难度，训练在结构化的Markdown上。特别是，让我们使用Hutter Prize的100MB原始维基百科数据集来训练一个LSTM。按照Graves等人的方法，我使用前96MB进行训练，剩余的用于验证，并在一夜之间运行了几个模型。现在我们可以采样生成维基百科文章了！以下是一些有趣的摘录。首先，是一些基本的Markdown输出：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">Naturalism and decision for the majority of Arab countries&#x27; capitalide was grounded</span><br><span class="line">by the Irish language by [[John Clair]], [[An Imperial Japanese Revolt]], associated </span><br><span class="line">with Guangzham&#x27;s sovereignty. His generals were the powerful ruler of the Portugal </span><br><span class="line">in the [[Protestant Immineners]], which could be said to be directly in Cantonese </span><br><span class="line">Communication, which followed a ceremony and set inspired prison, training. The </span><br><span class="line">emperor travelled back to [[Antioch, Perth, October 25|21]] to note, the Kingdom </span><br><span class="line">of Costa Rica, unsuccessful fashioned the [[Thrales]], [[Cynth&#x27;s Dajoard]], known </span><br><span class="line">in western [[Scotland]], near Italy to the conquest of India with the conflict. </span><br><span class="line">Copyright was the succession of independence in the slop of Syrian influence that </span><br><span class="line">was a famous German movement based on a more popular servicious, non-doctrinal </span><br><span class="line">and sexual power post. Many governments recognize the military housing of the </span><br><span class="line">[[Civil Liberalization and Infantry Resolution 265 National Party in Hungary]], </span><br><span class="line">that is sympathetic to be to the [[Punjab Resolution]]</span><br><span class="line">(PJS)[http://www.humah.yahoo.com/guardian.</span><br><span class="line">cfm/7754800786d17551963s89.htm Official economics Adjoint for the Nazism, Montgomery </span><br><span class="line">was swear to advance to the resources for those Socialism&#x27;s rule, </span><br><span class="line">was starting to signing a major tripad of aid exile.]]</span><br></pre></td></tr></table></figure></p><p>In case you were wondering, the yahoo url above doesn’t actually exist, the model just hallucinated it. Also, note that the model learns to open and close the parenthesis correctly. There’s also quite a lot of structured markdown that the model learns, for example sometimes it creates headings, lists, etc.:<br>如果你在好奇，上述的Yahoo网址实际上并不存在，这是模型臆想出来的。此外，请注意，模型学会了正确地打开和关闭括号。模型还学习了大量的结构化Markdown，例如，有时它会创建标题、列表等内容：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">&#123; &#123; cite journal | id=Cerling Nonforest Department|format=Newlymeslated|none &#125; &#125;</span><br><span class="line">&#x27;&#x27;www.e-complete&#x27;&#x27;.</span><br><span class="line"></span><br><span class="line">&#x27;&#x27;&#x27;See also&#x27;&#x27;&#x27;: [[List of ethical consent processing]]</span><br><span class="line"></span><br><span class="line">== See also ==</span><br><span class="line">*[[Iender dome of the ED]]</span><br><span class="line">*[[Anti-autism]]</span><br><span class="line"></span><br><span class="line">===[[Religion|Religion]]===</span><br><span class="line">*[[French Writings]]</span><br><span class="line">*[[Maria]]</span><br><span class="line">*[[Revelation]]</span><br><span class="line">*[[Mount Agamul]]</span><br><span class="line"></span><br><span class="line">== External links==</span><br><span class="line">* [http://www.biblegateway.nih.gov/entrepre/ Website of the World Festival. The labour of India-county defeats at the Ripper of California Road.]</span><br><span class="line"></span><br><span class="line">==External links==</span><br><span class="line">* [http://www.romanology.com/ Constitution of the Netherlands and Hispanic Competition for Bilabial and Commonwealth Industry (Republican Constitution of the Extent of the Netherlands)]</span><br></pre></td></tr></table></figure><br>Sometimes the model snaps into a mode of generating random but valid XML:<br>有时，模型会进入生成随机但有效的 XML 的模式：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">&lt;page&gt;</span><br><span class="line">  &lt;title&gt;Antichrist&lt;/title&gt;</span><br><span class="line">  &lt;id&gt;865&lt;/id&gt;</span><br><span class="line">  &lt;revision&gt;</span><br><span class="line">    &lt;id&gt;15900676&lt;/id&gt;</span><br><span class="line">    &lt;timestamp&gt;2002-08-03T18:14:12Z&lt;/timestamp&gt;</span><br><span class="line">    &lt;contributor&gt;</span><br><span class="line">      &lt;username&gt;Paris&lt;/username&gt;</span><br><span class="line">      &lt;id&gt;23&lt;/id&gt;</span><br><span class="line">    &lt;/contributor&gt;</span><br><span class="line">    &lt;minor /&gt;</span><br><span class="line">    &lt;comment&gt;Automated conversion&lt;/comment&gt;</span><br><span class="line">    &lt;text xml:space=&quot;preserve&quot;&gt;#REDIRECT [[Christianity]]&lt;/text&gt;</span><br><span class="line">  &lt;/revision&gt;</span><br><span class="line">&lt;/page&gt;</span><br></pre></td></tr></table></figure></p><p>The model completely makes up the timestamp, id, and so on. Also, note that it closes the correct tags appropriately and in the correct nested order. Here are <a href="http://cs.stanford.edu/people/karpathy/char-rnn/wiki.txt">100,000 characters of sampled wikipedia</a> if you’re interested to see more.<br>该模型完全由时间戳、id 等组成。另外，请注意，它正确地关闭了标签，并且按照正确的嵌套顺序。这里有 100,000 个字符的样本维基百科，如果您有兴趣查看更多。</p><h3 id="algebraic-geometry-latex-代数几何latex"><a href="#Algebraic-Geometry-Latex-代数几何（Latex）" class="headerlink" title="Algebraic Geometry (Latex)  代数几何（Latex）"></a>Algebraic Geometry (Latex)  代数几何（Latex）</h3><p>The results above suggest that the model is actually quite good at learning complex syntactic structures. Impressed by these results, my labmate (<a href="http://cs.stanford.edu/people/jcjohns/">Justin Johnson</a>) and I decided to push even further into structured territories and got a hold of <a href="http://stacks.math.columbia.edu/">this book</a> on algebraic stacks/geometry. We downloaded the raw Latex source file (a 16MB file) and trained a multilayer LSTM. Amazingly, the resulting sampled Latex _almost_ compiles. We had to step in and fix a few issues manually but then you get plausible looking math, it’s quite astonishing:<br>上述结果表明，模型在学习复杂句法结构方面实际上相当不错。这些结果给我留下了深刻的印象，我的实验室同事Justin Johnson和我决定进一步探索结构化领域，并找到了一本关于代数叠/几何的书。我们下载了原始的Latex源文件（一个16MB的文件）并训练了一个多层LSTM。令人惊讶的是，生成的Latex几乎可以编译。我们不得不手动修复一些问题，但最终得到了看起来很合理的数学表达，这真是令人惊叹：<br><img src="/2472be8a/3.png" class><br>Sampled (fake) algebraic geometry. <a href="http://cs.stanford.edu/people/jcjohns/fake-math/4.pdf">Here’s the actual pdf.</a><br>采样（假）代数几何。这是实际的pdf。</p><p>Here’s another sample: 下面是另一个示例：<br><img src="/2472be8a/4.png" class><br>More hallucinated algebraic geometry. Nice try on the diagram (right).<br>更多幻觉的代数几何。不错的尝试图（右）。</p><p>As you can see above, sometimes the model tries to generate latex diagrams, but clearly it hasn’t really figured them out. I also like the part where it chooses to skip a proof (_“Proof omitted.”_, top left). Of course, keep in mind that latex has a relatively difficult structured syntactic format that I haven’t even fully mastered myself. For instance, here is a raw sample from the model (unedited):<br>正如你在上面看到的，有时模型尝试生成Latex图表，但显然它还没有真正掌握这种技能。我也喜欢它选择跳过证明的部分（“Proof omitted.”，左上角）。当然，请记住，Latex有一个相对复杂的结构化句法格式，我自己都还没有完全掌握。例如，下面是模型生成的一个原始样本（未编辑）：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">\begin&#123;proof&#125;</span><br><span class="line">We may assume that $\mathcal&#123;I&#125;$ is an abelian sheaf on $\mathcal&#123;C&#125;$.</span><br><span class="line">\item Given a morphism $\Delta : \mathcal&#123;F&#125; \to \mathcal&#123;I&#125;$</span><br><span class="line">is an injective and let $\mathfrak q$ be an abelian sheaf on $X$.</span><br><span class="line">Let $\mathcal&#123;F&#125;$ be a fibered complex. Let $\mathcal&#123;F&#125;$ be a category.</span><br><span class="line">\begin&#123;enumerate&#125;</span><br><span class="line">\item \hyperref[setain-construction-phantom]&#123;Lemma&#125;</span><br><span class="line">\label&#123;lemma-characterize-quasi-finite&#125;</span><br><span class="line">Let $\mathcal&#123;F&#125;$ be an abelian quasi-coherent sheaf on $\mathcal&#123;C&#125;$.</span><br><span class="line">Let $\mathcal&#123;F&#125;$ be a coherent $\mathcal&#123;O&#125;_X$-module. Then</span><br><span class="line">$\mathcal&#123;F&#125;$ is an abelian catenary over $\mathcal&#123;C&#125;$.</span><br><span class="line">\item The following are equivalent</span><br><span class="line">\begin&#123;enumerate&#125;</span><br><span class="line">\item $\mathcal&#123;F&#125;$ is an $\mathcal&#123;O&#125;_X$-module.</span><br><span class="line">\end&#123;lemma&#125;</span><br></pre></td></tr></table></figure></p><p>This sample from a relatively decent model illustrates a few common mistakes. For example, the model opens a <code>\begin&#123;proof&#125;</code> environment but then ends it with a <code>\end&#123;lemma&#125;</code>. This is an example of a problem we’d have to fix manually, and is likely due to the fact that the dependency is too long-term: By the time the model is done with the proof it has forgotten whether it was doing a proof or a lemma. Similarly, it opens an <code>\begin&#123;enumerate&#125;</code> but then forgets to close it. We observed that these became less common with larger/better models, but nonetheless, these are the kinds of mistakes that come up.<br>这个来自相对不错模型的样本展示了一些常见的错误。例如，模型打开了一个 \begin{proof} 环境，但随后却以 \end{lemma} 结束。这是一个我们必须手动修复的问题的例子，可能是由于依赖关系过于长期：到模型完成证明时，它已经忘记了自己是在进行证明还是引理。类似地，它打开了一个 \begin{enumerate}，但随后忘记了关闭它。我们注意到，这些错误在较大/更好的模型中变得不那么常见，但这些都是可能出现的错误类型。</p><h3 id="linux-source-code-linux-源代码"><a href="#Linux-Source-Code-Linux-源代码" class="headerlink" title="Linux Source Code Linux 源代码"></a>Linux Source Code Linux 源代码</h3><p>I wanted to push structured data to its limit, so for the final challenge I decided to use code. In particular, I took all the source and header files found in the <a href="https://github.com/torvalds/linux">Linux repo on Github</a>, concatenated all of them in a single giant file (474MB of C code) (I was originally going to train only on the kernel but that by itself is only ~16MB). Then I trained several as-large-as-fits-on-my-GPU 3-layer LSTMs over a period of a few days. These models have about 10 million parameters, which is still on the lower end for RNN models. The results are superfun:<br>我想将结构化数据推向极限，因此在最后的挑战中我决定使用代码。具体来说，我收集了Github上Linux仓库中的所有源文件和头文件，将它们合并成一个巨大的文件（474MB的C代码）（我本来打算只训练内核代码，但它本身只有大约16MB）。然后，我在几天内训练了多个尽可能适应GPU容量的三层LSTM模型。这些模型大约有1000万个参数，这在RNN模型中仍然算是较少的。结果非常有趣：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">/*</span><br><span class="line"> * Increment the size file of the new incorrect UI_FILTER group information</span><br><span class="line"> * of the size generatively.</span><br><span class="line"> */</span><br><span class="line">static int indicate_policy(void)</span><br><span class="line">&#123;</span><br><span class="line">  int error;</span><br><span class="line">  if (fd == MARN_EPT) &#123;</span><br><span class="line">    /*</span><br><span class="line">     * The kernel blank will coeld it to userspace.</span><br><span class="line">     */</span><br><span class="line">    if (ss-&gt;segment &lt; mem_total)</span><br><span class="line">      unblock_graph_and_set_blocked();</span><br><span class="line">    else</span><br><span class="line">      ret = 1;</span><br><span class="line">    goto bail;</span><br><span class="line">  &#125;</span><br><span class="line">  segaddr = in_SB(in.addr);</span><br><span class="line">  selector = seg / 16;</span><br><span class="line">  setup_works = true;</span><br><span class="line">  for (i = 0; i &lt; blocks; i++) &#123;</span><br><span class="line">    seq = buf[i++];</span><br><span class="line">    bpf = bd-&gt;bd.next + i * search;</span><br><span class="line">    if (fd) &#123;</span><br><span class="line">      current = blocked;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  rw-&gt;name = &quot;Getjbbregs&quot;;</span><br><span class="line">  bprm_self_clearl(&amp;iv-&gt;version);</span><br><span class="line">  regs-&gt;new = blocks[(BPF_STATS &lt;&lt; info-&gt;historidac)] | PFMR_CLOBATHINC_SECONDS &lt;&lt; 12;</span><br><span class="line">  return segtable;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>The code looks really quite great overall. Of course, I don’t think it compiles but when you scroll through the generate code it feels very much like a giant C code base. Notice that the RNN peppers its code with comments here and there at random. It is also very good at making very few syntactic errors. For example, it uses strings properly, pointer notation, etc. It also opens and closes brackets {   correctly and learns to indent its code very well. A common error is that it can’t keep track of variable names: It often uses undefined variables (e.g. rw above), declares variables it never uses (e.g. int error), or returns non-existing variables. Lets see a few more examples. Here’s another snippet that shows a wider array of operations that the RNN learns:</p><p>总体来说，这些代码看起来非常不错。当然，我不认为它能编译，但当你滚动查看生成的代码时，它确实给人一种巨大的C代码库的感觉。注意，RNN会随机在代码中插入注释，并且在语法错误方面表现得非常出色。例如，它能正确使用字符串、指针符号等，还能正确地打开和关闭大括号{}，并很好地学习代码缩进。<br>一个常见的错误是它无法跟踪变量名：它经常使用未定义的变量（例如上面的rw），声明从未使用的变量（例如int error），或者返回不存在的变量。让我们再看一些例子。下面是另一个代码片段，展示了RNN学到的更广泛的操作：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">/*</span><br><span class="line"> * If this error is set, we will need anything right after that BSD.</span><br><span class="line"> */</span><br><span class="line">static void action_new_function(struct s_stat_info *wb)</span><br><span class="line">&#123;</span><br><span class="line">  unsigned long flags;</span><br><span class="line">  int lel_idx_bit = e-&gt;edd, *sys &amp; ~((unsigned long) *FIRST_COMPAT);</span><br><span class="line">  buf[0] = 0xFFFFFFFF &amp; (bit &lt;&lt; 4);</span><br><span class="line">  min(inc, slist-&gt;bytes);</span><br><span class="line">  printk(KERN_WARNING &quot;Memory allocated %02x/%02x, &quot;</span><br><span class="line">    &quot;original MLL instead\n&quot;),</span><br><span class="line">    min(min(multi_run - s-&gt;len, max) * num_data_in),</span><br><span class="line">    frame_pos, sz + first_seg);</span><br><span class="line">  div_u64_w(val, inb_p);</span><br><span class="line">  spin_unlock(&amp;disk-&gt;queue_lock);</span><br><span class="line">  mutex_unlock(&amp;s-&gt;sock-&gt;mutex);</span><br><span class="line">  mutex_unlock(&amp;func-&gt;mutex);</span><br><span class="line">  return disassemble(info-&gt;pending_bh);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">static void num_serial_settings(struct tty_struct *tty)</span><br><span class="line">&#123;</span><br><span class="line">  if (tty == tty)</span><br><span class="line">    disable_single_st_p(dev);</span><br><span class="line">  pci_disable_spool(port);</span><br><span class="line">  return 0;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">static void do_command(struct seq_file *m, void *v)</span><br><span class="line">&#123;</span><br><span class="line">  int column = 32 &lt;&lt; (cmd[2] &amp; 0x80);</span><br><span class="line">  if (state)</span><br><span class="line">    cmd = (int)(int_state ^ (in_8(&amp;ch-&gt;ch_flags) &amp; Cmd) ? 2 : 1);</span><br><span class="line">  else</span><br><span class="line">    seq = 1;</span><br><span class="line">  for (i = 0; i &lt; 16; i++) &#123;</span><br><span class="line">    if (k &amp; (1 &lt;&lt; 1))</span><br><span class="line">      pipe = (in_use &amp; UMXTHREAD_UNCCA) +</span><br><span class="line">        ((count &amp; 0x00000000fffffff8) &amp; 0x000000f) &lt;&lt; 8;</span><br><span class="line">    if (count == 0)</span><br><span class="line">      sub(pid, ppc_md.kexec_handle, 0x20000000);</span><br><span class="line">    pipe_set_bytes(i, 0);</span><br><span class="line">  &#125;</span><br><span class="line">  /* Free our user pages pointer to place camera if all dash */</span><br><span class="line">  subsystem_info = &amp;of_changes[PAGE_SIZE];</span><br><span class="line">  rek_controls(offset, idx, &amp;soffset);</span><br><span class="line">  /* Now we want to deliberately put it to device */</span><br><span class="line">  control_check_polarity(&amp;context, val, 0);</span><br><span class="line">  for (i = 0; i &lt; COUNTER; i++)</span><br><span class="line">    seq_puts(s, &quot;policy &quot;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Notice that in the second function the model compares tty == tty, which is vacuously true. On the other hand, at least the variable tty exists in the scope this time! In the last function, notice that the code does not return anything, which happens to be correct since the function signature is void. However, the first two functions were also declared void and did return values. This is again a form of a common mistake due to long-term interactions.<br>注意在第二个函数中，模型比较了tty == tty，这是显然为真的。另一方面，至少这次变量tty存在于作用域中！在最后一个函数中，注意代码没有返回任何东西，这正好是正确的，因为函数签名是void。然而，前两个函数也声明为void，但却返回了值。这再次是一种由于长时间交互而导致的常见错误形式。</p><p>Sometimes the model decides that it’s time to sample a new file. This is usually a very amusing part: The model first recites the GNU license character by character, samples a few includes, generates some macros and then dives into the code:<br>有时，模型会决定是时候采样一个新文件了。这通常是非常有趣的部分：模型首先逐字符地背诵GNU许可证，采样几个include语句，生成一些宏，然后深入到代码中：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">/*</span><br><span class="line"> *  Copyright (c) 2006-2010, Intel Mobile Communications.  All rights reserved.</span><br><span class="line"> *</span><br><span class="line"> *   This program is free software; you can redistribute it and/or modify it</span><br><span class="line"> * under the terms of the GNU General Public License version 2 as published by</span><br><span class="line"> * the Free Software Foundation.</span><br><span class="line"> *</span><br><span class="line"> *        This program is distributed in the hope that it will be useful,</span><br><span class="line"> * but WITHOUT ANY WARRANTY; without even the implied warranty of</span><br><span class="line"> *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span><br><span class="line"> *</span><br><span class="line"> *  GNU General Public License for more details.</span><br><span class="line"> *</span><br><span class="line"> *   You should have received a copy of the GNU General Public License</span><br><span class="line"> *    along with this program; if not, write to the Free Software Foundation,</span><br><span class="line"> *  Inc., 675 Mass Ave, Cambridge, MA 02139, USA.</span><br><span class="line"> */</span><br><span class="line"></span><br><span class="line">#include &lt;linux/kexec.h&gt;</span><br><span class="line">#include &lt;linux/errno.h&gt;</span><br><span class="line">#include &lt;linux/io.h&gt;</span><br><span class="line">#include &lt;linux/platform_device.h&gt;</span><br><span class="line">#include &lt;linux/multi.h&gt;</span><br><span class="line">#include &lt;linux/ckevent.h&gt;</span><br><span class="line"></span><br><span class="line">#include &lt;asm/io.h&gt;</span><br><span class="line">#include &lt;asm/prom.h&gt;</span><br><span class="line">#include &lt;asm/e820.h&gt;</span><br><span class="line">#include &lt;asm/system_info.h&gt;</span><br><span class="line">#include &lt;asm/setew.h&gt;</span><br><span class="line">#include &lt;asm/pgproto.h&gt;</span><br><span class="line"></span><br><span class="line">#define REG_PG    vesa_slot_addr_pack</span><br><span class="line">#define PFM_NOCOMP  AFSR(0, load)</span><br><span class="line">#define STACK_DDR(type)     (func)</span><br><span class="line"></span><br><span class="line">#define SWAP_ALLOCATE(nr)     (e)</span><br><span class="line">#define emulate_sigs()  arch_get_unaligned_child()</span><br><span class="line">#define access_rw(TST)  asm volatile(&quot;movd %%esp, %0, %3&quot; : : &quot;r&quot; (0));   \</span><br><span class="line">  if (__type &amp; DO_READ)</span><br><span class="line"></span><br><span class="line">static void stat_PC_SEC __read_mostly offsetof(struct seq_argsqueue, \</span><br><span class="line">          pC&gt;[1]);</span><br><span class="line"></span><br><span class="line">static void</span><br><span class="line">os_prefix(unsigned long sys)</span><br><span class="line">&#123;</span><br><span class="line">#ifdef CONFIG_PREEMPT</span><br><span class="line">  PUT_PARAM_RAID(2, sel) = get_state_state();</span><br><span class="line">  set_pid_sum((unsigned long)state, current_state_str(),</span><br><span class="line">           (unsigned long)-1-&gt;lr_full; low;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>There are too many fun parts to cover- I could probably write an entire blog post on just this part. I’ll cut it short for now, but here is <a href="http://cs.stanford.edu/people/karpathy/char-rnn/linux.txt">1MB of sampled Linux code</a> for your viewing pleasure.<br>有太多有趣的部分要涵盖 - 我可能会写一整篇关于这部分的博客文章。我现在会缩短它，但这里有 1MB 的 Linux 代码样本供您查看。</p><h3 id="generating-baby-names-生成婴儿名字"><a href="#Generating-Baby-Names-生成婴儿名字" class="headerlink" title="Generating Baby Names 生成婴儿名字"></a>Generating Baby Names 生成婴儿名字</h3><p>Lets try one more for fun. Lets feed the RNN a large text file that contains 8000 baby names listed out, one per line (names obtained from <a href="http://www.cs.cmu.edu/afs/cs/project/ai-repository/ai/areas/nlp/corpora/names/">here</a>). We can feed this to the RNN and then generate new names! Here are some example names, only showing the ones that do not occur in the training data (90% don’t):<br>再来试一个有趣的实验。我们向RNN输入一个包含8000个婴儿名字的大文本文件，每行一个名字（名字从这里获得）。我们可以将其输入RNN，然后生成新的名字！以下是一些示例名字，只展示那些没有出现在训练数据中的名字（90%都不在训练数据中）：<br>_Rudi Levette Berice Lussa Hany Mareanne Chrestina Carissy Marylen Hammine Janye Marlise Jacacrie Hendred Romand Charienna Nenotto Ette Dorane Wallen Marly Darine Salina Elvyn Ersia Maralena Minoria Ellia Charmin Antley Nerille Chelon Walmor Evena Jeryly Stachon Charisa Allisa Anatha Cathanie Geetra Alexie Jerin Cassen Herbett Cossie Velen Daurenge Robester Shermond Terisa Licia Roselen Ferine Jayn Lusine Charyanne Sales Sanny Resa Wallon Martine Merus Jelen Candica Wallin Tel Rachene Tarine Ozila Ketia Shanne Arnande Karella Roselina Alessia Chasty Deland Berther Geamar Jackein Mellisand Sagdy Nenc Lessie Rasemy Guen Gavi Milea Anneda Margoris Janin Rodelin Zeanna Elyne Janah Ferzina Susta Pey Castina_</p><p>You can see many more <a href="http://cs.stanford.edu/people/karpathy/namesGenUnique.txt">here</a>. Some of my favorites include “Baby” (haha), “Killie”, “Char”, “R”, “More”, “Mars”, “Hi”, “Saddie”, “With” and “Ahbort”. Well that was fun.﻿ Of course, you can imagine this being quite useful inspiration when writing a novel, or naming a new startup :)<br>你可以在这里看到更多。我最喜欢的一些包括“宝贝”（haha）、“Killie”、“Char”、“R”、“More”、“Mars”、“Hi”、“Saddie”、“With”和“Ahbort”。嗯，这很有趣。当然，你可以想象这在写小说或命名新的创业公司时是非常有用的灵感:)</p><h1 id="understanding-whats-going-on"><a href="#Understanding-what’s-going-on" class="headerlink" title="Understanding what’s going on"></a>Understanding what’s going on</h1><p>We saw that the results at the end of training can be impressive, but how does any of this work? Lets run two quick experiments to briefly peek under the hood.<br>我们看到训练结束时的结果令人印象深刻，但这些是如何实现的呢？让我们进行两个快速实验，简要探究一下其内部工作原理。</p><h2 id="the-evolution-of-samples-while-training"><a href="#The-evolution-of-samples-while-training" class="headerlink" title="The evolution of samples while training"></a>The evolution of samples while training</h2><p>训练时样本的演变<br>First, it’s fun to look at how the sampled text evolves while the model trains. For example, I trained an LSTM of Leo Tolstoy’s War and Peace and then generated samples every 100 iterations of training. At iteration 100 the model samples random jumbles:<br>首先，观察模型训练过程中生成文本的演变是很有趣的。例如，我训练了一个基于列夫·托尔斯泰的《战争与和平》的LSTM，并在每100次迭代训练后生成样本。在第100次迭代时，模型生成的样本是随机混杂的字符：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tyntd-iafhatawiaoihrdemot  lytdws  e ,tfti, astai f ogoh eoase rrranbyne &#x27;nhthnee e </span><br><span class="line">plia tklrgd t o idoe ns,smtt   h ne etie h,hregtrs nigtike,aoaenns lng</span><br></pre></td></tr></table></figure><p>However, notice that at least it is starting to get an idea about words separated by spaces. Except sometimes it inserts two spaces. It also doesn’t know that comma is amost always followed by a space. At 300 iterations we see that the model starts to get an idea about quotes and periods:<br>然而，请注意至少模型开始有了关于单词由空格分隔的概念。尽管有时它会插入两个空格。它还不知道逗号后几乎总是跟着一个空格。在训练300次迭代时，我们看到模型开始了解引号和句号的用法：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&quot;Tmont thithey&quot; fomesscerliund</span><br><span class="line">Keushey. Thom here</span><br><span class="line">sheulke, anmerenith ol sivh I lalterthend Bleipile shuwy fil on aseterlome</span><br><span class="line">coaniogennc Phe lism thond hon at. MeiDimorotion in ther thize.&quot;</span><br></pre></td></tr></table></figure><br>The words are now also separated with spaces and the model starts to get the idea about periods at the end of a sentence. At iteration 500:<br>现在，单词之间也用空格分隔，模型开始理解句末的句号。在第500次迭代时：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">we counter. He stutn co des. His stanted out one ofler that concossions and was </span><br><span class="line">to gearang reay Jotrets and with fre colt otf paitt thin wall. Which das stimn </span><br></pre></td></tr></table></figure><br>the model has now learned to spell the shortest and most common words such as “we”, “He”, “His”, “Which”, “and”, etc. At iteration 700 we’re starting to see more and more English-like text emerge:<br>模型现在已经学会了拼写最短和最常见的单词，比如“we”、“He”、“His”、“Which”、“and”等。在第700次迭代时，我们开始看到越来越多类似英语的文本出现：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Aftair fall unsuch that the hall for Prince Velzonski&#x27;s that me of</span><br><span class="line">her hearly, and behs to so arwage fiving were to it beloge, pavu say falling misfort </span><br><span class="line">how, and Gogition is so overelical and ofter.</span><br></pre></td></tr></table></figure></p><p>At iteration 1200 we’re now seeing use of quotations and question/exclamation marks. Longer words have now been learned as well:<br>在第1200次迭代时，我们现在看到了引号和问号/感叹号的使用。更长的单词也已经被学会了：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&quot;Kite vouch!&quot; he repeated by her</span><br><span class="line">door. &quot;But I would be done and quarts, feeling, then, son is people....&quot;</span><br></pre></td></tr></table></figure><br>Until at last we start to get properly spelled words, quotations, names, and so on by about iteration 2000:<br>直到最后，在大约第2000次迭代时，我们开始得到正确拼写的单词、引号、名字等：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&quot;Why do what that day,&quot; replied Natasha, and wishing to himself the fact the</span><br><span class="line">princess, Princess Mary was easier, fed in had oftened him.</span><br><span class="line">Pierre aking his soul came to the packs and drove up his father-in-law women.</span><br></pre></td></tr></table></figure></p><p>The picture that emerges is that the model first discovers the general word-space structure and then rapidly starts to learn the words; First starting with the short words and then eventually the longer ones. Topics and themes that span multiple words (and in general longer-term dependencies) start to emerge only much later.<br>出现的情况是，模型首先发现了整体的单词-空格结构，然后迅速开始学习单词；先是短单词，然后逐渐学习长单词。跨越多个单词的主题和主题（以及一般的长期依赖关系）要到很久以后才会开始出现。</p><h2 id="visualizing-the-predictions-and-the-neuron-firings-in-the-rnn"><a href="#Visualizing-the-predictions-and-the-“neuron”-firings-in-the-RNN" class="headerlink" title="Visualizing the predictions and the “neuron” firings in the RNN"></a>Visualizing the predictions and the “neuron” firings in the RNN</h2><p>可视化 RNN 中的预测和“神经元”放电</p><p>Another fun visualization is to look at the predicted distributions over characters. In the visualizations below we feed a Wikipedia RNN model character data from the validation set (shown along the blue/green rows) and under every character we visualize (in red) the top 5 guesses that the model assigns for the next character. The guesses are colored by their probability (so dark red = judged as very likely, white = not very likely). For example, notice that there are stretches of characters where the model is extremely confident about the next letter (e.g., the model is very confident about characters during the _<a href="http://www._">http://www._</a> sequence).<br>另一个有趣的可视化是查看模型对字符的预测分布。在下面的可视化中，我们向一个训练好的Wikipedia RNN模型，提供验证集的字符数据（显示在蓝色/绿色行上），并在每个字符下方可视化（用红色表示）模型对下一个字符的前5个猜测。猜测根据其概率进行着色（深红色=被认为非常可能，白色=不太可能）。例如，请注意在某些字符序列中，模型对下一个字符极为自信(例如，模型对 <a href="http://www">http://www</a> 序列中的字符非常有信心）</p><p>The input character sequence (blue/green) is colored based on the _firing_ of a randomly chosen neuron in the hidden representation of the RNN. Think about it as green = very excited and blue = not very excited (for those familiar with details of LSTMs, these are values between [-1,1] in the hidden state vector, which is just the gated and tanh’d LSTM cell state). Intuitively, this is visualizing the firing rate of some neuron in the “brain” of the RNN while it reads the input sequence. Different neurons might be looking for different patterns; Below we’ll look at 4 different ones that I found and thought were interesting or interpretable (many also aren’t):<br>输入字符序列（蓝色/绿色）是根据RNN隐藏表示中随机选择的一个神经元的激活情况进行着色的。可以理解为绿色=非常激动，蓝色=不太激动（对于熟悉LSTM细节的人来说，这些值在隐藏状态向量中介于[-1,1]之间，这是经过门控和tanh函数处理的LSTM单元状态）。直观地说，这是在可视化RNN“脑中”某个神经元在读取输入序列时的激活率。不同的神经元可能在寻找不同的模式；下面我们将查看4个我发现有趣或可解释的神经元（也有很多是无法解释的）：</p><img src="/2472be8a/5.png" class><p>The neuron highlighted in this image seems to get very excited about URLs and turns off outside of the URLs. The LSTM is likely using this neuron to remember if it is inside a URL or not.<br>此图像中突出显示的神经元似乎对 URL 非常兴奋，并在 URL 之外关闭。LSTM 可能使用这个神经元来记住它是否在 URL 内。</p><img src="/2472be8a/6.png" class><p>The highlighted neuron here gets very excited when the RNN is inside the [[ ]] markdown environment and turns off outside of it. Interestingly, the neuron can’t turn on right after it sees the character “[“, it must wait for the second “[“ and then activate. This task of counting whether the model has seen one or two “[“ is likely done with a different neuron.<br>当 RNN 位于 [[ ]] markdown 环境内部并在其外部关闭时，此处突出显示的神经元会非常兴奋。有趣的是，神经元在看到字符“[”后无法立即打开，它必须等待第二个“[”然后激活。计算模型是否看到一个或两个“[”的任务可能是用不同的神经元完成的。</p><img src="/2472be8a/7.png" class><p>Here we see a neuron that varies seemingly linearly across the [[ ]] environment. In other words its activation is giving the RNN a time-aligned coordinate system across the [[ ]] scope. The RNN can use this information to make different characters more or less likely depending on how early/late it is in the [[ ]] scope (perhaps?).<br>在这里，我们看到一个神经元，它在[[ ]]环境中似乎呈线性变化。换句话说，它的激活为 RNN 提供了一个跨 [[ ]] 范围的时间对齐坐标系。RNN 可以使用此信息或多或少地使不同的字符更有可能，具体取决于它在 [[ ]] 范围内的早/晚（也许？</p><img src="/2472be8a/8.png" class><p>Here is another neuron that has very local behavior: it is relatively silent but sharply turns off right after the first “w” in the “www” sequence. The RNN might be using this neuron to count up how far in the “www” sequence it is, so that it can know whether it should emit another “w”, or if it should start the URL.<br>这是另一个具有非常局部行为的神经元：它相对安静，但在“www”序列中的第一个“w”之后急剧关闭。RNN 可能正在使用这个神经元来计算它在“www”序列中的距离，以便它可以知道它是否应该发出另一个“w”，或者它是否应该启动 URL。</p><p>Of course, a lot of these conclusions are slightly hand-wavy as the hidden state of the RNN is a huge, high-dimensional and largely distributed representation. These visualizations were produced with custom HTML/CSS/Javascript, you can see a sketch of what’s involved <a href="http://cs.stanford.edu/people/karpathy/viscode.zip">here</a> if you’d like to create something similar.<br>当然，很多这些结论都有些笼统，因为RNN的隐藏状态是一个巨大的、高维的、广泛分布的表示。这些可视化是使用自定义的HTML/CSS/Javascript生成的，如果你想创建类似的东西，可以在这里看到涉及的内容示例。</p><p>We can also condense this visualization by excluding the most likely predictions and only visualize the text, colored by activations of a cell. We can see that in addition to a large portion of cells that do not do anything interpretible, about 5% of them turn out to have learned quite interesting and interpretible algorithms:<br>我们还可以通过排除最可能的预测并仅根据单元激活情况对文本进行着色来简化这种可视化。我们可以看到，除了大部分不可解释的单元外，大约有5%的单元学会了相当有趣且可解释的算法：<br><img src="/2472be8a/9.png" class><br><img src="/2472be8a/10.png" class><br>Again, what is beautiful about this is that we didn’t have to hardcode at any point that if you’re trying to predict the next character it might, for example, be useful to keep track of whether or not you are currently inside or outside of quote. We just trained the LSTM on raw data and it decided that this is a useful quantitity to keep track of. In other words one of its cells gradually tuned itself during training to become a quote detection cell, since this helps it better perform the final task. This is one of the cleanest and most compelling examples of where the power in Deep Learning models (and more generally end-to-end training) is coming from.<br>再次强调，这其中的美妙之处在于，我们不需要在任何时候硬编码，例如在预测下一个字符时需要跟踪当前是否在引号内或引号外。我们只是对LSTM进行原始数据的训练，它自己决定跟踪这个信息是有用的。换句话说，其中一个单元在训练过程中逐渐调整自己，变成了一个引号检测单元，因为这有助于它更好地完成最终任务。这是深度学习模型（更广泛地说，端到端训练）力量的最清晰和最有说服力的例子之一。</p><h1 id="source-code-源代码"><a href="#Source-Code-源代码" class="headerlink" title="Source Code 源代码"></a>Source Code 源代码</h1><p>I hope I’ve convinced you that training character-level language models is a very fun exercise. You can train your own models using the <a href="https://github.com/karpathy/char-rnn">char-rnn code</a> I released on Github (under MIT license). It takes one large text file and trains a character-level model that you can then sample from. Also, it helps if you have a GPU or otherwise training on CPU will be about a factor of 10x slower. In any case, if you end up training on some data and getting fun results let me know! And if you get lost in the Torch/Lua codebase remember that all it is is just a more fancy version of this <a href="https://gist.github.com/karpathy/d4dee566867f8291f086">100-line gist</a>.<br>我希望我已经让你相信，训练字符级语言模型是一个非常有趣的练习。你可以使用我在Github上发布的char-rnn代码（基于MIT许可证）来训练你自己的模型。它需要一个大型文本文件，并训练一个字符级模型，你可以从中进行采样。此外，如果你有GPU，这会更有帮助，否则在CPU上训练的速度大约会慢10倍。不管怎样，如果你在一些数据上训练并得到了有趣的结果，请告诉我！如果你在Torch/Lua代码库中迷失了方向，请记住，这只是这个100行代码示例的更复杂版本。</p><p>_Brief digression._ The code is written in <a href="http://torch.ch/">Torch 7</a>, which has recently become my favorite deep learning framework. I’ve only started working with Torch/LUA over the last few months and it hasn’t been easy (I spent a good amount of time digging through the raw Torch code on Github and asking questions on their _gitter_ to get things done), but once you get a hang of things it offers a lot of flexibility and speed. I’ve also worked with Caffe and Theano in the past and I believe Torch, while not perfect, gets its levels of abstraction and philosophy right better than others. In my view the desirable features of an effective framework are:<br>简单插曲一下。这段代码是用Torch 7编写的，它最近成为了我最喜欢的深度学习框架。我只是过去几个月才开始使用Torch/LUA，过程并不容易（我花了大量时间在Github上挖掘Torch的源代码，并在他们的gitter上提问以解决问题），但一旦你掌握了它，它就能提供很大的灵活性和速度。我过去也使用过Caffe和Theano，我认为Torch虽然不完美，但它在抽象层次和理念上做得比其他框架更好。在我看来，一个有效框架的理想特性是：</p><ol><li>CPU/GPU transparent Tensor library with a lot of functionality (slicing, array/matrix operations, etc. )<br><strong>CPU/GPU透明的张量库</strong>：具有丰富的功能（切片、数组/矩阵操作等）。）</li><li>An entirely separate code base in a scripting language (ideally Python) that operates over Tensors and implements all Deep Learning stuff (forward/backward, computation graphs, etc)<br><strong>完全独立的脚本语言代码库</strong>：理想情况下是Python，操作张量并实现所有深度学习相关功能（前向/后向传播、计算图等）。</li><li>It should be possible to easily share pretrained models (Caffe does this well, others don’t), and crucially<br><strong>能够轻松共享预训练模型</strong>：Caffe在这方面做得很好，其他框架则不尽如人意。</li><li>NO compilation step (or at least not as currently done in Theano). The trend in Deep Learning is towards larger, more complex networks that are are time-unrolled in complex graphs. It is critical that these do not compile for a long time or development time greatly suffers. Second, by compiling one gives up interpretability and the ability to log/debug effectively. If there is an _option_ to compile the graph once it has been developed for efficiency in prod that’s fine.<br><strong>没有编译步骤</strong>：或至少不像Theano目前那样。深度学习的发展趋势是使用更大、更复杂的网络，这些网络在复杂的计算图中进行时间展开。关键是这些图不应长时间编译，否则会严重影响开发时间。其次，通过编译，会失去可解释性和有效记录/调试的能力。如果有选项可以在开发完成后编译图以提高生产效率，那也很好。</li></ol><h1 id="further-reading-延伸阅读"><a href="#Further-Reading-延伸阅读" class="headerlink" title="Further Reading 延伸阅读"></a>Further Reading 延伸阅读</h1><p>Before the end of the post I also wanted to position RNNs in a wider context and provide a sketch of the current research directions. RNNs have recently generated a significant amount of buzz and excitement in the field of Deep Learning. Similar to Convolutional Networks they have been around for decades but their full potential has only recently started to get widely recognized, in large part due to our growing computational resources. Here’s a brief sketch of a few recent developments (definitely not complete list, and a lot of this work draws from research back to 1990s, see related work sections):<br>在文章的结尾，我还想把RNN放在更广泛的背景中，并提供当前研究方向的概述。最近，RNN在深度学习领域引起了大量关注和兴奋。类似于卷积网络，RNN已经存在了几十年，但它们的全部潜力直到最近才开始被广泛认可，这在很大程度上要归功于我们不断增长的计算资源。以下是一些最近发展的简要概述（绝不是完整的列表，其中很多工作可以追溯到1990年代，详见相关研究部分）：</p><p>In the domain of <strong>NLP/Speech</strong>, RNNs <a href="http://www.jmlr.org/proceedings/papers/v32/graves14.pdf">transcribe speech to text</a>, perform <a href="http://arxiv.org/abs/1409.3215">machine translation</a>, <a href="http://www.cs.toronto.edu/~graves/handwriting.html">generate handwritten text</a>, and of course, they have been used as powerful language models <a href="http://www.cs.utoronto.ca/~ilya/pubs/2011/LANG-RNN.pdf">(Sutskever et al.)</a> <a href="http://arxiv.org/abs/1308.0850">(Graves)</a> <a href="http://www.rnnlm.org/">(Mikolov et al.)</a> (both on the level of characters and words). Currently it seems that word-level models work better than character-level models, but this is surely a temporary thing.<br>在NLP/Speech领域，RNN用于将语音转录为文本、执行机器翻译、生成手写文本，当然，它们也被用作强大的语言模型（Sutskever等人）（Graves）（Mikolov等人）（包括字符级和单词级）。目前看来，单词级模型比字符级模型效果更好，但这肯定只是暂时的。</p><p><strong>Computer Vision.</strong> RNNs are also quickly becoming pervasive in Computer Vision. For example, we’re seeing RNNs in frame-level <a href="http://arxiv.org/abs/1411.4389">video classification</a>, <a href="http://arxiv.org/abs/1411.4555">image captioning</a> (also including my own work and many others), <a href="http://arxiv.org/abs/1505.00487">video captioning</a> and very recently <a href="http://arxiv.org/abs/1505.02074">visual question answering</a>. My personal favorite RNNs in Computer Vision paper is <a href="http://arxiv.org/abs/1406.6247">Recurrent Models of Visual Attention</a>, both due to its high-level direction (sequential processing of images with glances) and the low-level modeling (REINFORCE learning rule that is a special case of policy gradient methods in Reinforcement Learning, which allows one to train models that perform non-differentiable computation (taking glances around the image in this case)). I’m confident that this type of hybrid model that consists of a blend of CNN for raw perception coupled with an RNN glance policy on top will become pervasive in perception, especially for more complex tasks that go beyond classifying some objects in plain view.<br>计算机视觉。RNN也迅速在计算机视觉领域普及。例如，我们看到RNN用于帧级视频分类、图像描述生成（包括我自己的工作和许多其他人的工作）、视频描述生成以及最近的视觉问答。我个人最喜欢的计算机视觉领域的RNN论文是《视觉注意力的递归模型》，因为它在高层次方向（通过扫视对图像进行顺序处理）和低层次建模（REINFORCE学习规则，这是强化学习中策略梯度方法的特例，允许训练执行不可微计算的模型（在本例中是环顾图像））上都很出色。我相信这种混合模型，即结合了用于原始感知的CNN和用于扫视策略的RNN的模型，将在感知领域普及，特别是在超越简单对象分类的复杂任务中。</p><p><strong>Inductive Reasoning, Memories and Attention.</strong> Another extremely exciting direction of research is oriented towards addressing the limitations of vanilla recurrent networks. One problem is that RNNs are not inductive: They memorize sequences extremely well, but they don’t necessarily always show convincing signs of generalizing in the _correct_ way (I’ll provide pointers in a bit that make this more concrete). A second issue is they unnecessarily couple their representation size to the amount of computation per step. For instance, if you double the size of the hidden state vector you’d quadruple the amount of FLOPS at each step due to the matrix multiplication. Ideally, we’d like to maintain a huge representation/memory (e.g. containing all of Wikipedia or many intermediate state variables), while maintaining the ability to keep computation per time step fixed.<br>归纳推理、记忆和注意力。另一个极其令人兴奋的研究方向是解决普通循环网络的局限性。一个问题是RNN不具备归纳能力：它们非常擅长记忆序列，但不一定总是能以正确的方式表现出令人信服的泛化能力（稍后我会提供一些具体的例子）。第二个问题是它们不必要地将表示大小与每步计算量耦合在一起。例如，如果你将隐藏状态向量的大小加倍，那么由于矩阵乘法，每步的浮点运算量（FLOPS）将增加四倍。理想情况下，我们希望保持一个巨大的表示/记忆（例如包含整个维基百科或许多中间状态变量），同时保持每个时间步的计算量固定。</p><p>The first convincing example of moving towards these directions was developed in DeepMind’s <a href="http://arxiv.org/abs/1410.5401">Neural Turing Machines</a> paper. This paper sketched a path towards models that can perform read/write operations between large, external memory arrays and a smaller set of memory registers (think of these as our working memory) where the computation happens. Crucially, the NTM paper also featured very interesting memory addressing mechanisms that were implemented with a (soft, and fully-differentiable) attention model. The concept of <strong>soft attention</strong> has turned out to be a powerful modeling feature and was also featured in <a href="http://arxiv.org/abs/1409.0473">Neural Machine Translation by Jointly Learning to Align and Translate</a> for Machine Translation and <a href="http://arxiv.org/abs/1503.08895">Memory Networks</a> for (toy) Question Answering. In fact, I’d go as far as to say that<br>第一个朝着这些方向前进的令人信服的例子是DeepMind的《神经图灵机》（Neural Turing Machines）论文。这篇论文勾画了一个模型的路径，这些模型可以在大型外部存储阵列和一小组计算发生的存储寄存器（可以将这些视为我们的工作记忆）之间执行读/写操作。至关重要的是，NTM论文还展示了非常有趣的记忆寻址机制，这些机制通过一个（软且完全可微分的）注意力模型实现。软注意力的概念被证明是一个强大的建模特性，它也出现在《通过联合学习对齐和翻译的神经机器翻译》和《记忆网络用于（玩具）问答》中。实际上，我甚至可以说</p><p>The concept of <strong>attention</strong> is the most interesting recent architectural innovation in neural networks.<br>注意力的概念是神经网络中最近最有趣的架构创新。</p><p>Now, I don’t want to dive into too many details but a soft attention scheme for memory addressing is convenient because it keeps the model fully-differentiable, but unfortunately one sacrifices efficiency because everything that can be attended to is attended to (but softly). Think of this as declaring a pointer in C that doesn’t point to a specific address but instead defines an entire distribution over all addresses in the entire memory, and dereferencing the pointer returns a weighted sum of the pointed content (that would be an expensive operation!). This has motivated multiple authors to swap soft attention models for <strong>hard attention</strong> where one samples a particular chunk of memory to attend to (e.g. a read/write action for some memory cell instead of reading/writing from all cells to some degree). This model is significantly more philosophically appealing, scalable and efficient, but unfortunately it is also non-differentiable. This then calls for use of techniques from the Reinforcement Learning literature (e.g. REINFORCE) where people are perfectly used to the concept of non-differentiable interactions. This is very much ongoing work but these hard attention models have been explored, for example, in <a href="http://arxiv.org/abs/1503.01007">Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets</a>, <a href="http://arxiv.org/abs/1505.00521">Reinforcement Learning Neural Turing Machines</a>, and <a href="http://arxiv.org/abs/1502.03044">Show Attend and Tell</a>.<br>现在，我不想深入研究太多细节，但用于内存寻址的软注意力方案很方便，因为它使模型保持完全可微分，但不幸的是，人们牺牲了效率，因为所有可以处理的东西都被处理了（但很软）。可以把它想象成在 C 语言中声明一个指针，该指针不指向特定地址，而是在整个内存中的所有地址上定义整个分布，并且取消引用指针会返回指向内容的加权总和（这将是一个昂贵的操作！这促使多位作者将软注意力模型换成硬注意力模型，其中一个人对要关注的特定内存块进行采样（例如，对某些记忆单元进行读/写操作，而不是在某种程度上从所有单元读取/写入）。这个模型在哲学上更具吸引力、可扩展性和效率，但不幸的是，它也是不可微分的。然后，这需要使用强化学习文献中的技术（例如REINFORCE），在这些技术中，人们完全习惯了不可微分交互的概念。这是一项正在进行的工作，但这些硬注意力模型已经被探索过，例如，在使用堆栈增强的循环网络推断算法模式、强化学习神经图灵机和显示、出席和讲述中。</p><p><strong>People</strong>. If you’d like to read up on RNNs I recommend theses from <a href="http://www.cs.toronto.edu/~graves/">Alex Graves</a>, <a href="http://www.cs.toronto.edu/~ilya/">Ilya Sutskever</a> and <a href="http://www.rnnlm.org/">Tomas Mikolov</a>. For more about REINFORCE and more generally Reinforcement Learning and policy gradient methods (which REINFORCE is a special case of) <a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Home.html">David Silver</a>’s class, or one of <a href="http://www.cs.berkeley.edu/~pabbeel/">Pieter Abbeel</a>’s classes.<br>人。如果你想深入了解RNN，我推荐阅读Alex Graves、Ilya Sutskever和Tomas Mikolov的论文。关于REINFORCE和更广泛的强化学习及策略梯度方法（REINFORCE是其特例），可以参考David Silver的课程，或Pieter Abbeel的课程之一。</p><p><strong>Code</strong>. If you’d like to play with training RNNs I hear good things about <a href="https://github.com/fchollet/keras">keras</a> or <a href="https://github.com/IndicoDataSolutions/Passage">passage</a> for Theano, the <a href="https://github.com/karpathy/char-rnn">code</a> released with this post for Torch, or <a href="https://gist.github.com/karpathy/587454dc0146a6ae21fc">this gist</a> for raw numpy code I wrote a while ago that implements an efficient, batched LSTM forward and backward pass. You can also have a look at my numpy-based <a href="https://github.com/karpathy/neuraltalk">NeuralTalk</a> which uses an RNN/LSTM to caption images, or maybe this <a href="http://jeffdonahue.com/lrcn/">Caffe</a> implementation by Jeff Donahue.<br>代码。如果你想尝试训练RNN，我听说keras或passage for Theano的评价很好，可以参考本文发布的用于Torch的代码，或者我之前编写的这个实现了高效批量LSTM前向和后向传递的纯numpy代码。你也可以看看我基于numpy的NeuralTalk，它使用RNN/LSTM生成图像描述，或者看看Jeff Donahue的这个Caffe实现。</p><h1 id="conclusion-结论"><a href="#Conclusion-结论" class="headerlink" title="Conclusion 结论"></a>Conclusion 结论</h1><p>We’ve learned about RNNs, how they work, why they have become a big deal, we’ve trained an RNN character-level language model on several fun datasets, and we’ve seen where RNNs are going. You can confidently expect a large amount of innovation in the space of RNNs, and I believe they will become a pervasive and critical component to intelligent systems.<br>我们已经了解了RNN，它们是如何工作的，为什么它们变得如此重要。我们在几个有趣的数据集上训练了一个RNN字符级语言模型，并且看到了RNN的发展方向。你可以自信地期待在RNN领域出现大量的创新，我相信它们将成为智能系统中普遍且关键的组成部分。</p><p>Lastly, to add some <strong>meta</strong> to this post, I trained an RNN on the source file of this blog post. Unfortunately, at about 46K characters I haven’t written enough data to properly feed the RNN, but the returned sample (generated with low temperature to get a more typical sample) is:<br>最后，为了给这篇文章增加一些元内容，我用这篇博客文章的源文件训练了一个RNN。不幸的是，大约46K字符的数据量还不足以充分训练RNN，但生成的样本（在低温下生成，以获得更典型的样本）如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">I&#x27;ve the RNN with and works, but the computed with program of the </span><br><span class="line">RNN with and the computed of the RNN with with and the code</span><br></pre></td></tr></table></figure><p>Yes, the post was about RNN and how well it works, so clearly this works :). See you next time!<br>是的，这篇文章是关于 RNN 及其工作情况的，所以很明显这:)工作。下次再见！</p><h1 id="edit-extra-links-编辑额外链接"><a href="#EDIT-extra-links-编辑（额外链接）" class="headerlink" title="EDIT (extra links): 编辑（额外链接）"></a>EDIT (extra links): 编辑（额外链接）</h1><p>Videos: 视频：</p><ul><li>I gave a talk on this work at the <a href="https://skillsmatter.com/skillscasts/6611-visualizing-and-understanding-recurrent-networks">London Deep Learning meetup (video)</a>.<br>我在伦敦深度学习聚会上就这项工作发表了演讲（视频）。</li></ul><p>Discussions: 讨论：</p><ul><li><a href="https://news.ycombinator.com/item?id=9584325">HN discussion HN 讨论</a></li><li>Reddit discussion on <a href="http://www.reddit.com/r/MachineLearning/comments/36s673/the_unreasonable_effectiveness_of_recurrent/">r/machinelearning</a><br>Reddit 关于 r/machinelearning 的讨论</li><li>Reddit discussion on <a href="http://www.reddit.com/r/programming/comments/36su8d/the_unreasonable_effectiveness_of_recurrent/">r/programming</a><br>Reddit 上关于 r/programming 的讨论</li></ul><p>Replies: 答复：</p><ul><li><a href="https://twitter.com/yoavgo">Yoav Goldberg</a> compared these RNN results to <a href="http://nbviewer.ipython.org/gist/yoavg/d76121dfde2618422139">n-gram maximum likelihood (counting) baseline</a><br>Yoav Goldberg 将这些 RNN 结果与 n-gram 最大似然（计数）基线进行了比较</li><li><a href="https://twitter.com/nylk">@nylk</a> trained char-rnn on <a href="https://gist.github.com/nylki/1efbaa36635956d35bcc">cooking recipes</a>. They look great!<br>@nylk培训了char-rnn的烹饪食谱。它们看起来很棒！</li><li><a href="https://twitter.com/MrChrisJohnson">@MrChrisJohnson</a> trained char-rnn on Eminem lyrics and then synthesized a rap song with robotic voice reading it out. Hilarious :)<br>@MrChrisJohnson用 Eminem 的歌词训练了 char-rnn，然后合成了一首带有机器人声音的说唱歌曲。搞笑:)</li><li><a href="https://twitter.com/samim">@samim</a> trained char-rnn on <a href="https://medium.com/@samim/obama-rnn-machine-generated-political-speeches-c8abd18a2ea0">Obama Speeches</a>. They look fun!<br>@samim培训了奥巴马演讲的char-rnn。他们看起来很有趣！</li><li><a href="https://twitter.com/seaandsailor">João Felipe</a> trained char-rnn irish folk music and <a href="https://soundcloud.com/seaandsailor/sets/char-rnn-composes-irish-folk-music">sampled music</a><br>若昂·费利佩（João Felipe）训练了char-rnn爱尔兰民间音乐并采样了音乐</li><li><a href="https://twitter.com/boblsturm">Bob Sturm</a> also trained char-rnn on <a href="https://highnoongmt.wordpress.com/2015/05/22/lisls-stis-recurrent-neural-networks-for-folk-music-generation/">music in ABC notation</a><br>鲍勃·斯特姆（Bob Sturm）还对char-rnn进行了ABC记谱法的音乐培训</li><li><a href="https://twitter.com/RNN_Bible">RNN Bible bot</a> by <a href="https://twitter.com/the__glu/with_replies">Maximilien</a><br>RNN Bible bot 的 Maximilien</li><li><a href="http://cpury.github.io/learning-holiness/">Learning Holiness</a> learning the Bible<br>学习圣洁 学习圣经</li><li><a href="https://www.terminal.com/tiny/ZMcqdkWGOM">Terminal.com snapshot</a> that has char-rnn set up and ready to go in a browser-based virtual machine (thanks <a href="https://www.twitter.com/samim">@samim</a>)<br>Terminal.com 已设置 char-rnn 并准备在基于浏览器的虚拟机中使用的快照（感谢 @samim）</li></ul><h1 id="注释"><a href="#注释" class="headerlink" title="注释"></a>注释</h1><h2 id="1vanilla-神经网络"><a href="#1-Vanilla-神经网络" class="headerlink" title="1.Vanilla 神经网络"></a>1.Vanilla 神经网络</h2><p>“Vanilla 神经网络”通常指的是最基本、最简单的神经网络模型，没有使用任何特殊的层或复杂的架构。具体来说，它一般指的是简单的前馈神经网络（Feedforward Neural Network, FNN）</p><h2 id="2-向量vectorvs-序列sequence"><a href="#2-向量（Vector）vs-序列（Sequence）" class="headerlink" title="2. 向量（Vector）vs 序列（Sequence）"></a>2. 向量（Vector）vs 序列（Sequence）</h2><p><strong>向量（Vector）</strong>：</p><ul><li><strong>定义</strong>：在数学和计算机科学中，向量是一组有序的数值。这些数值可以表示多维空间中的一个点或某个特定的数据结构。</li><li><strong>特性</strong>：<ul><li>固定长度：向量的长度是固定的，例如一个包含三个数值的向量$x_1, x_2, x_3$。 这种处理方式对于图像分类、固定长度的文本分类等任务非常有效。</li><li>无时间依赖：向量中的元素没有时间或顺序上的依赖关系。例如，一张图片的像素数据可以作为输入向量，但这些像素之间没有时间顺序上的关系。</li></ul></li><li><strong>示例</strong>：<ul><li>图像处理中的像素值向量。</li><li>静态文本分类中的单词向量。</li></ul></li><li><strong>向量输入输出</strong>：向量输入输出通常指的是神经网络处理固定长度的向量，即一组固定大小的数值输入和输出。</li><li><strong>应用场景</strong>：<ul><li>图像分类：输入是一个固定大小的图像向量，输出是一个类别标签向量。</li><li>静态文本分类：输入是一个表示单个文档的向量，输出是分类标签。</li></ul></li></ul><p><strong>序列（Sequence）</strong>：</p><ul><li><strong>定义</strong>：序列是一组按特定顺序排列的数据，通常是时间或顺序相关的。</li><li><strong>特性</strong>：<ul><li>变长：序列的长度可以变化，例如一段文本可以是一个字符序列$c_1, c_2, …, c_t$</li><li>有时间依赖：序列中的元素有时间或顺序上的依赖关系。后续元素依赖于前面出现的元素。例如，在自然语言处理中，一个句子的单词顺序决定了句子的意义。</li></ul></li><li><strong>示例</strong>：<ul><li>时间序列数据，如股票价格的每日记录。</li><li>文本数据，如一段句子中的单词序列。</li></ul></li><li><strong>序列输入输出</strong>：序列输入输出指的是神经网络处理一系列的输入数据，这些数据有时间或顺序上的依赖关系。RNN（循环神经网络）就是处理序列数据的典型模型。</li><li><strong>应用场景</strong>：<ul><li>语言模型和文本生成：输入是一个文本序列，输出是下一个字符或单词的预测序列。</li><li>机器翻译：输入是源语言的句子序列，输出是目标语言的句子序列。</li><li>语音识别：输入是语音信号的时间序列，输出是对应的文本序列。</li></ul></li></ul><h2 id="3-rnn-computation"><a href="#3-RNN-computation" class="headerlink" title="3. RNN computation"></a>3. RNN computation</h2><p>论文正文中对该过程的描述文字较多，我反倒觉得结合数学公式后更好理解。<br>这里描述就是 RNN 的前向传播过程， 如果你了解了基础神经网络的前向传播过程，那么RNN也非常好理解。</p><p>简单理解就是 相比在神经网络一文中讲述的基础 前向传播过程中，递归神经网络在此基础上增加了一个隐藏层到隐藏层的权重矩阵参与计算。</p><p>这3个权重矩阵分别对应文中W_hh（隐藏层到隐藏层）、W_xh（输入层到隐藏层）和W_hy（隐藏层到输出层）,下面以$ℎ_𝑡=tanh⁡(𝑊_{ℎℎ}ℎ_{𝑡−1}+𝑊_{𝑥ℎ}𝑥_𝑡)$ 来拆解整个过程</p><p>假设网络结构和参数如下</p><ol><li><strong>输入层</strong>：2个节点，表示输入特征 $x_1$ 和 $x_2$。</li><li><strong>隐藏层</strong>：2个节点，表示隐藏状态 $h_1$ 和 $h_2$​，,使用tanh激活函数。</li><li><strong>输出层</strong>：1个节点，表示输出 $y$，使用线性激活函数。</li></ol><h4 id="权重矩阵"><a href="#权重矩阵" class="headerlink" title="权重矩阵"></a>权重矩阵</h4><ol><li><p><strong>输入层到隐藏层的权重</strong>： $W_{xh} = \begin{bmatrix} W_{11} &amp; W_{12} \ W_{21} &amp; W_{22} \end{bmatrix}$</p><p>$W_{11}$、$W_{12}$​ 连接 $x_1$​ 到 $h_1$​ 和 $h_2$​，$W_{21}$、$W_{22}$​ 连接 $x_2$​ 到 $h_1$​ 和 $h_2$。</p></li><li><p><strong>隐藏层到隐藏层的权重</strong>：$W_{hh} = \begin{bmatrix} U_{11} &amp; U_{12} \ U_{21} &amp; U_{22} \end{bmatrix}$</p><ul><li>$U_{11}$和 $U_{12}$ 连接 $h_1(t-1)$到 $h_1(t)$ 和 $h_2(t)$。</li><li>$U_{21}$ 和 $U_{22}$ 连接 $h_2(t-1)$ 到 $h_1(t)$ 和 $h_2(t)$。<br>[[#5. 序列数据的处理]]</li></ul></li><li><p><strong>隐藏层到输出层的权重</strong>： $W_{hy} = \begin{bmatrix} W_{31} &amp; W_{32} \end{bmatrix}$</p><ul><li>$W_{31}$​ 和 $W_{32}$分别连接$h_1$ 和$h_2$到 $y$。</li></ul></li></ol><h3 id="前向传播过程"><a href="#前向传播过程" class="headerlink" title="前向传播过程"></a>前向传播过程</h3><p>对于每个时间步 $t$，前向传播的计算步骤如下：</p><h4 id="1-输入层到隐藏层"><a href="#1-输入层到隐藏层" class="headerlink" title="1. 输入层到隐藏层"></a>1. <strong>输入层到隐藏层</strong></h4><p>计算当前时间步的隐藏状态, 即正文中的“隐藏状态的更新”<br>$ℎ_𝑡=tanh⁡(𝑊_{ℎℎ}ℎ_{𝑡−1}+𝑊_{𝑥ℎ}𝑥_𝑡)$<br>其中，$\mathbf{x}_t$​ 是当前时间步的输入向量，$\mathbf{h}_{t-1}$是前一时间步的隐藏状态。<br>具体展开如下：</p><p>$\begin{bmatrix} h_{1t} \ h_{2t} \end{bmatrix} = \text{tanh} \left( \begin{bmatrix} W_{11} &amp; W_{12} \ W_{21} &amp; W_{22} \end{bmatrix} \begin{bmatrix} x_{1t} \ x_{2t} \end{bmatrix} + \begin{bmatrix} U_{11} &amp; U_{12} \ U_{21} &amp; U_{22} \end{bmatrix} \begin{bmatrix} h_{1(t-1)} \ h_{2(t-1)} \end{bmatrix} + \begin{bmatrix} b_1 \ b_2 \end{bmatrix} \right)$</p><p>分开计算：<br>$h_{1t} = \text{tanh}(W_{11} x_{1t} + W_{12} x_{2t} + U_{11} h_{1(t-1)} + U_{12} h_{2(t-1)} )$</p><p>$h_{2t} = \text{tanh}(W_{21} x_{1t} + W_{22} x_{2t} + U_{21} h_{1(t-1)} + U_{22} h_{2(t-1)})$</p><h4 id="2-隐藏层到输出层"><a href="#2-隐藏层到输出层" class="headerlink" title="2.  隐藏层到输出层"></a>2.  <strong>隐藏层到输出层</strong></h4><p>计算当前时间步的输出。 $y_t = W_{hy} \mathbf{h}_t$<br>具体展开如下：<br>$y_t = \begin{bmatrix} W_{31} &amp; W_{32} \end{bmatrix} \begin{bmatrix} h_{1t} \ h_{2t} \end{bmatrix}$</p><p>分开计算：</p><p>$y_t = W_{31} h_{1t} + W_{32} h_{2t}$</p><p>⚠️： tanh 是非线性激活函数</p><h2 id="4-1-of-k编码"><a href="#4-1-of-k编码" class="headerlink" title="4. 1-of-k编码"></a>4. 1-of-k编码</h2><p>1-of-k编码，也称为独热编码（One-Hot Encoding），是一种将分类数据转换为二进制向量的方法。它的目的是将非数值型的类别变量转化为适合于机器学习算法处理的数值型数据。</p><h4 id="工作原理"><a href="#工作原理" class="headerlink" title="工作原理"></a>工作原理</h4><p>假设有一个类别变量，它有 $k$ 个不同的类别。1-of-k编码将每个类别表示为一个长度为 $k$ 的二进制向量，其中只有一个位置为1，其他位置为0。</p><p>例如，考虑一个有四个类别的变量：”A”, “B”, “C”, “D”。其1-of-k编码如下：</p><div class="table-container"><table><thead><tr><th>类别</th><th>1-of-k编码</th></tr></thead><tbody><tr><td>A</td><td>[1, 0, 0, 0]</td></tr><tr><td>B</td><td>[0, 1, 0, 0]</td></tr><tr><td>C</td><td>[0, 0, 1, 0]</td></tr><tr><td>D</td><td>[0, 0, 0, 1]</td></tr></tbody></table></div><h4 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h4><ul><li><strong>消除排序关系</strong>：独热编码将分类变量转化为二进制向量，避免了对分类变量的误解，即认为它们之间存在排序关系。</li><li><strong>适合模型处理</strong>：许多机器学习算法（如线性回归、逻辑回归等）无法直接处理非数值型数据，1-of-k编码使这些数据适合于这些算法。</li></ul><h4 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h4><ul><li><strong>高维度问题</strong>：当类别数量较多时，编码后的向量长度会变得很长，导致高维度问题，增加计算和存储成本。</li></ul><h4 id="实际应用"><a href="#实际应用" class="headerlink" title="实际应用"></a>实际应用</h4><ul><li><strong>文本处理</strong>：在自然语言处理（NLP）任务中，1-of-k编码常用于将单词转化为二进制向量。</li><li><strong>分类任务</strong>：在分类任务中，用于将类别标签转化为模型可以处理的数值型数据。</li></ul><p>1-of-k编码是数据预处理中的一种重要技术，广泛应用于各种机器学习和深度学习任务中。它帮助将分类数据转化为数值数据，使得各种模型能够更好地处理这些数据。</p><h2 id="5-置信度"><a href="#5-置信度" class="headerlink" title="5. 置信度"></a>5. 置信度</h2><p>在机器学习中，置信度（confidence）是衡量模型预测结果确定性的一个指标。置信度可以理解为模型对其预测的某个结果是正确的信心程度。</p><p>在机器学习中，置信度通常以概率值的形式表示，反映了模型对某个预测的确定性。例如，在分类任务中，模型对某个样本属于某一类的置信度可能是80%，表示模型认为该样本属于该类的概率为80%。<br>具体应用：</p><ul><li><strong>分类任务</strong>：如图像分类，模型输出每个类别的概率分布。例如，对于一张图片，模型可能输出：[猫: 0.7, 狗: 0.2, 兔子: 0.1]，其中猫的置信度最高。</li><li><p><strong>置信度阈值</strong>：在某些应用中，可能会设置一个置信度阈值，只有当预测的置信度超过某个值时，才认为预测有效。</p><h4 id="置信度的计算"><a href="#置信度的计算" class="headerlink" title="置信度的计算"></a>置信度的计算</h4></li><li><p><strong>概率分布</strong>：使用Softmax函数将模型输出的logits转化为概率分布，这些概率值即为置信度。</p></li><li><strong>Softmax公式</strong>：对于第 $i$ 个输出节点，置信度 $P(y_i)$ 为： $P(y_i) = \frac{e^{z_i}}{\sum_{j} e^{z_j}}$ 其中， $z_i$ 是第 $i$ 个节点的logit。<h3 id="logits"><a href="#Logits" class="headerlink" title="Logits"></a>Logits</h3></li></ul><p>Logits 是神经网络在分类任务中的输出层的原始得分。它们是未经过归一化的数值，用于表示每个类别的相对置信度。Logits 是通过前向传播计算得到的，表示模型对每个可能的类别的初始估计。</p><h4 id="具体理解"><a href="#具体理解" class="headerlink" title="具体理解"></a>具体理解</h4><ol><li><strong>原始得分</strong>：Logits 是神经网络最后一层输出的原始分数，这些分数还没有被转换为概率。</li><li><strong>用途</strong>：在分类任务中，Logits 被用来计算每个类别的概率，这通常通过 Softmax 函数完成。</li><li><strong>Softmax 转换</strong>：Softmax 函数将 Logits 转换为概率分布，使得这些概率的和为1。公式如上</li></ol><h2 id="6-softmax"><a href="#6-Softmax" class="headerlink" title="6. Softmax"></a>6. Softmax</h2><p>[[0-神经网络（Neural Networks）#^cfe178]]</p><h1 id="7-超参数-temperature-温度"><a href="#7-超参数-Temperature-温度" class="headerlink" title="7. 超参数 Temperature 温度"></a>7. 超参数 Temperature 温度</h1><p>Temperature是在模型使用或调优过程中设定的参数，并不通过模型训练过程中的优化算法来更新，因此它属于超参数。<br>Temperature作为超参数，在神经网络中用来控制Softmax输出的概率分布平滑程度，它的作用是调节模型生成样本的多样性和确定性。</p><h4 id="temperature的公式"><a href="#Temperature的公式" class="headerlink" title="Temperature的公式"></a>Temperature的公式</h4><p>Softmax函数带有Temperature的公式如下：</p><p>$P(y_i) = \frac{e^{z_i / T}}{\sum_{j} e^{z_j / T}}$</p><p>其中，$z_i$ 是logits值，$T$ 是Temperature参数。</p><h4 id="temperature的影响"><a href="#Temperature的影响" class="headerlink" title="Temperature的影响"></a>Temperature的影响</h4><ol><li><p><strong>高Temperature（T &gt; 1）</strong>：</p><ul><li><strong>平滑分布</strong>：使概率分布更加平滑，增加生成样本的多样性。</li><li><strong>增加随机性</strong>：输出类别之间的概率差距缩小，使得选择更随机。</li></ul><p>示例：假设 logits 为 $[2.0,1.0,0.1]$，使用 $T = 2$ 时，概率可能变得更接近，如 $[0.4,0.35,0.25]$。</p></li><li><p><strong>低Temperature（0 &lt; T &lt; 1）</strong>：</p><ul><li><strong>尖锐分布</strong>：使概率分布更加尖锐，增加生成样本的确定性。</li><li><strong>减少随机性</strong>：输出类别之间的概率差距加大，使得选择更确定。</li></ul><p>示例：假设 logits 为 $[2.0,1.0,0.1]$，使用 $T = 0.5$ 时，概率可能变得更尖锐，如 $[0.7,0.25,0.05]$。</p></li><li><p><strong>Temperature等于1</strong>：</p><ul><li><strong>标准Softmax</strong>：概率分布保持原样，不做任何调整。</li></ul></li></ol><h4 id="作用及应用场景"><a href="#作用及应用场景" class="headerlink" title="作用及应用场景"></a>作用及应用场景</h4><ol><li><strong>文本生成</strong>：<ul><li>控制生成文本的多样性和创造性。在文本生成任务中，高Temperature可能生成更有创意但不一定合理的句子，而低Temperature可能生成更合理但缺乏多样性的句子。</li></ul></li><li><strong>探索与利用</strong>：<ul><li>在强化学习中，高Temperature用于探索多样性策略，低Temperature用于利用已知的最佳策略。</li></ul></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;a href=&quot;https://karpathy.github.io/2015/05/21/rnn-effectiveness/&quot;&gt;Andrej Karpathy blog:# The Unreasonable Effectiveness of Recurrent Neu</summary>
      
    
    
    
    
    <category term="AI" scheme="http://example.com/tags/AI/"/>
    
    <category term="神经网络" scheme="http://example.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    <category term="Ilya sutskever‘s 30  papers" scheme="http://example.com/tags/Ilya-sutskever%E2%80%98s-30-papers/"/>
    
  </entry>
  
  <entry>
    <title>递归神经网络（Recurrent Neural Networks, RNNs）</title>
    <link href="http://example.com/6ac941eb/"/>
    <id>http://example.com/6ac941eb/</id>
    <published>2024-06-07T14:03:29.000Z</published>
    <updated>2024-06-16T14:25:26.622Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前置概念"><a href="#前置概念" class="headerlink" title="前置概念"></a>前置概念</h1><h2 id="1-时间步time-step和序列数据sequential-data"><a href="#1-时间步（Time-Step）和序列数据（Sequential-Data）" class="headerlink" title="1. 时间步（Time Step）和序列数据（Sequential Data）"></a>1. 时间步（Time Step）和序列数据（Sequential Data）</h2><h3 id="11-序列数据sequential-data"><a href="#1-1-序列数据（Sequential-Data）" class="headerlink" title="1.1 序列数据（Sequential Data）"></a>1.1 序列数据（Sequential Data）</h3><p>序列数据（Sequential Data）是指按照时间或其他顺序排列的数据，其中每个数据点的意义和价值都依赖于它在序列中的位置和前后数据点的关系。序列数据广泛存在于许多实际应用中，如时间序列、自然语言处理、语音识别等。</p><p><strong>序列数据的特点</strong></p><ol><li><strong>时间依赖性</strong>：序列数据中的每个数据点与其前后数据点存在依赖关系。这种依赖性可以是短期的（仅依赖于最近的数据点）或长期的（依赖于较早的数据点）。</li><li><strong>顺序关系</strong>：序列数据的顺序是至关重要的，数据点的顺序关系决定了其实际意义。例如，在语音信号中，音频帧的顺序决定了最终语音的内容。</li><li><strong>动态性</strong>：序列数据往往是动态变化的，数据点的值随时间或其他顺序变化而变化。</li></ol><h3 id="12-时间步"><a href="#1-2-时间步" class="headerlink" title="1.2 时间步"></a>1.2 时间步</h3><p>时间步指的是序列数据中的每一个元素在时间维度上的位置。例如，在一个时间序列中，每个时间点上的数据称为一个时间步。在递归神经网络中，输入序列按时间步逐步处理，每个时间步的输入不仅影响当前时间步的输出，还影响后续时间步的计算。</p><h3 id="13-序列数据与时间步的实际例子"><a href="#1-3-序列数据与时间步的实际例子" class="headerlink" title="1.3  序列数据与时间步的实际例子"></a>1.3  序列数据与时间步的实际例子</h3><p>序列数据可以分为多种类型，具体包括：</p><h4 id="1-时间序列数据"><a href="#1-时间序列数据" class="headerlink" title="1. 时间序列数据"></a>1. 时间序列数据</h4><p>时间序列数据是按照时间顺序排列的数据，常用于金融市场分析、气象预报等领域。<br><strong>示例</strong>：股票价格、温度记录等。<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">时间：    t1   t2   t3   t4   t5</span><br><span class="line">股票价格： 100  102  101  103  104</span><br></pre></td></tr></table></figure></p><h4 id="2-自然语言处理数据"><a href="#2-自然语言处理数据" class="headerlink" title="2. 自然语言处理数据"></a>2. 自然语言处理数据</h4><p>自然语言处理中的数据是按照文本中的顺序排列的单词或字符，常用于文本分类、机器翻译等任务。</p><p><strong>示例</strong>：一段文本、一句话等。<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">时间： t1   t2    t3    t4     t5</span><br><span class="line">句子： 我   喜欢   学习   机器   学习</span><br></pre></td></tr></table></figure></p><h4 id="3-语音信号数据"><a href="#3-语音信号数据" class="headerlink" title="3. 语音信号数据"></a>3. 语音信号数据</h4><p>语音信号是连续的音频帧序列，每个音频帧表示一小段时间内的声音特征，常用于语音识别、语音合成等任务。</p><p><strong>示例</strong>：一段语音信号。<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">时间步：  t1    t2    t3    t4    t5</span><br><span class="line">音频帧：  F1    F2    F3    F4    F5</span><br></pre></td></tr></table></figure></p><h4 id="4-视频数据"><a href="#4-视频数据" class="headerlink" title="4. 视频数据"></a>4. 视频数据</h4><p>视频数据是由一系列按时间顺序排列的图像帧组成，常用于视频分类、目标检测等任务。</p><p><strong>示例</strong>：一段视频。<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">时间步：  t1    t2    t3    t4    t5</span><br><span class="line">视频帧：  F1    F2    F3    F4    F5</span><br></pre></td></tr></table></figure></p><h3 id="14-序列数据的实际应用场景"><a href="#1-4-序列数据的实际应用场景" class="headerlink" title="1.4 序列数据的实际应用场景"></a>1.4 序列数据的实际应用场景</h3><ol><li><p>时间序列预测：通过分析历史数据，预测未来的值。<br><strong>示例</strong>：股票价格预测、天气预报、销售量预测等。</p></li><li><p>自然语言处理：处理和理解人类语言，执行各种语言相关任务。<br><strong>示例</strong>：文本分类、情感分析、机器翻译、文本生成等。</p></li><li><p>语音识别：将语音信号转换为文本，或者识别语音中的情感、语种等信息。<br><strong>示例</strong>：语音到文本转换、语音情感识别、语音翻译等。</p></li><li><p>视频处理：处理和分析视频数据，执行各种视频相关任务。<br><strong>示例</strong>：视频分类、目标检测、行为识别等</p></li></ol><h3 id="15-序列数据的处理"><a href="#1-5-序列数据的处理" class="headerlink" title="1.5 序列数据的处理"></a>1.5 序列数据的处理</h3><p>处理序列数据时，需要考虑数据的顺序和时间依赖性。以下是一些常见的处理方法和模型：</p><h4 id="1-递归神经网络recurrent-neural-networks-rnns"><a href="#1-递归神经网络（Recurrent-Neural-Networks-RNNs）" class="headerlink" title="1. 递归神经网络（Recurrent Neural Networks, RNNs）"></a>1. 递归神经网络（Recurrent Neural Networks, RNNs）</h4><p>RNNs通过循环结构能够记住和利用序列数据中的时间依赖性，适用于处理各种序列数据。</p><p><strong>特点</strong>：</p><ul><li>能够处理变长序列数据。</li><li>能够捕捉短期和长期依赖关系。</li></ul><h4 id="2-长短期记忆网络long-short-term-memory-lstm"><a href="#2-长短期记忆网络（Long-Short-Term-Memory-LSTM）" class="headerlink" title="2. 长短期记忆网络（Long Short-Term Memory, LSTM）"></a>2. 长短期记忆网络（Long Short-Term Memory, LSTM）</h4><p>LSTM是RNN的一种变体，专门用于解决RNN中的梯度消失和梯度爆炸问题，能够更好地捕捉长序列中的依赖关系。</p><p><strong>特点</strong>：</p><ul><li>通过引入遗忘门、输入门和输出门控制信息流动。</li><li>能够记住长时间的依赖关系。</li></ul><h4 id="3-门控循环单元gated-recurrent-unit-gru"><a href="#3-门控循环单元（Gated-Recurrent-Unit-GRU）" class="headerlink" title="3. 门控循环单元（Gated Recurrent Unit, GRU）"></a>3. 门控循环单元（Gated Recurrent Unit, GRU）</h4><p>GRU是LSTM的简化版本，具有类似的门控机制，但参数更少，计算效率更高。</p><p><strong>特点</strong>：</p><ul><li>通过更新门和重置门控制信息流动。</li><li>计算效率更高，适合处理长序列数据。</li></ul><h4 id="4-transformer模型"><a href="#4-Transformer模型" class="headerlink" title="4. Transformer模型"></a>4. Transformer模型</h4><p>Transformer模型通过自注意力机制（Self-Attention）处理序列数据，能够并行处理序列中的每个位置，解决了传统RNN的计算瓶颈问题。</p><p><strong>特点</strong>：</p><ul><li>通过注意力机制捕捉序列中任意位置之间的依赖关系。</li><li>计算效率高，适合处理长序列数据。</li></ul><h1 id="2-什么是rnn"><a href="#2-什么是RNN" class="headerlink" title="2. 什么是RNN"></a>2. 什么是RNN</h1><p>递归神经网络（Recurrent Neural Networks, RNNs）是一类专门用于处理序列数据的神经网络模型。RNNs通过循环结构使得网络能够捕捉和记忆输入数据中的时间依赖性，适用于各种序列数据处理任务，如时间序列预测、自然语言处理、语音识别等。</p><p>如果你已经对<a href="obsidian://open?vault=Documents&amp;file=second-brain%2F0-projects%2Filya%2027%E4%BE%BF%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB%2F0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88Neural%20Networks%EF%BC%89">基础神经网络的层次结构和前向传播过程</a>了解了， 那么RNN 与它的区别就是其隐藏层具有循环连接，使得当前时间步的隐藏状态不仅依赖于当前输入，还依赖于前一时间步的隐藏状态。</p><p>下面在 基础神经网络中的层次结构的基础上说明 RNN</p><h2 id="网络结构和参数"><a href="#网络结构和参数" class="headerlink" title="网络结构和参数"></a>网络结构和参数</h2><ol><li><strong>输入层</strong>：2个节点，表示输入特征 $x_1$ 和 $x_2​$。</li><li><strong>隐藏层</strong>：2个节点，表示隐藏状态 $h_1$ 和 $h_2$​，偏置向量分别为$b_1$、$b_2$,使用ReLU激活函数。</li><li><strong>输出层</strong>：1个节点，表示输出 $y$，偏置向量分别为$b_3$,使用线性激活函数。</li><li><p><strong>权重矩阵</strong></p><ol><li><strong>输入层到隐藏层的权重</strong>： $W_{xh} = \begin{bmatrix} W_{11} &amp; W_{12} \ W_{21} &amp; W_{22} \end{bmatrix}$</li></ol><p>$W_{11}​$、$W_{12}$​ 连接 $x_1$​ 到 $h_1$​ 和 $h_2$​，$W_{21}​$、$W_{22}$​ 连接 $x_2$​ 到 $h_1$​ 和 $h_2​$。</p><ol><li><p><strong>隐藏层到隐藏层的权重</strong>：$W_{hh} = \begin{bmatrix} U_{11} &amp; U_{12} \ U_{21} &amp; U_{22} \end{bmatrix}$</p><ul><li>$U_{11}​$和 $U_{12}​$ 连接 $h_1(t-1)$到 $h_1(t)$ 和 $h_2(t)$。</li><li>$U_{21}​$ 和 $U_{22}$ 连接 $h_2(t-1)$ 到 $h_1(t)$ 和 $h_2(t)$。</li></ul></li><li><strong>隐藏层到输出层的权重</strong>： $W_{hy} = \begin{bmatrix} W_{31} &amp; W_{32} \end{bmatrix}$<ul><li>$W_{31}$​ 和 $W_{32}$分别连接$h_1$ 和$h_2​$到 $y$。</li></ul></li></ol></li><li><p>偏置向量</p><ul><li><strong>隐藏层的偏置</strong>：$\mathbf{b}_h = \begin{bmatrix} b_1 \ b_2 \end{bmatrix}$</li><li><strong>输出层的偏置</strong>：$b_3$<h2 id="前向传播过程"><a href="#前向传播过程" class="headerlink" title="前向传播过程"></a>前向传播过程</h2></li></ul></li></ol><p>对于每个时间步 $t$，前向传播的计算步骤如下：</p><ol><li><strong>输入层到隐藏层</strong>：计算当前时间步的隐藏状态。$\mathbf{h}_t = \text{ReLU}(W_{xh} \mathbf{x}_t + W_{hh} \mathbf{h}_{t-1} + \mathbf{b}_h)$ 其中，$\mathbf{x}_t$​ 是当前时间步的输入向量，$\mathbf{h}_{t-1}​$是前一时间步的隐藏状态。<br>具体展开如下：</li></ol><p>$\begin{bmatrix} h_{1t} \ h_{2t} \end{bmatrix} = \text{ReLU} \left( \begin{bmatrix} W_{11} &amp; W_{12} \ W_{21} &amp; W_{22} \end{bmatrix} \begin{bmatrix} x_{1t} \ x_{2t} \end{bmatrix} + \begin{bmatrix} U_{11} &amp; U_{12} \ U_{21} &amp; U_{22} \end{bmatrix} \begin{bmatrix} h_{1(t-1)} \ h_{2(t-1)} \end{bmatrix} + \begin{bmatrix} b_1 \ b_2 \end{bmatrix} \right)$</p><p>分开计算：</p><p>$h_{1t} = \text{ReLU}(W_{11} x_{1t} + W_{12} x_{2t} + U_{11} h_{1(t-1)} + U_{12} h_{2(t-1)} + b_1)$</p><p>$h_{2t} = \text{ReLU}(W_{21} x_{1t} + W_{22} x_{2t} + U_{21} h_{1(t-1)} + U_{22} h_{2(t-1)} + b_2)$</p><ol><li><strong>隐藏层到输出层</strong>：计算当前时间步的输出。 $y_t = W_{hy} \mathbf{h}_t + b_3$<br>具体展开如下：</li></ol><p>$y_t = \begin{bmatrix} W_{31} &amp; W_{32} \end{bmatrix} \begin{bmatrix} h_{1t} \ h_{2t} \end{bmatrix} + b_3$</p><p>分开计算：</p><p>$y_t = W_{31} h_{1t} + W_{32} h_{2t} + b_3$</p><h3 id="dropout在rnn中的表现"><a href="#Dropout在RNN中的表现" class="headerlink" title="Dropout在RNN中的表现"></a>Dropout在RNN中的表现</h3><p>在传统的前馈神经网络中，Dropout被证明是非常有效的正则化方法。然而，直接将这种方法应用于RNN时，会破坏时间步之间的依赖关系，导致模型性能下降。</p><p>在RNN中，Dropout的效果确实没有在前馈神经网络中那么明显，原因如下：</p><ol><li><strong>时间步之间的依赖性</strong>：RNN中的隐藏层具有时间步之间的依赖关系，即当前时间步的隐藏状态依赖于前一时间步的隐藏状态。传统的Dropout方法在RNN中会破坏这种时间步之间的依赖关系，从而影响模型的学习效果。</li><li><strong>梯度传播的影响</strong>：RNN通过反向传播通过时间（Backpropagation Through Time, BPTT）来更新权重，Dropout会在时间步之间引入不稳定性，可能导致梯度传播过程中的问题，如梯度消失或梯度爆炸。</li></ol><h1 id="3-rnn中的梯度消失和梯度爆炸"><a href="#3-RNN中的梯度消失和梯度爆炸" class="headerlink" title="3. RNN中的梯度消失和梯度爆炸"></a>3. RNN中的梯度消失和梯度爆炸</h1><p>在训练循环神经网络（RNN）时，常常会遇到梯度消失和梯度爆炸问题，这两者都是由RNN的反向传播算法（BPTT, Backpropagation Through Time）引起的。</p><h4 id="1-梯度消失vanishing-gradient"><a href="#1-梯度消失（Vanishing-Gradient）" class="headerlink" title="1. 梯度消失（Vanishing Gradient）"></a>1. 梯度消失（Vanishing Gradient）</h4><p><strong>现象</strong>：</p><ul><li>当梯度在反向传播过程中逐层传递时，它的数值会逐渐变小，最终趋近于零。</li><li>这导致前面层的权重更新几乎停止，使网络难以训练。</li></ul><p><strong>原因</strong>：</p><ul><li>在反向传播过程中，梯度是通过链式法则逐层相乘的。如果某些层的梯度小于1（例如小于1的激活函数导数），则乘积会快速缩小。</li><li>常见的激活函数如tanh和sigmoid在输入值较大或较小时，其导数接近零，从而加剧了梯度消失问题。</li></ul><h4 id="2-梯度爆炸exploding-gradient"><a href="#2-梯度爆炸（Exploding-Gradient）" class="headerlink" title="2. 梯度爆炸（Exploding Gradient）"></a>2. 梯度爆炸（Exploding Gradient）</h4><p><strong>现象</strong>：</p><ul><li>当梯度在反向传播过程中逐层传递时，它的数值会逐渐变大，最终变得非常大。</li><li>这导致前面层的权重更新过大，使网络参数变得不稳定，甚至导致溢出。</li></ul><p><strong>原因</strong>：</p><ul><li>在反向传播过程中，如果某些层的梯度大于1（例如大于1的激活函数导数），则乘积会快速增大。</li><li>常见的原因包括不合理的初始化权重和未处理的数值不稳定问题。</li></ul><h3 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h3><ol><li><strong>梯度裁剪（Gradient Clipping）</strong>：<ul><li>当梯度的范数超过某个阈值时，将其缩放到该阈值。</li><li>这可以有效防止梯度爆炸。</li></ul></li><li><strong>长短期记忆网络（LSTM）和门控循环单元（GRU）</strong>：<ul><li>这些是专门设计用于缓解梯度消失和梯度爆炸问题的RNN变种。</li><li>通过引入门机制，它们能够更好地保持长时间依赖。</li></ul></li><li><strong>适当的权重初始化</strong>：<ul><li>使用如Xavier初始化或He初始化来设置初始权重，可以减少梯度消失和爆炸的风险。</li></ul></li><li><strong>使用不同的激活函数</strong>：<ul><li>ReLU等激活函数在一定程度上可以缓解梯度消失问题。</li></ul></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;前置概念&quot;&gt;&lt;a href=&quot;#前置概念&quot; class=&quot;headerlink&quot; title=&quot;前置概念&quot;&gt;&lt;/a&gt;前置概念&lt;/h1&gt;&lt;h2 id=&quot;1-时间步time-step和序列数据sequential-data&quot;&gt;&lt;a href=&quot;#1-时间步（Time-</summary>
      
    
    
    
    
    <category term="AI" scheme="http://example.com/tags/AI/"/>
    
    <category term="神经网络" scheme="http://example.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    <category term="Ilya sutskever‘s 30  papers" scheme="http://example.com/tags/Ilya-sutskever%E2%80%98s-30-papers/"/>
    
  </entry>
  
  <entry>
    <title>神经网络（Neural Networks）</title>
    <link href="http://example.com/f27811be/"/>
    <id>http://example.com/f27811be/</id>
    <published>2024-06-07T09:16:16.000Z</published>
    <updated>2024-07-11T08:40:25.010Z</updated>
    
    <content type="html"><![CDATA[<h1 id="0-什么是神经网络-neural-networks"><a href="#0-什么是神经网络-Neural-Networks" class="headerlink" title="0. 什么是神经网络 (Neural Networks)"></a>0. 什么是神经网络 (Neural Networks)</h1><p>神经网络是实现AI 的一种技术手段，一种广泛用于机器学习（Machine Learning）和深度学习（Deep Learning）领域的计算模型/算法架构。</p><p>它受到人类大脑神经元（Neurons）和它们的互动方式的启发，它由多个层（Layers）组成，每层包含多个神经元，这些神经元通过权重（Weights）连接传递信息。</p><p>神经网络的训练过程基于机器学习的基本前提，即能够从数据中学习。通过向网络提供大量的数据样本（包括输入和期望的输出），神经网络可以学习到如何映射输入到输出，这种能力是通过调整内部结构（即权重）来实现的。</p><p>这一学习过程使用了机器学习中的核心概念，如损失函数（Loss Functions）、梯度下降（Gradient Descent）和反向传播算法（Backpropagation Algorithms）。这些都是机器学习领域的基本工具，用于训练模型以改进其性能。</p><h1 id="1-神经网络的层次结构"><a href="#1-神经网络的层次结构" class="headerlink" title="1. 神经网络的层次结构"></a>1. 神经网络的层次结构</h1><h2 id="11-节点nodes或神经元neurons"><a href="#1-1-节点（Nodes）或神经元（Neurons）" class="headerlink" title="1.1  节点（Nodes）或神经元（Neurons）"></a>1.1  节点（Nodes）或神经元（Neurons）</h2><p>节点是神经网络的基本单位，也称为神经元。每个节点接收输入信号，进行加权求和和非线性变换，然后将结果传递给下一层的节点。节点的主要功能包括：</p><ol><li><strong>接收输入</strong>：从前一层的所有节点接收输入信号。</li><li><strong>加权求和</strong>：对接收到的输入信号乘以相应的权重并求和。</li><li><strong>应用激活函数</strong>：对加权求和的结果应用激活函数，生成节点的输出。</li><li><strong>传递输出</strong>：将输出信号传递给下一层的节点。</li></ol><h2 id="12-层layers"><a href="#1-2-层（Layers）" class="headerlink" title="1.2  层（Layers）"></a>1.2  层（Layers）</h2><p>层是由多个节点组成的一个集合，在神经网络中起着分层处理输入数据的作用。每一层中的节点执行相同类型的计算，但每个节点有各自的权重和偏置。</p><p>一个典型的神经网络由以下几个部分组成：</p><ul><li><strong>输入层（Input Layer）</strong>：接收外部输入数据，每个节点代表一个输入特征。</li><li><strong>隐藏层（Hidden Layers）</strong>：位于输入层和输出层之间，用于特征提取和数据变换。一个神经网络可以有一个或多个隐藏层。</li><li><strong>输出层（Output Layer）</strong>：生成最终的预测结果，每个节点代表一个输出。</li></ul><h2 id="13-节点和层的关系"><a href="#1-3-节点和层的关系" class="headerlink" title="1.3  节点和层的关系"></a>1.3  节点和层的关系</h2><p>节点和层之间的关系可以概括如下：</p><ol><li><strong>层是节点的集合</strong>：每一层由多个节点组成，这些节点在同一层内进行相同类型的计算。</li><li><strong>数据传递和处理</strong>：数据在神经网络中逐层传递，每一层的节点接收前一层的输出，进行计算后将结果传递给下一层的节点。</li><li><strong>层次结构</strong>：神经网络的层次结构决定了数据处理的顺序和方式。输入数据经过输入层后，依次通过一个或多个隐藏层进行复杂的特征提取，最终通过输出层生成预测结果。</li></ol><h2 id="14-层与层之间的连接"><a href="#1-4-层与层之间的连接" class="headerlink" title="1.4 层与层之间的连接"></a>1.4 层与层之间的连接</h2><p>层与层之间的连接由权重矩阵决定。每一层的节点与下一层的每个节点相连，连接的强度由权重值决定。在前向传播过程中，输入层的节点将输入数据传递给第一个隐藏层的节点，后者再将处理后的数据传递给下一个隐藏层，依此类推，直到输出层。</p><h1 id="2-权重"><a href="#2-权重" class="headerlink" title="2. 权重"></a>2. 权重</h1><p>权重是连接神经网络中不同层节点的参数，用于调节输入信号在网络中的传递和变换强度。每个连接都有一个权重，表示输入信号对输出信号的影响力。</p><ul><li><strong>调节信号强度</strong>：权重乘以输入信号后，再通过求和和激活函数的变换，决定了输出信号的值。不同的权重值代表输入信号的重要程度。</li><li><strong>特征学习</strong>：通过学习和调整权重，神经网络能够识别和提取输入数据中的重要特征。</li><li><strong>优化</strong>：在训练过程中，权重被调整以最小化预测误差。通过反向传播算法，权重的值不断更新，使得模型的预测更加准确。</li></ul><h2 id="21-权重的存在形式"><a href="#2-1-权重的存在形式" class="headerlink" title="2.1 权重的存在形式"></a>2.1 权重的存在形式</h2><p>权重并不存在于单独的节点内，而是存在于节点之间的连接上。具体来说，每一层的节点与下一层的节点之间的连接由一组权重参数表示。权重矩阵表示了两个层之间所有节点连接的权重值。</p><h2 id="22-权重的初始化和调整"><a href="#2-2-权重的初始化和调整" class="headerlink" title="2.2 权重的初始化和调整"></a>2.2 权重的初始化和调整</h2><p>权重的初始化方法有多种，包括零初始化、随机初始化、Xavier初始化和He初始化等。这些方法旨在为模型提供一个良好的起点，以便更好地进行训练。</p><h2 id="23-权重的训练调整"><a href="#2-3-权重的训练调整" class="headerlink" title="2.3 权重的训练调整"></a>2.3 权重的训练调整</h2><p>在训练过程中，权重通过优化算法进行调整。最常用的优化算法是梯度下降（Gradient Descent）及其变种（如随机梯度下降、Adam优化算法等）。训练步骤如下：</p><ol><li><strong>前向传播</strong>：计算网络的预测输出。</li><li><strong>计算损失</strong>：通过损失函数衡量预测输出与真实标签之间的差距。</li><li><strong>反向传播</strong>：计算损失函数对每个权重的梯度。</li><li><strong>更新权重</strong>：使用梯度下降法更新权重，以最小化损失函数。</li></ol><h2 id="24-优化算法"><a href="#2-4-优化算法" class="headerlink" title="2.4 优化算法"></a>2.4 优化算法</h2><p>为了提高训练效率和稳定性，可以使用各种优化算法，包括：</p><ul><li><strong>动量（Momentum）</strong>：结合当前梯度和之前更新的方向，加速收敛。</li><li><strong>AdaGrad</strong>：根据过去的梯度调整学习率，适应性地进行参数更新。</li><li><strong>RMSProp</strong>：改进AdaGrad，使用指数加权平均计算梯度平方和，避免学习率过快减小。</li><li><strong>Adam</strong>：结合动量和RMSProp的优点，适应性地调整学习率和动量，广泛应用于各种神经网络训练。</li></ul><h2 id="25-权重在不同类型神经网络中的角色"><a href="#2-5-权重在不同类型神经网络中的角色" class="headerlink" title="2.5 权重在不同类型神经网络中的角色"></a>2.5 权重在不同类型神经网络中的角色</h2><p>不同类型的神经网络中，权重的具体作用可能有所不同：</p><ul><li><strong>前馈神经网络（Feedforward Neural Networks, FNNs）</strong>：权重连接每一层的所有节点，负责将输入数据逐层传递和变换。</li><li><strong>卷积神经网络（Convolutional Neural Networks, CNNs）</strong>：权重是卷积核的参数，负责在局部感知野上提取特征。</li><li><strong>递归神经网络（Recurrent Neural Networks, RNNs）</strong>：权重用于处理序列数据，包括当前时间步的输入与前一时间步的隐藏状态之间的关系。</li></ul><h1 id="3-偏置biases"><a href="#3-偏置（Biases）" class="headerlink" title="3. 偏置（Biases）"></a>3. 偏置（Biases）</h1><p>偏置是每个节点附加的一个参数，用于调整节点的输出独立于输入信号。偏置帮助神经网络学到更灵活的决策边界。</p><p><strong>作用</strong></p><ul><li><strong>调节输出</strong>：偏置提供一个额外的自由度，使得神经网络能够更好地拟合数据。</li><li><strong>避免零输出</strong>：在输入信号为零的情况下，偏置确保节点仍然能够产生非零输出。</li></ul><h1 id="4-激活函数activation-function"><a href="#4-激活函数（Activation-Function）" class="headerlink" title="4. 激活函数（Activation Function）"></a>4. 激活函数（Activation Function）</h1><p>激活函数（Activation Function）是神经网络中的一个关键组件，它引入了非线性变换，使得神经网络能够学习和表示复杂的非线性关系。如果没有激活函数，神经网络的每一层只进行线性变换，那么无论多少层的堆叠，整体仍然是一个线性变换。这将极大限制神经网络的表示能力。因此，激活函数对于神经网络的性能和能力至关重要。</p><p>线性（Linear）和非线性（Nonlinear）是数学和信号处理中两个基本的概念，这些概念在神经网络和机器学习中也具有重要意义。理解这两个概念有助于掌握为什么激活函数对神经网络的性能如此重要。</p><h2 id="41-线性linearvs-非线性nonlinear"><a href="#4-1-线性（Linear）vs-非线性（Nonlinear）" class="headerlink" title="4.1 线性（Linear）vs  非线性（Nonlinear）"></a>4.1 线性（Linear）vs  非线性（Nonlinear）</h2><h3 id="411-线性linear"><a href="#4-1-1-线性（Linear）" class="headerlink" title="4.1.1 线性（Linear）"></a>4.1.1 线性（Linear）</h3><p>线性关系是指两个变量之间的关系可以用一个线性方程表示。对于一个变量 $x$ 和其对应的输出 $y$，线性关系可以表示为：<br>$y = mx + b$<br>其中，$m$ 是斜率，$b$ 是截距。这意味着如果我们绘制 $y$ 对 $x$ 的图像，它将是一条直线。</p><p><strong>特点</strong></p><ul><li><strong>叠加性</strong>：线性系统满足叠加原理，即输入的线性组合会产生输出的线性组合。如果 $f(x_1) = y_1$​ 和$f(x_2) = y_2$，那么对于任何常数 $a$和 $b$，有 $f(ax_1 + bx_2) = ay_1 + by_2$​。</li><li><strong>同质性</strong>：线性系统满足同质性，即输入的放大会导致输出的相应放大。如果 $f(x) = y$，那么对于任何常数 $k$，有 $f(kx) = ky$。</li></ul><p><strong>在神经网络中的应用</strong><br>在神经网络中，线性变换通常通过矩阵乘法和加法实现，例如输入与权重矩阵的乘积加上偏置：<br>$z = W \cdot x + b$</p><h3 id="412-非线性nonlinear"><a href="#4-1-2-非线性（Nonlinear）" class="headerlink" title="4.1.2 非线性（Nonlinear）"></a>4.1.2 非线性（Nonlinear）</h3><p>非线性关系是指两个变量之间的关系不能用一个简单的线性方程表示。非线性关系的数学表示形式可以非常多样，常见的形式包括多项式、指数函数、对数函数、三角函数等。<br>例如，对于变量 $x$ 和 $y$，非线性关系可以表示为：<br>$y=ax^2+bx+c$<br>其中，$a$、$b$、$c$ 是常数。这意味着如果我们绘制 $y$ 对 $x$ 的图像，它将不是一条直线，而是一个曲线。<br><strong>特点</strong></p><ul><li><strong>非叠加性</strong>：非线性系统不满足叠加原理。如果 $f(x_1) = y_1​$ 和 $f(x_2) = y_2​$，那么 $f(ax_1 + bx_2) \neq ay_1 + by_2$​。</li><li><strong>复杂性</strong>：非线性系统可以表示复杂的关系和模式，能够捕捉到数据中的复杂结构和动态。</li></ul><p><strong>为什么需要非线性</strong><br>如果神经网络只使用线性激活函数（例如，恒等函数 f(z)=z），那么无论网络有多少层，其最终输出仍然是输入的线性变换。也就是说，整个网络等效于一个单层的线性模型，无法捕捉数据中的复杂关系。因此，引入非线性激活函数使得网络具有更强的表达能力，能够学习和表示复杂的非线性关系，从而解决更复杂的问题。</p><h2 id="42-常见的非线性激活函数"><a href="#4-2-常见的非线性激活函数" class="headerlink" title="4.2 常见的非线性激活函数"></a>4.2 常见的非线性激活函数</h2><h4 id="421-sigmoid-函数"><a href="#4-2-1-Sigmoid-函数" class="headerlink" title="4.2.1 Sigmoid 函数"></a>4.2.1 Sigmoid 函数</h4><p>$σ(z)=\frac{1}{1+e^{−z}} ​$</p><p><strong>特点</strong>：</p><ul><li>输出值在0到1之间，适用于概率预测。</li><li>在极值区间梯度较小，可能导致梯度消失问题。</li><li>计算复杂度较高。</li></ul><p><strong>应用</strong>：</p><ul><li>常用于二分类问题的输出层。</li></ul><h4 id="422-tanh双曲正切函数"><a href="#4-2-2-Tanh（双曲正切）函数" class="headerlink" title="4.2.2 Tanh（双曲正切）函数"></a>4.2.2 Tanh（双曲正切）函数</h4><p>$tanh(z)=\frac{e^z-e^{-z}}{e^z+e^{-z}}$<br>​<br><strong>特点</strong>：</p><ul><li>输出值在-1到1之间。</li><li>相对于Sigmoid函数，Tanh函数的输出均值为0，使得数据更中心化。</li><li>也存在梯度消失问题，但在0附近的梯度较大，梯度消失问题稍好于Sigmoid。</li></ul><p><strong>应用</strong>：</p><ul><li>常用于隐藏层。<h3 id="423-relurectified-linear-unit函数"><a href="#4-2-3-ReLU（Rectified-Linear-Unit）函数" class="headerlink" title="4.2.3 ReLU（Rectified Linear Unit）函数"></a>4.2.3 ReLU（Rectified Linear Unit）函数</h3>$ReLU(z)=max(0,z)$</li></ul><p><strong>特点</strong>：</p><ul><li>计算简单，收敛速度快。</li><li>在正区间保持线性关系，在负区间输出为0。</li><li>解决了Sigmoid和Tanh的梯度消失问题。</li><li>可能导致部分神经元“死亡”，即在训练过程中某些神经元的输出始终为0，不再更新。</li></ul><p><strong>应用</strong>：</p><ul><li>广泛用于隐藏层。<h4 id="424-leaky-relu-函数"><a href="#4-2-4-Leaky-ReLU-函数" class="headerlink" title="4.2.4 Leaky ReLU 函数"></a>4.2.4 Leaky ReLU 函数</h4>$Leaky  ReLU(z)=max(αz,z)$</li></ul><p><strong>特点</strong>：</p><ul><li>解决了ReLU的“死亡神经元”问题。</li><li>在负区间给定一个很小的斜率（通常为0.01）。</li></ul><p><strong>应用</strong>：</p><ul><li>替代ReLU，在隐藏层中使用<h4 id="425-softmax-函数"><a href="#4-2-5-Softmax-函数" class="headerlink" title="4.2.5 Softmax 函数"></a>4.2.5 Softmax 函数</h4>$Softmax(z_i​)=\frac{e^z_i}{∑_j​​e^z_j} ​$</li></ul><p><strong>特点</strong>：</p><ul><li>将输出值转换为概率分布，总和为1。</li><li>适用于多分类问题。</li></ul><p><strong>应用</strong>：</p><ul><li>常用于多分类问题的输出层。<h4 id="426-swish-函数"><a href="#4-2-6-Swish-函数" class="headerlink" title="4.2.6 Swish 函数"></a>4.2.6 Swish 函数</h4>$Swish(z)=z⋅σ(z)=\frac{z}{1+e^{−z}} ​​$</li></ul><p><strong>特点</strong>：</p><ul><li>平滑的非线性函数，性能优于ReLU和Sigmoid。</li><li>由Google提出，结合了ReLU和Sigmoid的特点。</li></ul><p><strong>应用</strong>：</p><ul><li>新型激活函数，在一些深度学习模型中表现出色。</li></ul><h2 id="43-激活函数的选择"><a href="#4-3-激活函数的选择" class="headerlink" title="4.3 激活函数的选择"></a>4.3 激活函数的选择</h2><p>激活函数的选择对于神经网络的训练和性能有重要影响。以下是一些常见的选择准则：</p><ol><li><strong>隐藏层</strong>：通常使用ReLU或其变种（如Leaky ReLU、Swish），因为它们计算简单且能有效缓解梯度消失问题。</li><li><strong>输出层</strong>：<ul><li><strong>回归问题</strong>：使用线性激活函数。</li><li><strong>二分类问题</strong>：使用Sigmoid函数。</li><li><strong>多分类问题</strong>：使用Softmax函数</li></ul></li></ol><h2 id="5训练过程"><a href="#5-训练过程" class="headerlink" title="5.训练过程"></a>5.训练过程</h2><p>假设我们有一个简单的神经网络，包含一个输入层、一个隐藏层和一个输出层：</p><ol><li>输入层**：2个节点，表示输入特征 $x_1$ 和 $x_2​$。</li><li><strong>隐藏层</strong>：2个节点，表示隐藏状态 $h_1$ 和 $h_2$​，偏置向量分别为$b_1$、$b_2$,使用ReLU激活函数。</li><li><strong>输出层</strong>：1个节点，表示输出 $y$，偏置向量分别为$b_3$,使用线性激活函数。</li></ol><p><strong>权重矩阵</strong></p><ol><li><p><strong>输入层到隐藏层的权重</strong>：假设权重为 $W^{(1)} = \begin{bmatrix} W_{11} &amp; W_{12} \ W_{21} &amp; W_{22} \end{bmatrix}$,其中 $W_{11}​$、$W_{12}$​ 连接 $x_1$​ 到 $h_1$​ 和 $h_2$​，$W_{21}​$、$W_{22}$​ 连接 $x_2$​ 到 $h_1$​ 和 $h_2​$。</p></li><li><p><strong>隐藏层到输出层的权重</strong>：假设权重为 $W^{(2)} = \begin{bmatrix} W_{31} &amp; W_{32} \end{bmatrix}$</p><ul><li>$W_{31}$​ 和 $W_{32}$分别连接$h_1$ 和$h_2​$到 $y$。其中 $W_{31}$、$W_{32}$分别连接 $h_1$和 $h_2$到 $y$</li></ul></li></ol><p>如果指定具体数据，可以设置为</p><ul><li>输入数据为$\mathbf{X} = [0.5, 0.6]$</li><li><p>隐藏层权重矩阵 $\mathbf{W}^{(1)} = \begin{bmatrix} 0.1 &amp; 0.2 \ 0.3 &amp; 0.4 \end{bmatrix}$</p></li><li><p>隐藏层偏置向量 $\mathbf{b}^{(1)} = \begin{bmatrix} 0.1 \ 0.2 \end{bmatrix}$</p></li><li><p>输出层权重矩阵 $\mathbf{W}^{(2)} = \begin{bmatrix} 0.5 &amp; 0.6 \end{bmatrix}$</p></li><li>输出层偏置向量$\mathbf{b}^{(2)} = \begin{bmatrix} 0.3 \end{bmatrix}$</li></ul><p>在训练过程中，通过调整权重和偏置，使得模型的预测结果尽可能准确。以下的主要步骤.</p><h3 id="51-前向传播forward-propagation"><a href="#5-1-前向传播（Forward-Propagation）" class="headerlink" title="5.1 前向传播（Forward Propagation）"></a>5.1 前向传播（Forward Propagation）</h3><p>前向传播是数据从输入层经过隐藏层传递到输出层的过程。在这个过程中，每一层的节点接收前一层的输出，进行加权求和，并通过激活函数生成输出。<br>具体步骤如下：</p><ul><li><strong>加权求和</strong>：每个节点接收前一层所有节点的输出，计算加权和。 $z_i = \sum_{j} w_{ij} x_j + b_i$​ 其中，$z_i$ 是第 $i$ 个节点的加权和，$w_{ij}$ 是从第 $j$ 个输入到第 $i$ 个节点的权重，$x_j$是第 $j$ 个输入，$b_i$​ 是偏置。</li><li><strong>激活函数</strong>：对加权和应用激活函数，生成节点的输出。 $a_i = f(z_i)$常用的激活函数包括Sigmoid、Tanh和ReLU。<h4 id="511-输入层"><a href="#5-1-1-输入层" class="headerlink" title="5.1.1 输入层"></a>5.1.1 输入层</h4>输入层接收外部数据，将其传递给第一个隐藏层。假设输入数据为 $\mathbf{X} = [x_1, x_2]$<h4 id="512-隐藏层"><a href="#5-1-2-隐藏层" class="headerlink" title="5.1.2 隐藏层"></a>5.1.2 隐藏层</h4></li></ul><ol><li><p>计算隐藏层的输入加权和：<br>$z_1^{(1)} = w_{11}x_1 + w_{12}x_2 + b_1$</p><p>$z_2^{(1)} = w_{21}x_1 + w_{22}x_2 + b_2$</p><p>代入具体数据：$z^{(1)} = \mathbf{W}^{(1)} \mathbf{X} + \mathbf{b}^{(1)} = \begin{bmatrix} 0.1 &amp; 0.2 \ 0.3 &amp; 0.4 \end{bmatrix} \begin{bmatrix} 0.5 \ 0.6 \end{bmatrix} + \begin{bmatrix} 0.1 \ 0.2 \end{bmatrix} = \begin{bmatrix} 0.1 \cdot 0.5 + 0.2 \cdot 0.6 + 0.1 \ 0.3 \cdot 0.5 + 0.4 \cdot 0.6 + 0.2 \end{bmatrix} = \begin{bmatrix} 0.27 \ 0.62 \end{bmatrix}$</p></li><li><p>应用激活函数，计算隐藏层的输出：<br>$h_1 = \text{ReLU}(z_1^{(1)})$</p><p>$h_2 = \text{ReLU}(z_2^{(1)})$</p></li></ol><pre><code>$h^&#123;(1)&#125; = \text&#123;ReLU&#125;(z^&#123;(1)&#125;) = \begin&#123;bmatrix&#125; \max(0, 0.27) \\ \max(0, 0.62) \end&#123;bmatrix&#125; = \begin&#123;bmatrix&#125; 0.27 \\ 0.62 \end&#123;bmatrix&#125;$</code></pre><h4 id="513-输出层"><a href="#5-1-3-输出层" class="headerlink" title="5.1.3 输出层"></a>5.1.3 输出层</h4><ol><li>计算输出层的输入加权和：<br>$z^{(2)} = w_{31}h_1 + w_{32}h_2 + b_3$<br>$z^{(2)} = \mathbf{W}^{(2)} a^{(1)} + \mathbf{b}^{(2)} = \begin{bmatrix} 0.5 &amp; 0.6 \end{bmatrix} \begin{bmatrix} 0.27 \ 0.62 \end{bmatrix} + \begin{bmatrix} 0.3 \end{bmatrix} = 0.5 \cdot 0.27 + 0.6 \cdot 0.62 + 0.3 = 0.735$</li><li>应用激活函数，计算最终输出：<br>$y = z^{(2)}$<br>假设输出层使用线性激活函数（即不做非线性变换）<br>$y = z^{(2)} = 0.735$</li></ol><h3 id="52-计算损失loss-calculation"><a href="#5-2-计算损失（Loss-Calculation）" class="headerlink" title="5.2 计算损失（Loss Calculation）"></a>5.2 计算损失（Loss Calculation）</h3><p>使用损失函数计算预测输出与真实标签之间的差异。损失函数是一个衡量模型预测误差的指标，常见的损失函数包括均方误差（MSE）和交叉熵损失。</p><ul><li><p><strong>均方误差（MSE, Mean Squared Error）</strong>：用于回归问题。 $\text{MSE} = \frac{1}{N} \sum_{i=1}^N (y_i - \hat{y}_i)^2$</p></li><li><p><strong>交叉熵损失（Cross-Entropy Loss）</strong>：用于分类问题。 $\text{Cross-Entropy} = -\sum_{i=1}^N y_i \log(\hat{y}_i)$</p></li></ul><h3 id="53-反向传播backpropagation"><a href="#5-3-反向传播（Backpropagation）" class="headerlink" title="5.3 反向传播（Backpropagation）"></a>5.3 反向传播（Backpropagation）</h3><p>反向传播算法通过计算损失函数对每个模型参数（权重和偏置）的偏导数/梯度，来指导参数更新，使得损失函数逐步减小，从而提高模型的准确性。</p><ul><li><strong>计算梯度</strong>：首先计算输出层节点的损失梯度，即损失函数对输出层每个节点输出的偏导数。然后通过链式法则，依次计算每个隐藏层节点的梯度。梯度由后一层节点的梯度和当前层节点的输出值共同决定。</li><li><strong>传播误差</strong>：误差从输出层逐层传播回输入层，计算每个参数的梯度。</li></ul><p>∂ 是偏导数符号</p><h4 id="531-偏导数和梯度"><a href="#5-3-1-偏导数和梯度" class="headerlink" title="5.3.1 偏导数和梯度"></a>5.3.1 偏导数和梯度</h4><p>偏导数表示在固定其他变量的情况下，一个变量的变化率。假设 $f(x, y)$ 是一个关于 $x$ 和 $y$的函数，则 $f$ 对 $x$ 的偏导数记作 $\frac{\partial f}{\partial x}$。</p><p>在神经网络中，偏导数用于计算梯度，帮助反向传播算法更新权重。梯度是损失函数关于每个参数的导数，表示损失函数变化率。对于一个权重 $w$，梯度 $\frac{\partial L}{\partial w}$ 表示权重变化对损失函数 $L$ 的影响。<br>具体而言，梯度表示损失函数相对于每个参数的偏导数：</p><p>$\frac{\partial L}{\partial W} = \text{梯度}$</p><p>通过计算每个参数的偏导数，反向传播算法能逐步调整网络权重，使得损失函数 $L$ 最小化，提高模型的预测能力。</p><h4 id="532-偏导数推导过程"><a href="#5-3-2-偏导数推导过程" class="headerlink" title="5.3.2 偏导数推导过程"></a>5.3.2 偏导数推导过程</h4><p>如果 $f(x, y) = x^2 + y^2$，则对 $x$ 的偏导数为：</p><p>$\frac{\partial f}{\partial x} = \frac{\partial (x^2 + y^2)}{\partial x} = 2x$</p><p>根据求导法则，分开对每一项求导：<br>$\frac{\partial f}{\partial x} = \frac{\partial (x^2 + y^2)}{\partial x} =\frac{\partial}{\partial x} (x^2) + \frac{\partial}{\partial x} (y^2)$</p><p>对于 $x^2$，使用幂函数求导法则 $\frac{\partial}{\partial x} (x^n) = nx^{n-1}$：<br>$\frac{\partial}{\partial x} (x^2) = 2x^{2-1}= 2x$<br>对于 $y^2$，因为$y$ 被视为常数，对 $x$ 求导结果为 0：<br>$\frac{\partial}{\partial x} (y^2) = 0$</p><p>同理，对 $y$ 的偏导数为：</p><p>$\frac{\partial f}{\partial y} = \frac{\partial (x^2 + y^2)}{\partial y} = 2y$</p><p>现在根据以上理解逐层反向计算每个参数的梯度</p><h4 id="533-计算输出层的梯度"><a href="#5-3-3-计算输出层的梯度" class="headerlink" title="5.3.3 计算输出层的梯度"></a>5.3.3 计算输出层的梯度</h4><p>假设我们使用均方误差（MSE）作为损失函数：<br>$L = \frac{1}{2} (y - t)^2$</p><p>其中，$t$ 是目标值。</p><p>首先，计算损失相对于输出 $y$ 的梯度：</p><p>$\frac{\partial L}{\partial y} = y - t$</p><p>然后，计算损失相对于隐藏层到输出层权重 $W_{31}$​ 和 $W_{32}$的梯度：</p><p>$\frac{\partial L}{\partial W_{31}} = \frac{\partial L}{\partial y} \cdot \frac{\partial y}{\partial W_{31}}=\frac{\partial L}{\partial y} \cdot \frac{\partial (w_{31}h_1 + w_{32}h_2 + b_3)}{\partial W_{31}} = (y - t) \cdot h_1$</p><p>由于 $W_{31}$和 $h_1$​ 相乘，而 $h_1$​  不依赖于 $W_{31}$，其余项在偏导数计算中都是常数，因此可以忽略。所以</p><p>在这个表达式中，$W_{31}$和 $h_1$​ 相乘，其余项与 $W_{31}$无关，因此在对 $W_{31}​$ 求导时可以忽略。<br>$\frac{\partial L}{\partial W_{31}} = \frac{\partial L}{\partial y} \cdot \frac{\partial y}{\partial W_{31}}=\frac{\partial L}{\partial y} \cdot \frac{\partial (w_{31}h_1)}{\partial W_{31}}$</p><p>根据幂函数求导法则 $\frac{\partial}{\partial x} (x^n) = nx^{n-1}$<br>根据线性求导法则，常数项可以直接提取出来  $\frac{\partial}{\partial x} (a\cdot x) = a$</p><p>$\frac{\partial L}{\partial W_{31}} = \frac{\partial L}{\partial y} \cdot \frac{\partial y}{\partial W_{31}}=\frac{\partial L}{\partial y} \cdot \frac{\partial (w_{31}h_1)}{\partial W_{31}} = \frac{\partial L}{\partial y} \cdot \frac{\partial (w_{31}h_1)}{\partial W_{31}}= (y-t) \cdot h_1$</p><p>同理可得：<br>$\frac{\partial L}{\partial W_{32}} = \frac{\partial L}{\partial y} \cdot \frac{\partial y}{\partial W_{32}} = (y - t) \cdot h_2$</p><p>计算损失相对于偏置 $b_3​$ 的梯度：</p><p>$\frac{\partial L}{\partial b_3} = \frac{\partial L}{\partial y} \cdot \frac{\partial y}{\partial b_3} = (y - t) \cdot 1 = y - t$</p><h4 id="534-计算隐藏层的梯度"><a href="#5-3-4-计算隐藏层的梯度" class="headerlink" title="5.3.4 计算隐藏层的梯度"></a>5.3.4 计算隐藏层的梯度</h4><p>对于隐藏层的梯度，需要计算损失相对于隐藏状态 $h_1$​ 和 $h_2$​ 的梯度：</p><p>$\frac{\partial L}{\partial h_1} = \frac{\partial L}{\partial y} \cdot \frac{\partial y}{\partial h_1} = (y - t) \cdot W_{31}$</p><p>$\frac{\partial L}{\partial h_2} = \frac{\partial L}{\partial y} \cdot \frac{\partial y}{\partial h_2} = (y - t) \cdot W_{32}$</p><p>其中，$y = w_{31}h_1 + w_{32}h_2 + b_3$</p><p>计算推导过程同上，可以得到以上结果</p><p>由于隐藏层使用的是ReLU激活函数，其导数为：</p><p>$\frac{\partial \text{ReLU}(z)}{\partial z} = \begin{cases} 1 &amp; \text{if } z &gt; 0 \ 0 &amp; \text{if } z \leq 0 \end{cases}$</p><p>其中 $h_1 = \text{ReLU}(z_1^{(1)})$</p><p>因此应用链式法则后：</p><p>$\frac{\partial L}{\partial z_1} = \frac{\partial L}{\partial h_1} \cdot \frac{\partial h_1}{\partial z_1} = (y - t) \cdot W_{31} \cdot \begin{cases} 1 &amp; \text{if } z_1 &gt; 0 \ 0 &amp; \text{if } z_1 \leq 0 \end{cases}$</p><p>$\frac{\partial L}{\partial z_2} = \frac{\partial L}{\partial h_2} \cdot \frac{\partial h_2}{\partial z_2} = (y - t) \cdot W_{32} \cdot \begin{cases} 1 &amp; \text{if } z_2 &gt; 0 \ 0 &amp; \text{if } z_2 \leq 0 \end{cases}$</p><h4 id="535-计算输入层的梯度"><a href="#5-3-5-计算输入层的梯度" class="headerlink" title="5.3.5 计算输入层的梯度"></a>5.3.5 计算输入层的梯度</h4><p>最后，计算损失相对于输入层权重 $W_{11}, W_{12}, W_{21}, W_{22}$​ 的梯度：</p><p>$\frac{\partial L}{\partial W_{11}} = \frac{\partial L}{\partial z_1} \cdot \frac{\partial z_1}{\partial W_{11}} = \frac{\partial L}{\partial z_1} \cdot x_1$</p><p>$\frac{\partial L}{\partial W_{12}} = \frac{\partial L}{\partial z_2} \cdot \frac{\partial z_2}{\partial W_{12}} = \frac{\partial L}{\partial z_2} \cdot x_1$</p><p>$\frac{\partial L}{\partial W_{21}} = \frac{\partial L}{\partial z_1} \cdot \frac{\partial z_1}{\partial W_{21}} = \frac{\partial L}{\partial z_1} \cdot x_2$</p><p>$\frac{\partial L}{\partial W_{22}} = \frac{\partial L}{\partial z_2} \cdot \frac{\partial z_2}{\partial W_{22}} = \frac{\partial L}{\partial z_2} \cdot x_2$​</p><h3 id="54-更新权重weight-update"><a href="#5-4-更新权重（Weight-Update）" class="headerlink" title="5.4 更新权重（Weight Update）"></a>5.4 更新权重（Weight Update）</h3><p>使用梯度下降算法根据计算得到的梯度调整权重。梯度下降的基本公式为：<br>$W_{new​}=W_{old}​−η⋅\frac{\partial L}{\partial W}$</p><p>其中，$\eta$ 是学习率（Learning Rate），$\frac{\partial L}{\partial W}$​ 是损失函数对权重的梯度。</p><p>常见的梯度下降变种</p><ul><li><strong>批量梯度下降（Batch Gradient Descent）</strong>：在整个训练数据集上计算梯度，然后更新权重。适用于小数据集，但计算量大，效率较低。</li><li><strong>随机梯度下降（Stochastic Gradient Descent, SGD）</strong>：在每个训练样本上计算梯度，然后更新权重。计算效率高，但梯度噪声大，收敛不稳定。</li><li><strong>小批量梯度下降（Mini-Batch Gradient Descent）</strong>：在小批量训练样本上计算梯度，然后更新权重。结合了批量和随机梯度下降的优点，常用在实际训练中。</li></ul><h2 id="6-超参数hyperparameters-vs-模型参数parameters"><a href="#6-超参数（Hyperparameters）-vs-模型参数（Parameters）" class="headerlink" title="6. 超参数（Hyperparameters） vs  模型参数（Parameters）"></a>6. 超参数（Hyperparameters） vs  模型参数（Parameters）</h2><h3 id="超参数hyperparameters"><a href="#超参数（Hyperparameters）：" class="headerlink" title="超参数（Hyperparameters）："></a><strong>超参数（Hyperparameters）</strong>：</h3><ul><li>定义：超参数是指在模型训练之前需要手动设置的参数，不通过训练数据学习得到，而是通过试验、经验或自动调优方法设定。</li><li>作用：控制模型的训练过程、模型复杂度、正则化程度等。</li><li>示例：学习率（learning rate）、批大小（batch size）、隐藏层的数量和大小、正则化系数（如L2正则化中的λ）、训练轮数（epochs）等。</li><li>调整方法：手动调试（Manual Tuning）、网格搜索（Grid Search）、随机搜索（Random Search）、贝叶斯优化（Bayesian Optimization）等。</li><li>调整频率：通常在训练之前设定，在训练过程中不变。可能需要多次试验和调优过程才能确定最佳超参数。</li></ul><h3 id="模型参数model-parameters"><a href="#模型参数（Model-Parameters）：" class="headerlink" title="模型参数（Model Parameters）："></a><strong>模型参数（Model Parameters）</strong>：</h3><ul><li>定义：模型参数是指在模型训练过程中通过数据学习得到的参数，这些参数定义了模型的最终形态和行为。</li><li>作用：直接影响模型的预测输出，反映了模型从数据中学到的知识。</li><li>示例：神经网络中的权重和偏置、线性回归中的回归系数、支持向量机中的支持向量等。</li><li>调整方法：通过训练数据和优化算法（如梯度下降）自动调整。</li><li>调整频率：在每个训练迭代中都要更新，直到模型收敛或达到预设的训练轮数。</li></ul><h3 id="神经网络中的超参数和模型参数"><a href="#神经网络中的超参数和模型参数" class="headerlink" title="神经网络中的超参数和模型参数"></a>神经网络中的超参数和模型参数</h3><p><strong>超参数</strong>：</p><ul><li>学习率（Learning Rate）：决定每次权重更新的步长。</li><li>批大小（Batch Size）：决定每次权重更新时使用的训练样本数量。</li><li>隐藏层数和每层神经元数量：定义神经网络的结构和复杂度。</li><li>正则化系数：控制正则化项在损失函数中的权重，防止过拟合。</li><li>训练轮数（Epochs）：模型在整个训练数据集上完整训练的次数。</li></ul><p><strong>模型参数</strong>：</p><ul><li>权重（Weights）：连接神经元的权重，表示输入特征的重要性。</li><li>偏置（Biases）：每个神经元的偏置，用于调整激活函数的输出。</li></ul><h2 id="7-序列数据-vs-非序列数据"><a href="#7-序列数据-vs-非序列数据" class="headerlink" title="7. 序列数据 vs 非序列数据"></a>7. 序列数据 vs 非序列数据</h2><h3 id="1-序列数据sequential-data"><a href="#1-序列数据（Sequential-Data）" class="headerlink" title="1. 序列数据（Sequential Data）"></a>1. 序列数据（Sequential Data）</h3><p>序列数据（Sequential Data）是指按照时间或其他顺序排列的数据，其中每个数据点的意义和价值都依赖于它在序列中的位置和前后数据点的关系。序列数据广泛存在于许多实际应用中，如时间序列、自然语言处理、语音识别等。</p><h4 id="序列数据的特点"><a href="#序列数据的特点" class="headerlink" title="序列数据的特点"></a>序列数据的特点</h4><ol><li><strong>时间依赖性</strong>：序列数据中的每个数据点与其前后数据点存在依赖关系。这种依赖性可以是短期的（仅依赖于最近的数据点）或长期的（依赖于较早的数据点）。</li><li><strong>顺序关系</strong>：序列数据的顺序是至关重要的，数据点的顺序关系决定了其实际意义。例如，在语音信号中，音频帧的顺序决定了最终语音的内容。</li><li><strong>动态性</strong>：序列数据往往是动态变化的，数据点的值随时间或其他顺序变化而变化。</li></ol><p>递归神经网络（RNN）和其变体如LSTM和GRU擅长处理序列数据</p><h3 id="2-非序列数据"><a href="#2-非序列数据" class="headerlink" title="2. 非序列数据"></a>2. 非序列数据</h3><p>非序列化数据是指那些数据点之间没有时间或顺序依赖关系的数据。与序列化数据（如时间序列、文本、语音信号等）不同，非序列化数据中的每个数据点都是独立的，不依赖于前后的数据点。非序列化数据在各种领域中广泛存在，包括图像数据、表格数据（结构化数据）、图数据等。<br>不同类型的非序列化数据可以通过不同的神经网络进行处理，如卷积神经网络（CNN）处理图像数据，前馈神经网络（FNN）处理表格数据，图神经网络（GNN）处理图数据。在实际应用中，选择合适的神经网络模型能够有效地处理各种非序列化数据，解决实际问题。</p><h4 id="非序列化数据的实际应用"><a href="#非序列化数据的实际应用" class="headerlink" title="非序列化数据的实际应用"></a>非序列化数据的实际应用</h4><h5 id="1-图像数据的应用"><a href="#1-图像数据的应用" class="headerlink" title="1. 图像数据的应用"></a>1. 图像数据的应用</h5><p>图像数据是高维非序列数据，具有空间结构特性。卷积神经网络（CNN）是处理图像数据的主要神经网络类型。</p><p><strong>医疗影像分析</strong>：通过CNN处理医疗影像（如MRI、CT图像），进行疾病诊断和分类。</p><p><strong>自动驾驶</strong>：使用CNN分析汽车摄像头捕捉的道路图像，识别行人、交通标志和其他车辆。<br><strong>图像分类</strong>：使用CNN对输入图像进行分类。例如，ImageNet数据集上的物体识别任务。</p><ul><li>具体应用：卷积层提取图像的局部特征，池化层减少特征维度，全连接层进行分类。</li><li>典型模型：AlexNet、VGG、ResNet等。<br><strong>图像分割</strong>：将图像划分为具有不同语义意义的区域。例如，自动驾驶中的道路标记识别。</li><li>具体应用：利用全卷积神经网络（FCN）或U-Net对图像进行像素级分类。</li><li>典型模型：U-Net、SegNet等。</li></ul><h5 id="2-表格数据的应用"><a href="#2-表格数据的应用" class="headerlink" title="2. 表格数据的应用"></a>2. 表格数据的应用</h5><p>表格数据通常存储在数据库或电子表格中，包含多种特征和目标变量。前馈神经网络（FNN）适用于处理表格数据。<br><strong>回归分析</strong>：预测连续值，如房价预测。</p><ul><li>具体应用：输入层接收多种特征，隐藏层提取特征之间的复杂关系，输出层给出预测值。</li><li>典型模型：多层感知器（MLP）。<br>-<strong>分类任务</strong>：对数据进行分类，如信用卡欺诈检测。</li><li>具体应用：输入层接收各特征值，隐藏层提取特征间关系，输出层进行分类。</li><li>典型模型：多层感知器（MLP）。</li></ul><p><strong>客户分类</strong>：使用FNN对客户进行分类，如根据客户购买行为预测客户流失风险。</p><h5 id="3-图数据的应用"><a href="#3-图数据的应用" class="headerlink" title="3. 图数据的应用"></a>3. 图数据的应用</h5><p>图数据由节点和边构成，具有复杂的连接结构。图神经网络（Graph Neural Networks, GNNs）专门用于处理图数据。</p><p><strong>节点分类</strong>：在图中为每个节点分配标签，如社交网络中的用户分类。</p><ul><li>具体应用：图卷积神经网络（GCN）通过聚合邻居节点的信息更新每个节点的表示，然后进行分类。</li><li>典型模型：GCN、GraphSAGE。<br><strong>图分类</strong>：对整个图进行分类，如分子结构的化学性质预测。</li><li>具体应用：将图嵌入到固定长度的向量表示中，然后使用前馈神经网络进行分类。</li><li>典型模型：DGCNN、GraphSAGE。</li></ul><p><strong>社交网络分析</strong>：通过GNN分析社交网络中的用户关系，进行用户分类和推荐系统。</p><p><strong>化学分子建模</strong>：使用GNN分析化学分子结构，预测分子的物理和化学性质。</p><h2 id="8-向量"><a href="#8-向量" class="headerlink" title="8. 向量"></a>8. 向量</h2><p>在神经网络中，向量是一个重要的数学工具，用于表示和操作多个数值。向量在神经网络的各个部分都有广泛的应用，包括输入数据、权重、偏置、激活值等。为了更好地理解向量在神经网络中的角色，我们可以从以下几个方面进行详细阐述：</p><h4 id="向量的定义"><a href="#向量的定义" class="headerlink" title="向量的定义"></a>向量的定义</h4><p>一个向量是一个具有方向和大小的数量集合，通常用一维数组来表示。在神经网络中，向量可以用来表示输入特征、隐藏层的激活值、输出值以及模型的权重和偏置。</p><h4 id="向量在神经网络中的具体应用"><a href="#向量在神经网络中的具体应用" class="headerlink" title="向量在神经网络中的具体应用"></a>向量在神经网络中的具体应用</h4><h5 id="1-输入向量"><a href="#1-输入向量" class="headerlink" title="1. 输入向量"></a>1. 输入向量</h5><p>输入向量表示神经网络接收到的原始数据。在一个简单的前馈神经网络中，输入向量通常是一个包含多个特征的数据点。例如，对于一个图像分类任务，每个输入向量可能代表一张图像的像素值。</p><p><strong>示例</strong>： 对于一个具有三个特征的输入数据点$(x_1, x_2, x_3)$，输入向量可以表示为：$\mathbf{x} = \begin{bmatrix} x_1 \ x_2 \ x_3 \end{bmatrix}$</p><h5 id="2-权重向量"><a href="#2-权重向量" class="headerlink" title="2. 权重向量"></a>2. 权重向量</h5><p>权重向量表示神经元之间的连接强度。在神经网络中，每个神经元的输出都是前一层神经元输出的加权和。权重向量决定了输入特征对输出的影响程度。</p><p><strong>示例</strong>： 对于一个具有三个输入特征的神经元，其权重向量可以表示为： $\mathbf{w} = \begin{bmatrix} w_1 \ w_2 \ w_3 \end{bmatrix}$</p><h5 id="3-偏置向量"><a href="#3-偏置向量" class="headerlink" title="3. 偏置向量"></a>3. 偏置向量</h5><p>偏置向量是一个额外的参数，用于调整神经元的输出，使其能够更好地拟合数据。偏置向量与权重向量一起，影响每个神经元的输出。</p><p><strong>示例</strong>： 对于一个具有三个输入特征的神经元，其偏置向量可以表示为： $\mathbf{b} = b$</p><h5 id="4-激活值向量"><a href="#4-激活值向量" class="headerlink" title="4. 激活值向量"></a>4. 激活值向量</h5><p>激活值向量表示神经网络中每一层的输出。在前向传播过程中，输入向量与权重向量相乘并加上偏置向量，经过激活函数后得到的值即为激活值。</p><p><strong>示例</strong>： 对于一个具有三个神经元的隐藏层，其激活值向量可以表示为： $\mathbf{a} = \begin{bmatrix} a_1 \ a_2 \ a_3 \end{bmatrix}$</p><h4 id="向量操作"><a href="#向量操作" class="headerlink" title="向量操作"></a>向量操作</h4><p>在神经网络中，常见的向量操作包括向量加法、向量乘法（点积）、标量乘法和向量的激活函数应用。</p><h5 id="1-向量加法"><a href="#1-向量加法" class="headerlink" title="1. 向量加法"></a>1. 向量加法</h5><p>向量加法是将两个向量的对应元素相加。假设有两个向量 $\mathbf{a}$ 和 $\mathbf{b}$，它们的向量加法表示为： $\mathbf{c} = \mathbf{a} + \mathbf{b}$<br>$\mathbf{c} = \begin{bmatrix} a_1 + b_1 \ a_2 + b_2 \ a_3 + b_3 \end{bmatrix}$</p><h5 id="2-向量乘法点积"><a href="#2-向量乘法（点积）" class="headerlink" title="2. 向量乘法（点积）"></a>2. 向量乘法（点积）</h5><p>向量点积是将两个向量的对应元素相乘并求和。假设有两个向量 a\mathbf{a}a 和 $\mathbf{b}$，它们的点积表示为：<br>$c = \mathbf{a} \cdot \mathbf{b}$<br>$c = a_1 \cdot b_1 + a_2 \cdot b_2 + a_3 \cdot b_3$</p><h5 id="3-标量乘法"><a href="#3-标量乘法" class="headerlink" title="3. 标量乘法"></a>3. 标量乘法</h5><p>标量乘法是将向量的每个元素乘以一个标量。假设有一个向量 $\mathbf{a}$ 和一个标量 $k$，它们的标量乘法表示为： $\mathbf{b} = k \cdot \mathbf{a}$</p><p>$\mathbf{b} = \begin{bmatrix} k \cdot a_1 \ k \cdot a_2 \ k \cdot a_3 \end{bmatrix}$</p><h4 id="示例前向传播中的向量运算"><a href="#示例：前向传播中的向量运算" class="headerlink" title="示例：前向传播中的向量运算"></a>示例：前向传播中的向量运算</h4><p>以一个简单的两层神经网络为例，说明向量在前向传播中的应用。</p><p><strong>输入层</strong>：输入向量 $\mathbf{x}$<br>$\mathbf{x} = \begin{bmatrix} x_1 \ x_2 \end{bmatrix}$</p><p><strong>隐藏层</strong>：权重向量 $\mathbf{W}$ 和偏置向量 $\mathbf{b}$<br>$\mathbf{W} = \begin{bmatrix} w_{11} &amp; w_{12} \ w_{21} &amp; w_{22} \end{bmatrix}$</p><p>$\mathbf{b} = \begin{bmatrix} b_1 \ b_2 \end{bmatrix}$</p><p><strong>计算隐藏层激活值</strong>：<br>$\mathbf{z} = \mathbf{W} \cdot \mathbf{x} + \mathbf{b}$</p><p>$\mathbf{z} = \begin{bmatrix} w_{11}x_1 + w_{12}x_2 + b_1 \ w_{21}x_1 + w_{22}x_2 + b_2 \end{bmatrix}$</p><p><strong>应用激活函数（如ReLU）</strong>：<br>$\mathbf{a} = \text{ReLU}(\mathbf{z})$<br>$\mathbf{a} = \begin{bmatrix} \text{ReLU}(z_1) \ \text{ReLU}(z_2) \end{bmatrix}$</p><p><strong>输出层</strong>：权重向量 $\mathbf{W’}$ 和偏置 $\mathbf{b’}$<br>$\mathbf{W’} = \begin{bmatrix} w_{31} &amp; w_{32} \end{bmatrix}$<br>$\mathbf{b’} = b’$</p><p><strong>计算输出值</strong>：<br>$y = \mathbf{W’} \cdot \mathbf{a} + \mathbf{b’}$<br>$y = w_{31}a_1 + w_{32}a_2 + b’$</p><h3 id="9误差"><a href="#9-误差" class="headerlink" title="9.误差"></a>9.误差</h3><p>训练误差、测试误差和验证误差是三个不同的概念，它们分别衡量模型在不同数据集上的表现。这些误差帮助我们评估模型的拟合程度和泛化能力</p><h4 id="91-区别和联系"><a href="#9-1-区别和联系" class="headerlink" title="9.1 区别和联系"></a>9.1 区别和联系</h4><ul><li><strong>训练误差</strong>：衡量模型在训练数据上的表现，主要用于训练过程中调整模型参数。</li><li><strong>验证误差</strong>：衡量模型在验证数据上的表现，主要用于超参数调优和模型选择。验证数据是从训练数据中分离出来的一部分，不参与模型训练。</li><li><strong>测试误差</strong>：衡量模型在测试数据上的表现，主要用于评估模型的最终泛化能力。测试数据在训练和验证过程中都不使用，只有在模型训练完成后才用于评估。</li></ul><h4 id="92-训练误差training-error"><a href="#9-2-训练误差（Training-Error）" class="headerlink" title="9.2 训练误差（Training Error）"></a>9.2 训练误差（Training Error）</h4><p>训练误差是指模型在训练数据上的误差。它反映了模型对训练数据的拟合程度。<br><strong>计算方法</strong></p><p>训练误差通常通过在训练数据上计算损失函数（例如均方误差、交叉熵损失等）来得到。例如，如果使用均方误差（MSE）作为损失函数，训练误差可以表示为：</p><p>$\text{MSE}_{\text{train}} = \frac{1}{N_{\text{train}}} \sum_{i=1}^{N_{\text{train}}} (y_i - \hat{y}_i)^2$</p><p>其中，NtrainN_{\text{train}}Ntrain​ 是训练数据的样本数量，yiy_iyi​ 是第 iii 个样本的真实值，y^i\hat{y}_i y^​i​ 是模型对第 iii 个样本的预测值。</p><p><strong>目标</strong><br>最小化训练误差，以便模型能够良好地拟合训练数据。</p><p><strong>意义</strong><br>低训练误差表明模型能够很好地拟合训练数据。但这并不一定意味着模型在新数据上的表现也会良好。</p><h4 id="93-预测误差测试误差"><a href="#9-3-预测误差-测试误差" class="headerlink" title="9.3 预测误差/测试误差"></a>9.3 预测误差/测试误差</h4><p>预测误差/测试误差 是指模型在未见过的数据（通常是测试数据或验证数据）上的误差。它反映了模型的泛化能力，即模型在新数据上的表现。</p><ul><li><strong>计算方法</strong>：在模型训练完成后，使用测试数据或验证数据计算损失函数的值，计算方法与训练误差类似</li><li><strong>目标</strong>：评估模型的泛化能力，期望模型在测试数据上的误差尽可能低。</li><li><strong>意义</strong>：低预测误差表明模型具有良好的泛化能力，能够在新数据上表现良好。</li></ul><h4 id="94-误差的作用"><a href="#9-4-误差的作用" class="headerlink" title="9.4 误差的作用"></a>9.4 误差的作用</h4><p>训练误差和预测误差的关系可以帮助我们诊断模型的状态，判断模型是否过拟合或欠拟合。</p><ul><li><strong>欠拟合（Underfitting）</strong>：模型在训练数据和测试数据上的误差都很高，说明模型复杂度不足，无法捕捉数据中的规律。</li><li><strong>合适拟合（Good Fit）</strong>：模型在训练数据上的误差较低，并且在测试数据上的误差也较低，说明模型具有良好的泛化能力。</li><li><strong>过拟合（Overfitting）</strong>：模型在训练数据上的误差很低，但在测试数据上的误差很高，说明模型过于复杂，捕捉到了训练数据中的噪声和细节，泛化能力较差。</li></ul><h3 id="10过度拟合"><a href="#10-过度拟合" class="headerlink" title="10.过度拟合"></a>10.过度拟合</h3><p>过度拟合（Overfitting）是机器学习中的一个常见问题，指的是模型在训练数据上表现良好，但在未见过的测试数据或实际应用中表现不佳。这通常是因为模型过于复杂，捕捉到了训练数据中的噪声和偶然性模式，而不是数据的潜在规律。</p><h4 id="过度拟合的具体表现"><a href="#过度拟合的具体表现" class="headerlink" title="过度拟合的具体表现"></a>过度拟合的具体表现</h4><ol><li><strong>训练误差低，测试误差高</strong>：模型在训练数据上的误差很低，但在测试数据或新数据上的误差很高。</li><li><strong>高方差</strong>：模型对训练数据中的细节和噪声过于敏感，导致对不同数据集的表现差异很大。</li><li><strong>复杂模型</strong>：过于复杂的模型（例如，具有太多参数的深度神经网络）容易过度拟合。</li></ol><h4 id="过度拟合的原因"><a href="#过度拟合的原因" class="headerlink" title="过度拟合的原因"></a>过度拟合的原因</h4><ol><li><strong>模型复杂度高</strong>：模型的参数过多，能够拟合训练数据中的每一个细节和噪声。</li><li><strong>训练数据不足</strong>：训练数据量过少，模型无法学习到数据的真实分布和规律。</li><li><strong>噪声数据</strong>：训练数据中包含大量噪声，模型将这些噪声误认为是数据的潜在模式。</li><li><strong>缺乏正则化</strong>：没有使用正则化技术来约束模型的复杂度。</li></ol><h4 id="如何检测过度拟合"><a href="#如何检测过度拟合" class="headerlink" title="如何检测过度拟合"></a>如何检测过度拟合</h4><ol><li><strong>训练误差与验证误差</strong>：在训练过程中，观察训练误差和验证误差的变化。如果训练误差持续下降，而验证误差在某个点之后开始上升，这通常是过度拟合的信号。</li><li><strong>交叉验证</strong>：使用交叉验证技术评估模型在多个数据子集上的表现，避免模型对单一训练集的过度依赖。</li><li><strong>学习曲线</strong>：绘制学习曲线（训练误差和验证误差随训练样本数量变化的曲线），分析模型的学习行为。</li></ol><h4 id="解决过度拟合的方法"><a href="#解决过度拟合的方法" class="headerlink" title="解决过度拟合的方法"></a>解决过度拟合的方法</h4><ol><li><strong>增加训练数据</strong>：通过增加训练数据量，模型可以更好地学习数据的真实分布，减少对噪声的拟合。</li><li><strong>简化模型</strong>：减少模型的参数数量或选择更简单的模型，避免过度拟合。</li><li><strong>正则化</strong>：使用正则化技术（如L1和L2正则化）来约束模型参数，使其更平滑，减少对训练数据的过度拟合。<ul><li><strong>L1正则化</strong>：通过对模型参数的绝对值求和，使部分参数变为零，起到特征选择的作用。</li><li><strong>L2正则化</strong>：通过对模型参数的平方和进行约束，使参数值尽可能小，从而使模型更平滑。</li></ul></li><li><strong>Dropout</strong>：在训练过程中随机丢弃一部分神经元，防止模型对某些路径的过度依赖。</li><li><strong>数据增强</strong>：通过对训练数据进行旋转、缩放、裁剪等变换，生成更多的训练样本，增加数据的多样性。</li><li><strong>早停法（Early Stopping）</strong>：在训练过程中监控验证误差，当验证误差不再下降时，提前停止训练，防止模型过度拟合。</li></ol><h3 id="11-泛化能力"><a href="#11-泛化能力" class="headerlink" title="11. 泛化能力"></a>11. 泛化能力</h3><p>泛化能力（Generalization）是指机器学习模型在训练数据以外的数据（通常是未见过的测试数据或真实应用中的数据）上表现良好的能力。它反映了模型对数据的普遍规律的学习程度，而不是对训练数据的记忆程度。</p><p>一个具有良好泛化能力的模型能够有效地从训练数据中学习到潜在的规律，并将这些规律应用于新数据上，从而在实际应用中保持高效和准确的表现。</p><p>理解泛化能力需要考虑以下几个方面：</p><ol><li>训练误差和测试误差<br>如果模型在训练数据上的误差很低，但在测试数据上的误差很高，这通常表明模型过度拟合（Overfitting）。相反，如果模型在训练数据和测试数据上的误差都较低，这表明模型具有良好的泛化能力。</li><li>模型复杂度<ul><li><strong>简单模型</strong>：模型复杂度低，参数较少，容易欠拟合（Underfitting），即无法充分捕捉数据中的规律。</li><li><strong>复杂模型</strong>：模型复杂度高，参数较多，容易过度拟合，即捕捉了训练数据中的噪声和偶然模式。<br>一个具有良好泛化能力的模型应在简单和复杂之间取得平衡，既能捕捉数据的潜在规律，又不过度拟合噪声。</li></ul></li></ol><h3 id="12正则化"><a href="#12-正则化" class="headerlink" title="12.正则化"></a>12.正则化</h3><p>正则化（Regularization）是一种在机器学习和统计学中用于防止模型过拟合（overfitting）的技术,提高模型泛化能力的关键技术。</p><h4 id="正则化的类型"><a href="#正则化的类型" class="headerlink" title="正则化的类型"></a>正则化的类型</h4><ol><li><strong>L1 正则化（Lasso 正则化）</strong>：<ul><li><strong>定义</strong>：在损失函数中添加所有模型参数绝对值的和。</li><li><strong>数学表达</strong>： $\text{Loss} = \text{Original Loss} + \lambda \sum |w_i|$</li><li><strong>特点</strong>：可以导致一些参数完全为零，起到特征选择的作用。</li></ul></li><li><strong>L2 正则化（Ridge 正则化）</strong>：<ul><li><strong>定义</strong>：在损失函数中添加所有模型参数平方和。</li><li><strong>数学表达</strong>： $\text{Loss} = \text{Original Loss} + \lambda \sum w_i^2$</li><li><strong>特点</strong>：可以防止参数变得过大，但不会使参数完全为零。</li></ul></li><li><strong>弹性网络（Elastic Net）正则化</strong>：<ul><li><strong>定义</strong>：结合 L1 和 L2 正则化。</li><li><strong>数学表达</strong>：$\text{Loss} = \text{Original Loss} + \lambda_1 \sum |w_i| + \lambda_2 \sum w_i^2$</li><li><strong>特点</strong>：结合了 L1 和 L2 的优点，既可以选择特征又可以防止参数过大。</li></ul></li><li><strong>Dropout 正则化</strong>：<ul><li><strong>定义</strong>：在每次训练时随机丢弃一部分神经元，使得模型在训练过程中不会过于依赖某些特定的神经元。</li><li><strong>特点</strong>：通过在训练过程中引入随机性，增强模型的鲁棒性。</li></ul></li></ol><h4 id="正则化的原理"><a href="#正则化的原理" class="headerlink" title="正则化的原理"></a>正则化的原理</h4><ul><li><strong>复杂度惩罚</strong>：通过向损失函数中添加一个表示模型复杂度的项，模型在训练时不仅要最小化原始损失函数，还要考虑模型的复杂度。</li><li><strong>参数约束</strong>：限制模型参数的大小或数量，防止模型在训练数据上过度拟合。</li><li><strong>增强泛化能力</strong>：通过控制模型的复杂度，提升模型在未见数据上的表现。</li></ul><h4 id="正则化在模型中的应用"><a href="#正则化在模型中的应用" class="headerlink" title="正则化在模型中的应用"></a>正则化在模型中的应用</h4><p>正则化技术在许多机器学习模型中应用广泛，包括线性回归、逻辑回归、支持向量机（SVM）、神经网络等。在实际应用中，正则化参数（如 λ\lambdaλ）通常需要通过交叉验证等方法进行调优，以获得最佳的模型性能。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;0-什么是神经网络-neural-networks&quot;&gt;&lt;a href=&quot;#0-什么是神经网络-Neural-Networks&quot; class=&quot;headerlink&quot; title=&quot;0. 什么是神经网络 (Neural Networks)&quot;&gt;&lt;/a&gt;0. 什么是神经</summary>
      
    
    
    
    
    <category term="AI" scheme="http://example.com/tags/AI/"/>
    
    <category term="神经网络" scheme="http://example.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    <category term="Ilya sutskever‘s 30  papers" scheme="http://example.com/tags/Ilya-sutskever%E2%80%98s-30-papers/"/>
    
  </entry>
  
  <entry>
    <title>Ilya sutskever&#39;s approx 30 research papers about AI</title>
    <link href="http://example.com/17f5a37e/"/>
    <id>http://example.com/17f5a37e/</id>
    <published>2024-06-07T09:13:13.000Z</published>
    <updated>2024-06-16T14:27:01.519Z</updated>
    
    <content type="html"><![CDATA[<p>之前无意中刷到这个<a href="https://x.com/keshavchan/status/1787861946173186062?utm_source=www.mattprd.com&amp;utm_medium=referral&amp;utm_campaign=openai-cofounder-the-27-papers-to-read-to-know-90-about-ai">twitter</a>,  有点好奇这30篇paper 到底讲了啥，学完到底能知道什么，所以决定读一读。</p><img src="/17f5a37e/1.png" class><h1 id="如何读"><a href="#如何读" class="headerlink" title="如何读"></a>如何读</h1><p>作为一个算法和AI小白,  真的学会在今天90%关于AI 的内容有点超出能力范畴， 所以我的目标是读懂这些paper的文本内容，建立一个整体的大框架即可。</p><p>依然对于一个算法和AI小白来说，直接阅读paper,、会遇到大量读不懂的概念， 需要查询相关资料，我觉得如果你也是小白同时也对这些paper 感兴趣的话，我查询的资料和阅读过程对你也会有帮助，所以我会把这些内容都记录下来，供你参考。</p><h1 id="阅读记录"><a href="#阅读记录" class="headerlink" title="阅读记录"></a>阅读记录</h1><p><a href="https://sunyan.xyz/f27811be/">神经网络（Neural Networks）</a></p><p><a href="https://sunyan.xyz/6ac941eb/">递归神经网络（Recurrent Neural Networks, RNNs）</a></p><p><a href="https://sunyan.xyz/2472be8a/">The Unreasonable Effectiveness of Recurrent Neural Networks</a></p><p><a href="https://sunyan.xyz/4579c6a3/">Understanding LSTM Networks</a></p><p><a href="https://sunyan.xyz/7057a5e3/">RECURRENT NEURAL NETWORK REGULARIZATION</a>)</p><h1 id="30-research-papers"><a href="#30-research-papers" class="headerlink" title="30 research papers"></a>30 research papers</h1><p><a href="https://arc.net/folder/D0472A20-9C20-4D3F-B145-D2865C0A9FEE">https://arc.net/folder/D0472A20-9C20-4D3F-B145-D2865C0A9FEE</a></p><ol><li>The Annotated Transformer<br>简介：Transformer 模型注释版，详细解析了 Transformer 模型的内部结构和工作原理。推荐理由：理解现代 NLP 模型的基础。</li><li>The First Law of Complexodynamics<br>简介：探讨了复杂动力学的第一定律，解释了复杂系统的演变规律。推荐理由：提供了关于复杂系统的一些理论基础。</li><li>The Unreasonable Effectiveness of Recurrent Neural Networks<br>简介：讨论了 RNN 在处理序列数据时的有效性。推荐理由：帮助理解 RNN 的应用和优势。</li><li>Understanding LSTM Networks<br>简介：详细介绍了 LSTM 网络的结构和功能。推荐理由：LSTM 是 RNN 的重要变种，广泛应用于序列数据处理。</li><li>Recurrent Neural Network Regulation<br>简介：探讨了 RNN 的正则化方法。推荐理由：正则化是提高模型泛化能力的重要手段。</li><li>Keeping Neural Networks Simple by Minimizing the Description Length of the Weights<br>简介：通过最小化权重描述长度来简化神经网络。推荐理由：提供了一种简化模型的方法，提升模型的解释性。</li><li>Pointer Networks<br>简介：介绍了指针网络及其在处理离散序列问题上的应用。推荐理由：拓展了对序列模型的认识。</li><li>ImageNet Classification with Deep Convolutional Neural Networks<br>简介：深度卷积神经网络在 ImageNet 分类上的应用。推荐理由：经典论文，推动了深度学习在计算机视觉领域的革命。</li><li><p>Order Matters: Sequence to Sequence for Sets<br>简介：讨论了顺序在序列到序列模型中的重要性。推荐理由：提供了对序列模型的深刻理解。</p></li><li><p>GPipe: Easy Scaling with Micro-Batch Pipeline Parallelism<br>简介：介绍了通过微批次流水线并行实现模型扩展的方法。推荐理由：解决大规模模型训练的瓶颈问题。</p></li><li>Deep Residual Learning for Image Recognition<br>简介：深度残差学习在图像识别中的应用。推荐理由：残差网络是深度学习的一大突破。</li><li>Multi-Scale Context Aggregation by Dilated Convolutions<br>简介：通过膨胀卷积实现多尺度上下文聚合。推荐理由：在处理图像和信号时的有效方法。</li><li>Neural Message Passing for Quantum Chemistry<br>简介：神经消息传递在量子化学中的应用。推荐理由：展示了神经网络在科学计算中的潜力。</li><li>Attention Is All You Need<br>简介：Transformer 模型的奠基论文，提出了注意力机制。推荐理由：现代 NLP 模型的基石。</li><li>Neural Machine Translation By Jointly Learning To Align And Translate<br>简介：通过联合学习对齐和翻译的神经机器翻译方法。推荐理由：NMT 的重要发展。</li><li>Identity Mappings in Deep Residual Networks<br>简介：残差网络中的恒等映射。推荐理由：帮助理解深度网络的训练。</li><li>A simple neural network module for relational reasoning<br>简介：用于关系推理的简单神经网络模块。推荐理由：增强模型的推理能力。</li><li>Variational Lossy Autoencoder<br>简介：变分有损自编码器的介绍。推荐理由：提供了一种新颖的生成模型。</li><li>Relational recurrent neural networks<br>简介：关系递归神经网络。推荐理由：结合关系推理和序列建模的优势。</li><li>Quantifying the Rise and Fall of Complexity in Closed Systems: The Coffee Automaton<br>简介：定量分析封闭系统中复杂性的兴衰。推荐理由：理论性强，有助于理解复杂系统。</li><li>Neural Turing Machines<br>简介：神经图灵机的概念和应用。推荐理由：连接神经网络和计算理论的重要工作。</li><li>Deep Speech 2: End-to-End Speech Recognition in English and Mandarin<br>简介：端到端语音识别系统 Deep Speech 2 的介绍。推荐理由：语音识别领域的重要进展。</li><li>Scaling Laws for Neural Language Models<br>简介：神经语言模型的规模法则。推荐理由：帮助理解模型扩展的规律。</li><li>A Tutorial Introduction to the Minimum Description Length Principle<br>简介：最小描述长度原理的教程。推荐理由：理论基础，适用于多种模型选择问题。</li><li>Machine Super Intelligence<br>简介：机器超级智能的讨论。推荐理由：未来 AI 发展的重要参考。</li><li>Kolmogorov Complexity and Algorithmic Randomness<br>简介：Kolmogorov 复杂性和算法随机性的介绍。推荐理由：计算复杂性理论的经典。</li><li>CS231n Convolutional Neural Networks for Visual Recognition<br>简介：CS231n 课程网站，包含卷积神经网络的详细教程。推荐理由：全面的学习资源，适合入门和进阶学习。</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;之前无意中刷到这个&lt;a href=&quot;https://x.com/keshavchan/status/1787861946173186062?utm_source=www.mattprd.com&amp;amp;utm_medium=referral&amp;amp;utm_campaign</summary>
      
    
    
    
    
    <category term="AI" scheme="http://example.com/tags/AI/"/>
    
    <category term="神经网络" scheme="http://example.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    <category term="Ilya sutskever‘s 30  papers" scheme="http://example.com/tags/Ilya-sutskever%E2%80%98s-30-papers/"/>
    
  </entry>
  
  <entry>
    <title>InnoDB事务-持久性的实现,binglog &amp; redo log</title>
    <link href="http://example.com/6cb5dc64/"/>
    <id>http://example.com/6cb5dc64/</id>
    <published>2024-05-02T14:47:18.000Z</published>
    <updated>2024-05-10T15:24:50.204Z</updated>
    
    <content type="html"><![CDATA[<p>在MySQL InnoDB 这个语境下， crash safe、数据不丢失 都指的是事务的持久性特性，即事务一旦提交，应当保证所有被成功提交的数据修改都能够正确地被持久化，不丢失数据, 即使宕机也能够恢复数据</p><p>在InnoDB 中，持久性 基于binlog 和redo log 实现， 且binlog 与redo log 的写入通过2PC 协调.</p><h1 id="0-xa-事务binlog-和redo-log-的两阶段提交"><a href="#0-XA-事务：binlog-和redo-log-的两阶段提交" class="headerlink" title="0 XA 事务：binlog 和redo log 的两阶段提交"></a>0 XA 事务：binlog 和redo log 的两阶段提交</h1><img src="/6cb5dc64/1.png" class><p>在MySQL中，InnoDB存储引擎 的 redo log 和MySQL服务器层binlog 之间的一致性是通过内部的XA机制（即分布式事务）来实现的，任何一个数据出现问题都会进行会滚。</p><p><strong>XA事务</strong>是一种分布式事务。通过两阶段提交协议和XA接口标准，事务管理器和资源管理器能够可靠地协同工作，实现跨系统的事务处理，确保多个独立资源的一致性。</p><p>在binlog 和redo log 的两阶段提交， binlog 充当协调者的角色。</p><p><a href="http://localhost:4000/5b064db6/">关于XA 事务具体可在这篇文章中查看</a></p><p>binlog 和 redo log 各自写入的过程还有很多细节，接下来进行讲解</p><h1 id="1-binlog"><a href="#1-binlog" class="headerlink" title="1 binlog"></a>1 binlog</h1><p>binlog是 MySQL 服务器层使用的日志文件，记录了所有修改数据库内容的SQL语句（如 INSERT, UPDATE, DELETE）,也被称为逻辑日志。</p><p>binlog 主要用于主备复制同步、崩溃恢复等功能。</p><h2 id="11-binlog-的三种日志格式"><a href="#1-1-binlog-的三种日志格式" class="headerlink" title="1.1 binlog 的三种日志格式"></a>1.1 binlog 的三种日志格式</h2><div class="table-container"><table><thead><tr><th><strong>格式</strong></th><th><strong>定义</strong></th><th><strong>优点</strong></th><th><strong>缺点</strong></th></tr></thead><tbody><tr><td><strong>Statement-Based Logging (SBL)</strong></td><td>记录执行的 SQL 语句本身，而不是每行数据的变更。</td><td>1. <strong>空间效率高</strong>：通常占用更少的空间，因为记录的是 SQL 语句。 <br>2. <strong>易于审计</strong>：直接记录 SQL 语句，易于阅读和理解。</td><td>1. <strong>非确定性行为</strong>：可能在主从复制中导致数据不一致，特别是涉及到非确定性函数（如 NOW()、RAND()）的 SQL 语句。<br>2. <strong>复制错误</strong>：某些特定情况下可能引起从服务器的复制错误。</td></tr><tr><td><strong>Row-Based Logging (RBL)</strong></td><td>记录数据变更前后的每行数据的具体变化，而不是执行的 SQL 语句。</td><td>1. <strong>数据一致性</strong>：在复制过程中提供高度的数据一致性。<br>2. <strong>安全性更高</strong>：不记录 SQL 语句，降低了 SQL 注入的风险。</td><td>1. <strong>空间占用大</strong>：因为记录了每一行的变化，可能导致 binlog 文件迅速增大。<br>2. <strong>可读性差</strong>：不记录 SQL 语句，对于人类审计不友好。</td></tr><tr><td><strong>Mixed-Based Logging (MBL)</strong></td><td>结合了 SBL 和 RBL 的特点，根据操作的类型自动选择使用基于语句的格式或基于行的格式记录。</td><td>1. <strong>灵活性高</strong>：根据 SQL 语句的特性选择最合适的日志格式。<br>2. <strong>平衡性能和一致性</strong>：在确保数据一致性的同时考虑日志大小和性能。</td><td>1. <strong>配置复杂</strong>：需要适当配置以确保效率和准确性。<br>2. <strong>预测性差</strong>：自动切换日志格式可能使得日志的结果难以预测。</td></tr></tbody></table></div><h2 id="12-binlog写入过程"><a href="#1-2-binlog写入过程" class="headerlink" title="1.2 binlog写入过程"></a>1.2 binlog写入过程</h2><p>binlog 的写入逻辑比较简单：事务执行过程中，先把日志写到 binlog cache，事务提交的时候，再把 binlog cache 写到 binlog 文件中。</p><h3 id="121-binlog-cache"><a href="#1-2-1-binlog-cache" class="headerlink" title="1.2.1 binlog  cache"></a>1.2.1 binlog  cache</h3><p>对于每个客户端会话，MySQL 服务器为其分配一个 binlog cache。这个缓存是用来临时存储一个事务中产生的所有 binlog 事件。 但是binlog cache 刷新到磁盘时 多个线程是共写同一份 binlog 文件。</p><p>当一个新事务开始时，根据binlog 日志格式记录 每个修改SQL  语句到binlog cache 中</p><h3 id="122-page-cache-与-磁盘刷新持久化"><a href="#1-2-2-page-cache-与-磁盘刷新持久化" class="headerlink" title="1.2.2  page cache 与 磁盘刷新持久化"></a>1.2.2  page cache 与 磁盘刷新持久化</h3><p>当事务到达提交阶段时，首先将 binlog  cache 中的内容 写入到binlog 文件中，然后提交事务到 InnoDB，即 commit redo log 。</p><p>注意，这里的写入并不是直接写到到磁盘，而是先写入到文件系统的page cache, 然后通过<code>sync_binlog</code> 参数来决定 何时把数据写入到 磁盘。<br><img src="/6cb5dc64/2.png" class></p><p>磁盘刷新频率通过 <code>sync_binlog</code> 配置参数，</p><ol><li>sync_binlog=0 的时候，表示每次提交事务都不主动刷新磁盘，由文件系统自己控制刷盘频率</li><li>sync_binlog=1 的时候，表示每次提交事务都会将 binlog cache 中的内容刷新到磁盘</li><li>sync_binlog=N(N&gt;1) 的时候，表示累积 N 个提交事务后才将多个binlog cache中的内容刷新到磁盘。</li></ol><p>可以看到如果sync_binlog不设置为1 ，有有助于提高刷盘效率， 但是有丢失binlog 的风险。</p><h3 id="123-binlog-cache-不够用怎么办"><a href="#1-2-3-binlog-cache-不够用怎么办" class="headerlink" title="1.2.3  binlog cache 不够用怎么办"></a>1.2.3  binlog cache 不够用怎么办</h3><p>如果binlog cache  写满了怎么办？需要把数据暂存到磁盘</p><p>每个事务的 binlog 事件首先被写入到 binlog cache 中，这个缓存的大小由 <code>binlog_cache_size</code> 系统变量控制。</p><p>如果一个事务非常大，涉及大量的数据修改，导致binlog cache不足以存储当前事务的所有事件时，MySQL采用的处理机制是将缓存中的数据写入到磁盘上的一个临时文件中。这一过程可以分为以下几个步骤：</p><ol><li><strong>检测缓存溢出</strong>：当试图向binlog cache中写入数据，而缓存空间不足以容纳更多数据时，将触发溢出处理机制。</li><li><strong>数据写入临时文件</strong>：MySQL将当前binlog cache中的数据写入到一个临时文件中。这个临时文件通常位于MySQL的数据目录下，具有唯一标识，确保数据的隔离和安全。</li><li><strong>清空binlog cache</strong>：将数据写入临时文件后，binlog cache会被清空，为接下来的日志数据腾出空间。</li><li><strong>继续事务日志的记录</strong>：事务继续执行，新的日志事件会再次被记录到现在已经被清空的binlog cache中。</li><li><strong>事务提交</strong>：事务如果最终被提交，MySQL会将临时文件中的日志数据以及现在binlog cache中的数据一并写入到全局的binlog文件中。如果事务回滚，则临时文件和binlog cache中的数据都将被丢弃。</li></ol><h2 id="13-xid"><a href="#1-3-xid" class="headerlink" title="1.3  xid"></a>1.3  xid</h2><p>XID（Transaction Identifier） 可以理解成时MySQL server 层的事务唯一标识。</p><ul><li>MySQL服务器内部维护一个全局事务ID计数器，每个新事务都会分配一个唯一的ID。该计数器在内存中递增，保证每个事务ID在实例中是唯一的。</li><li>当一个新事务开始时，MySQL服务器层会从全局计数器中获取一个新的事务ID，将其赋予该事务，并存储在该事务的上下文中。</li></ul><h1 id="2-redo-log"><a href="#2-redo-log" class="headerlink" title="2  redo log"></a>2  redo log</h1><p>redo log是 InnoDB 存储引擎特有的日志文件，用于记录对数据库做出的更改前的数据页状态,也被称作物理日志，确保在数据库系统发生崩溃后能够恢复这些更改。<br><strong>记录内容</strong>：Redo log 记录的是数据页修改的物理操作，而非具体的 SQL 语句。</p><ul><li><strong>循环使用</strong>：Redo log 是固定大小的，通常配置为一组文件，工作在循环写入的方式。</li><li><strong>崩溃恢复</strong>：系统重启后，InnoDB 通过回放 redo log 来恢复未完成的事务，确保数据的完整性和一致性。</li><li><strong>提高性能</strong>：Redo log 允许 InnoDB 在事务提交时不必将所有数据页写回磁盘，只需确保 redo log 已被写入磁盘。</li><li>记录的是数据页的物理修改。 不论数据页是否在buffer pool 中， redo log 都要记录修改， 因为不记不能保证crash safe.</li><li>保存自增值</li></ul><h2 id="21-为什么要记录redo-log"><a href="#2-1-为什么要记录redo-log" class="headerlink" title="2.1 为什么要记录redo log"></a>2.1 为什么要记录redo log</h2><h3 id="211-buffer-pool"><a href="#2-1-1-buffer-pool" class="headerlink" title="2.1.1 buffer pool"></a>2.1.1 buffer pool</h3><p>MySQL 为了实现高性能，是不可能每次都从磁盘读数据或者把对数据的修改持久化到磁盘上的,所以 InnoDB 申请了一块连续的内存，用于存储从磁盘上读取的pages, 这个内存就是buffer pool。</p><p>buffer pool 有一块内存叫做，change buffer 用于暂存对数据的修改</p><img src="/6cb5dc64/3.png" class><p>那么在修改数据时，就会遇到两种情况</p><ol><li>数据所在的page 在buffer pool 中， 就会直接更新page</li><li>数据所在的page 不在buffer pool 中， 如果不需要加载对应page, 就会先把对数据的修改先记在change buffer 中</li></ol><p>不论是buffer pool, 还是 buffer pool 中的change buffer, 都是内存，一旦发生宕机，那就数据的修改的修改就会丢失，此时就违背了事务的持久性。</p><p>为了能把修改过的数据持久化又不影响性能，InnoDB 给出的方案是优先把修改操作记下来并持久化， 事务提交后，万一宕机丢失了buffer pool 中已修改但是未持久化的内容，就可以根据持久化的修改操作重新得到修改后数据。</p><p>这里记录下来的修改操作就是redo log,  而这种先记录修改操作，再记录修改后的技术叫做WAL。</p><h3 id="212-wal"><a href="#2-1-2-WAL" class="headerlink" title="2.1.2 WAL"></a>2.1.2 WAL</h3><p>WAL（Write-Ahead Logging）是一种在数据库系统中广泛采用的日志管理技术，用于保证数据库的事务持久性和恢复能力。</p><p>它的关键点就是先写日志，再写真正的数据。</p><p>redo log 直接应用了 WAL 技术，确保在任何数据被写入数据库页之前，相应的日志信息（如数据页的修改）先被写入到 redo log 中。</p><p>总的来说WAL 技术的优势有以下3项，</p><ol><li><strong>恢复能力</strong>：WAL 提供了强大的数据恢复能力。在发生系统故障后，可以利用日志文件中的记录来重做或撤销事务，恢复到最后一致的状态。</li><li><strong>性能优化</strong>：通过将对磁盘数据的随机写转换为<code>顺序写</code> ， 同时利用 <code>组提交</code> ，WAL 可以显著提高数据库的写性能。</li><li><strong>事务原子性和持久性</strong>：WAL 通过确保所有日志记录在实际数据写入前被提交到磁盘，从而支持数据库事务的原子性和持久性。</li></ol><h2 id="22-redo-log-记录的内容"><a href="#2-2-redo-log-记录的内容" class="headerlink" title="2.2 redo log 记录的内容"></a>2.2 redo log 记录的内容</h2><p>之所以说redo log 是物理日志， 是因为其记录了对特定数据page 数据的修改。<br>该例子来自极客专栏《MySQL 实战45讲》</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; create table t(ID int primary key, c int);</span><br><span class="line">mysql&gt; insert into t(id,k) values(id1,k1),(id2,k2);</span><br></pre></td></tr></table></figure><img src="/6cb5dc64/4.png" class><p>这条更新语句做了如下的操作（按照图中的数字顺序）：</p><ol><li>Page 1 在内存中，直接更新内存；</li><li>Page 2 没有在内存中，就在内存的 change buffer 区域，记录下“我要往 Page 2 插入一行”这个信息</li><li>将上述两个动作记入 redo log 中（图中 3 和 4）。</li></ol><p>Redo log不是记录数据页“更新之后的状态”，而是记录这个页 “做了什么改动”。</p><h2 id="23-redo-log-写入过程"><a href="#2-3-redo-log-写入过程" class="headerlink" title="2.3 redo log 写入过程"></a>2.3 redo log 写入过程</h2><p><a href="https://time.geekbang.org/column/article/76161"># 23 | MySQL是怎么保证数据不丢的？ redo log 的写入机制-redo log buffer </a></p><p>redo log 的写入机制和 binlog 类型， 需要经历</p><ol><li>MySQL 系统内存cache ， redo lo buffer</li><li>文件系统page cache</li><li>刷新持久化到磁盘<img src="/6cb5dc64/5.png" class></li></ol><h3 id="231-redo-log-buffer"><a href="#2-3-1-redo-log-buffer" class="headerlink" title="2.3.1 redo log buffer"></a>2.3.1 redo log buffer</h3><p>add(id1,k1) to page1, new change buffer item add(id2,k2) to page2 都是先写入redo log buffer 中</p><p>相比较 每个线程都拥有自己一块独立的 binlog cache ， 而 redo log buffer 是全局共用的。</p><h3 id="232-redo-log持久化到磁盘"><a href="#2-3-2-redo-log持久化到磁盘" class="headerlink" title="2.3.2 redo log持久化到磁盘"></a>2.3.2 redo log持久化到磁盘</h3><p>事务提交，执行commit redo log 后，会触发redo log buffer 中内容写入到redo log 中。</p><p>为了控制 redo log 的写入策略，InnoDB 提供了 innodb_flush_log_at_trx_commit 参数，它有三种可能取值：</p><ol><li>设置为 0 的时候，表示每次事务提交时都只是把 redo log 留在 redo log buffer 中 ;</li><li>设置为 1 的时候，表示每次事务提交时都将 redo log 直接持久化到磁盘；</li><li>设置为 2 的时候，表示每次事务提交时都只是把 redo log 写到 page cache。</li></ol><p>所以想要确保MySQL异常重启之后redo log 数据不丢失，innodb_flush_log_at_trx_commit 这个参数 建议设置成1.</p><p>前面在binlog部分说到， 在事务提交前，事务binlog 是不会被写入到真正的binlog 文件中的。 redo log 不一样，在事务提交前，redo log 有可能备持久化磁盘。有以下3种情况</p><ol><li>后台线程,每隔 1 秒，就会把 redo log buffer 中的日志，调用 write 写到文件系统的 page cache，然后调用 fsync 持久化到磁盘。，</li><li>redo log buffer 占用的空间即将达到 innodb_log_buffer_size 一半的时候，后台线程会主动写盘。注意，由于这个事务并没有提交，所以这个写盘动作只是 write，而没有调用 fsync，也就是只留在了文件系统的 page cache。</li><li>并行的事务提交的时候，顺带将这个事务的 redo log buffer 持久化到磁盘。如果 innodb_flush_log_at_trx_commit 设置的是 1，那么按照这个参数的逻辑， 要把 redo log buffer 里的日志全部持久化到磁盘。这时候，就会带上未提交事务 在 redo log buffer 里的日志一起持久化到磁盘。<h3 id="233-2pc的细化过程"><a href="#2-3-3-2PC的细化过程" class="headerlink" title="2.3.3 2PC的细化过程"></a>2.3.3 2PC的细化过程</h3></li></ol><h2 id="24-日志文件组"><a href="#2-4-日志文件组" class="headerlink" title="2.4  日志文件组"></a>2.4  日志文件组</h2><p>InnoDB 的 redo log 是以日志文件组的形式组织的。一个日志文件组通常包含两个或更多的日志文件，这些文件在物理上是连续的，并且循环使用。当一个日志文件写满后，InnoDB 会自动切换到下一个日志文件继续写入。当最后一个文件写满后，它会回到第一个文件并开始覆盖旧的日志记录，这就是所谓的“环形写入”。</p><h2 id="25-lsn"><a href="#2-5-LSN" class="headerlink" title="2.5 LSN"></a>2.5 LSN</h2><p>LSN（Log Sequence Number）,日志序列号,是一个不断增长的全局变量， 用来记录当前redo log 文件中 已经写入的日志量， 单位是字节。</p><img src="/6cb5dc64/6.png" class><p>图片中的write pos LSN 指当前已经产生的的日志量，随着更多的事务数据被写入，write pos LSN 会不断增加</p><p>checkpoint LSN 是redo log 中的一个位置，表示所有之前的日志记录都已经被应用（或说是“刷新”）到了磁盘的数据页上，因此，从这个位置以前的日志数据可以安全地被覆写， 不会出现数据丢失的情况。 redo log 会有多个检查点</p><p>write pos LSN 和 checkpoint LSN之间空着的部分，可以用来记录新的操作。</p><p>如果 write pos LSN  赶上了最一个checkpoint  LSN 位置，这意味着 redo log 的空间不足，可能会导致数据库操作停顿，因为系统需要等待足够的日志空间来记录新的事务数据。</p><h2 id="26-组提交"><a href="#2-6-组提交" class="headerlink" title="2.6  组提交"></a>2.6  组提交</h2><p>前面提过，redo log 提升性能，一个是把对磁盘的随机写转换成了顺序写，一个是组提交机制。</p><p>组提交机制（Group Commit）是一种通过合并多个事务的日志提交操作来提高I/O效率的策略。这一机制基于LSN（Log Sequence Number，日志序列号）来追踪和管理日志提交。</p><p>以下图为例解释<br><img src="/6cb5dc64/8.png" class></p><ol><li><strong>事务<code>trx1</code>开始</strong>：<ul><li><code>trx1</code>进入事务队列并被选为组的领导者，日志记录的LSN开始增加。</li></ul></li><li><strong>事务<code>trx2</code>和<code>trx3</code>加入</strong><ul><li>在<code>trx1</code>进入队列之后，<code>trx2</code>和<code>trx3</code>紧随其后进入提交队列。</li></ul></li><li><strong>LSN更新到160</strong>：<ul><li>随着<code>trx2</code>和<code>trx3</code>的日志写入缓冲区，整个组的最后一个日志序列号<code>LSN</code>变为160。</li></ul></li><li><strong>领导者<code>trx1</code>执行写盘</strong>：<ul><li><code>trx1</code>作为组的领导者，携带<code>LSN=160</code>去执行一次性日志写盘（fsync）操作。</li></ul></li><li><strong>写盘完成</strong>：<ul><li><code>trx1</code>的fsync操作完成后，所有<code>LSN &lt;= 160</code>的日志记录都被持久化到磁盘。</li></ul></li><li><strong>事务返回提交成功</strong>：<ul><li><code>trx1</code>、<code>trx2</code>和<code>trx3</code>都标记为提交成功并从提交队列中移除。</li></ul></li></ol><h1 id="3-事务执行过程中的binlog-和redolog-和undo-log"><a href="#3-事务执行过程中的binlog-和redolog-和undo-log" class="headerlink" title="3 事务执行过程中的binlog 和redolog 和undo log"></a>3 事务执行过程中的binlog 和redolog 和undo log</h1><p>下面将结合MySQL 的逻辑架构 和具体SQL , 来具体地看一下binlog 和redo log 的写入</p><img src="/6cb5dc64/9.png" class><p>SQL<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; create table T(ID int primary key, c int);</span><br><span class="line">mysql&gt; update T set c=c+1 where ID=2;</span><br></pre></td></tr></table></figure></p><p>结果MySQL 的逻辑架构， 该update sql的执行过程如下</p><ol><li>执行器先找InnoDB取 ID=2 这一行。ID 是主键，引擎直接用树搜索找到这一行。如果 ID=2 这一行所在的数据页本来就在buffer pool 中，就直接返回给执行器；否则，需要先从磁盘读入buffer pool，然后再返回。</li><li>执行器拿到引擎给的行数据，把这个值加上 1，比如原来是 N，现在就是 N+1，得到新的一行数据，再调用引擎接口写入这行新数据。</li><li>InnoDB引擎记录该行数据的undo log, 然后新数据更新到内存中，如果数据本来就在内存中，则直接修改数据页，如果不再内存中，则将修改记录在change buffer  中，同时将这个更新操作记录到 redo log 里面，此时 redo log 处于 prepare 状态。</li><li>然后告知执行器执行完成了，随时可以提交事务。执行器生成这个操作的 binlog，并把 binlog 写入磁盘。</li><li>执行器调用引擎的提交事务接口，引擎把刚刚写入的 redo log 改成提交（commit）状态，更新完成。根据 innodb_flush_log_at_trx_commit 决定redo log 是否持久化到磁盘</li><li>buffer pool 中对数据页的更新 ,等待脏页刷线操作持久化到磁盘</li></ol><p>14.prepare阶段,将事务的xid写入，将binlog_cache里的进行flush以及sync操作(大事务的话这步非常耗时)<br>15.commit阶段，由于之前该事务产生的redo log已经sync到磁盘了。所以这步只是在redo log里标记commit</p><h1 id="4-崩溃恢复的逻辑"><a href="#4-崩溃恢复的逻辑" class="headerlink" title="4 崩溃恢复的逻辑"></a>4 崩溃恢复的逻辑</h1><p>崩溃恢复过程中，InnoDB 会从最近的 checkpoint LSN开始，应用 redo log 中的更改，直到达到崩溃时的 write pos LSN，以此来恢复数据库到最后一次提交的状态。</p><p>看一下崩溃恢复时的判断规则</p><ol><li>如果 redo log 里面的事务是完整的，则直接提交；</li><li>如果 redo log 里面的事务只有完整的 prepare，则判断对应的事务 binlog 是否存在并完整：<ol><li>如果完整，则提交事务；</li><li>否则，回滚事务。==此处事务回滚基于undo log ==</li></ol></li><li>如果redo log 没有完整的prepare, 则事务基于undo log 回滚</li></ol><p>⚠️说明一下，innodb_flush_log_at_trx_commit  实际上控制了redo prepare 和commit 两个阶段的刷盘策略，比如innodb_flush_log_at_trx_commit  =1 时在 <code>prepare</code> 阶段和 <code>commit</code> 阶段，<code>redo log</code> 都会持久化写入磁盘。所以才会出现第二种磁盘有且只有完整prepare 的情况。</p><p>接下来根据一些具体的问题来详细说明崩溃恢复时的细节</p><h2 id="41-如何判断-redo-log-是完整的"><a href="#4-1-如何判断-redo-log-是完整的" class="headerlink" title="4.1  如何判断 redo log 是完整的"></a>4.1  如何判断 redo log 是完整的</h2><p>redo log commit 阶段会有commit 标识</p><h2 id="42-如果判断binlog-完整性"><a href="#4-2-如果判断binlog-完整性" class="headerlink" title="4.2. 如果判断binlog 完整性"></a>4.2. 如果判断binlog 完整性</h2><p>一个事务的 binlog 是有完整格式的：<br>statement 格式的 binlog，最后会有 COMMIT；<br>row 格式的 binlog，最后会有一个 XID event。</p><h2 id="43-redo-log-和-binlog-是怎么关联起来的"><a href="#4-3-redo-log-和-binlog-是怎么关联起来的" class="headerlink" title="4.3. redo log 和 binlog 是怎么关联起来的"></a>4.3. redo log 和 binlog 是怎么关联起来的</h2><p>在崩溃恢复时，通过读取Redo Log中的Xid，能够将其与Binlog中的Xid进行匹配。</p><p>XID（Transaction Identifier） 可以理解成时MySQL server 层的事务唯一标识。<br>redo log  中会记录XID</p><p>如果碰到只有 parepare、而没有 commit 的 redo log，就拿着 XID 去 binlog 找对应的事务。</p><h2 id="44-为什么要用2pc-协调binlog和redo-log"><a href="#4-4-为什么要用2PC-协调binlog和redo-log" class="headerlink" title="4.4. 为什么要用2PC 协调binlog和redo log"></a>4.4. 为什么要用2PC 协调binlog和redo log</h2><p>类似的问题还有，为什么处于 prepare 阶段的 redo log 加上完整 binlog 就可以提交事务。</p><p>这两个问题本质上都是数据一致性的问题。</p><p>binlog 是server 层日志， 是MySQL 一开始就有的功能，被用在了很多地方，比如备份、主备同步复制。redo log 是InnoDB 层日志，是InnoDB 为了实现事务功能新增的。使用2PC可以维护两份之间的逻辑一致。</p><p>那么，为什么要维护两份日志间的逻辑一致呢。</p><p>binlog 是server 层日志， 是MySQL 一开始就有的功能，被用在了很多地方，比如备份、主备同步复制。redo log 是InnoDB 层日志，是InnoDB 为了实现事务功能新增的。如果两份日志逻辑或者说数据不一致， 那么用日志恢复出来的数据库状态就有可能和它本来应该的状态不一致。</p><p>具体举例来讲，如果不用2PC，两种日志要么是先写 redo log 再写 binlog，或者先写binlog 再写redo log 。<br>仍然用前面的 update 语句来做例子。假设当前 ID=2 的行，字段 c 的值是 0，再假设执行 update 语句过程中在写完第一个日志后，第二个日志还没有写完期间发生了 crash，会出现什么情况呢？</p><ol><li><p>先写 redo log 后写 binlog。假设在 redo log 写完，binlog 还没有写完的时候，MySQL 进程异常重启。由于我们前面说过的，redo log 写完之后，系统即使崩溃，仍然能够把数据恢复回来，所以恢复后这一行 c 的值是 1。但是由于 binlog 没写完就 crash 了，这时候 binlog 里面就没有记录这个语句。因此，之后备份日志的时候，存起来的 binlog 里面就没有这条语句。然后你会发现，如果需要用这个 binlog 来恢复临时库的话，由于这个语句的 binlog 丢失，这个临时库就会少了这一次更新，恢复出来的这一行 c 的值就是 0，与原库的值不同。</p></li><li><p>先写 binlog 后写 redo log。如果在 binlog 写完之后 crash，由于 redo log 还没写，崩溃恢复以后这个事务无效，所以这一行 c 的值是 0。但是 binlog 里面已经记录了“把 c 从 0 改成 1”这个日志。所以，在之后用 binlog 来恢复的时候就多了一个事务出来，恢复出来的这一行 c 的值就是 1，与原库的值不同。可以看到，如果不使用“两阶段提交”，那么数据库的状态就有可能和用它的日志恢复出来的库的状态不一致。</p></li></ol><p>同理，为什么处于 prepare 阶段的 redo log 加上完整 binlog 就可以提交事务。因为如果binlog 写完以后 MySQL 发生崩溃，这时候 binlog 已经写入了，之后就会被从库（或者用这个 binlog 恢复出来的库）使用。如果redo log 事务不提交的话，就会发生数据不一致的情况</p><h2 id="45-不要binlog-可以吗"><a href="#4-5-不要binlog-可以吗" class="headerlink" title="4.5. 不要binlog 可以吗"></a>4.5. 不要binlog 可以吗</h2><p>仅从事务持久化/崩溃恢复这个功能来讲， 只要redo log  是可以完成的。<br>但是binlog 作为 MySQL 一开始就有的功能，被用在了很多地方，有redo log 无法替代的功能 。</p><ol><li>归档。redo log 是循环写，写到末尾是要回到开头继续写的。这样历史日志没法保留，redo log 也就起不到归档的作用。</li><li>主从复制同步</li><li>MySQL 高可用</li><li>在一些业务场景中， 也会使用binlog做数据同步，比如使用canal 同步binlog数据 到ES<h2 id="46-数据一定不会丢失吗-双1-设置"><a href="#4-6-数据一定不会丢失吗-双1-设置" class="headerlink" title="4.6 数据一定不会丢失吗-双1 设置"></a>4.6 数据一定不会丢失吗-双1 设置</h2></li></ol><p>在介绍binlog和redo log 写入过程的时候，有两个参数<br>sync_binlog  控制binlog 持久化到磁盘的频率</p><ol><li>sync_binlog=0 的时候，表示每次提交事务都不主动刷新磁盘，由文件系统自己控制刷盘频率</li><li>sync_binlog=1 的时候，表示每次提交事务都会将 binlog cache 中的内容刷新到磁盘</li><li>sync_binlog=N(N&gt;1) 的时候，表示累积 N 个提交事务后才将多个binlog cache中的内容刷新到磁盘。</li></ol><p>innodb_flush_log_at_trx_commit  控制redo log 持久化到磁盘的频率</p><ol><li>设置为 0 的时候，表示每次事务提交时都只是把 redo log 留在 redo log buffer 中 ;</li><li>设置为 1 的时候，表示每次事务提交时都将 redo log 直接持久化到磁盘；</li><li>设置为 2 的时候，表示每次事务提交时都只是把 redo log 写到 page cache。</li></ol><p>可以看到吗，只有在双1设置的时候，sync_binlog 和 innodb_flush_log_at_trx_commit 都设置成 1， 才能确保一定不会丢数据</p><p>通常我们说 MySQL 的“双 1”配置，指的就是 sync_binlog 和 innodb_flush_log_at_trx_commit 都设置成 1。也就是说，一个事务完整提交前，需要等待两次刷盘，一次是 redo log（prepare 阶段），一次是 binlog。</p><p>如果不设置成双1， 有助于提高性能。</p><h1 id="5-binlog-vs-redo-log"><a href="#5-binlog-vs-redo-log" class="headerlink" title="5. binlog vs redo log"></a>5. binlog vs redo log</h1><h4 id="差异"><a href="#差异" class="headerlink" title="差异"></a>差异</h4><ul><li><strong>层级差异</strong>：Binlog 工作在 MySQL 服务器层，所有引擎都可以使用；而 redo log 是 InnoDB 存储引擎层特有的。</li><li><strong>记录形式</strong>：Binlog 可以记录 SQL 语句或行变更，redo log 记录的是数据页的物理变化，即“在某个数据页上做了什么修改”</li><li><strong>目的和用途</strong>：Binlog 主要用于数据复制和崩溃恢复，而 redo log 主要用于事务的持久性和崩溃恢复。</li><li><strong>大小管理</strong>：Redo log 的大小是固定的，循环使用循环写；binlog 是追加写，可以不断增长，需要定期进行清理。</li><li><strong>日志写入</strong>：每个线程都拥有自己一块独立的 binlog cache ， 而 redo log buffer 是全局共用的</li></ul><h4 id="共同点"><a href="#共同点" class="headerlink" title="共同点"></a>共同点</h4><ul><li><strong>事务安全</strong>：两者都是为了保证事务的持久性和原子性。</li><li><strong>恢复支持</strong>：在系统或硬件故障后，两者都能被用来恢复数据。</li><li><strong>写前日志</strong>：都采用了写前日志（write-ahead logging, WAL）的技术，即在实际修改数据库内容前先记录日志。</li><li>从生产到写入磁盘均有内存page - 到page cache - 磁盘，刷新到磁盘的时机均有参数控制</li></ul><h1 id="相关文章"><a href="#相关文章" class="headerlink" title="相关文章"></a>相关文章</h1><p><a href="https://sunyan.xyz/5b064db6/">Intro to 事务</a><br><a href="https://sunyan.xyz/9cd551f5/">Intro to InnoDB 事务</a><br><a href="https://sunyan.xyz/b36b0ce9/">InnoDB事务-原子性的实现,undo log</a><br><a href="https://sunyan.xyz/9faedfe0/">InnoDB事务-隔离性的实现,MVCC &amp; 锁</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;在MySQL InnoDB 这个语境下， crash safe、数据不丢失 都指的是事务的持久性特性，即事务一旦提交，应当保证所有被成功提交的数据修改都能够正确地被持久化，不丢失数据, 即使宕机也能够恢复数据&lt;/p&gt;
&lt;p&gt;在InnoDB 中，持久性 基于binlog 和r</summary>
      
    
    
    
    
    <category term="MySQL" scheme="http://example.com/tags/MySQL/"/>
    
    <category term="事务" scheme="http://example.com/tags/%E4%BA%8B%E5%8A%A1/"/>
    
  </entry>
  
  <entry>
    <title>InnoDB事务-隔离性的实现, MVCC &amp; 锁</title>
    <link href="http://example.com/9faedfe0/"/>
    <id>http://example.com/9faedfe0/</id>
    <published>2024-05-02T14:42:57.000Z</published>
    <updated>2024-05-10T15:24:50.199Z</updated>
    
    <content type="html"><![CDATA[<p>隔离性，还有一个说法就是 数据可见性。</p><p>隔离性、数据可见性是一个在并发事务下才需要考虑的问题，并发事务可以分3种情况考虑</p><ol><li>读-读， 读操作不会对数据产生影响，所以不需要关注</li><li>读-写 or 写-读， 可能会出现脏读、不可重复读、幻读</li><li>写-写，可能会脏写的情况</li></ol><p>并发事务下的数据的一致性写问题</p><ul><li>脏写：一个事务修改了另一个未提交事务修改过的数据。</li></ul><p>并发事务下的数据的一致性读问题</p><ul><li><strong>脏读</strong>：事务读取了未提交的数据，可能造成数据不一致。</li><li><strong>不可重复读</strong>：事务在内部的多次读取中看到了同一数据的不同版本，主要由于其他事务的更新操作。</li><li><strong>幻读</strong>：事务在两次查询同一个范围时看到了不一样的行，通常是因为其他事务添加或删除了行。</li></ul><p>MySQL  的 4种 事务隔离级别</p><div class="table-container"><table><thead><tr><th>隔离级别</th><th>解决的问题</th><th>未解决的问题</th><th>原理描述</th></tr></thead><tbody><tr><td><strong>读未提交</strong></td><td>无</td><td>脏读、不可重复读、幻读</td><td>允许事务读取其他事务未提交的修改，可能导致脏读。</td></tr><tr><td><strong>读已提交</strong></td><td>脏读</td><td>不可重复读、幻读</td><td>只能看到已经被其他事务提交的数据，避免了脏读，但不能防止在同一事务中看到不一致的数据。</td></tr><tr><td><strong>可重复读</strong></td><td>脏读、不可重复读</td><td>MySQL 在该隔离级别下加上gap 锁可部分解决幻读问题</td><td>在事务开始后所有SELECT操作都看到一致的快照，避免了不可重复读，但无法防止其他事务插入新行（幻读）。</td></tr><tr><td><strong>串行化</strong></td><td>脏读、不可重复读、幻读</td><td>无</td><td>“写”会加“写锁”，“读”会加“读锁”。当出现读写锁冲突的时候，后访问的事务必须等前一个事务执行完成，才能继续执行。通过强制事务串行执行，防止了脏读、不可重复读和幻读，提供了最高级别的隔离。</td></tr></tbody></table></div><p>由于脏写导致的数据一致性问题非常严重，任何一种隔离级别下都不允许发生，对数据的修改操作必须通过锁串行执行</p><p>InnoDB 在解决 并发事务时，分成两种情况对应不同的解决方案</p><ol><li>快照读-MVCC</li><li>当前读- 锁</li></ol><h1 id="1-快照读的隔离性-mvcc"><a href="#1-快照读的隔离性-MVCC" class="headerlink" title="1 快照读的隔离性-MVCC"></a>1 快照读的隔离性-MVCC</h1><ul><li><strong>一致性视图</strong>：在快照读中，事务会创建一个一致性视图（Consistent Read View），确保当前事务读取到的都是事务开始时的数据状态。它依赖于MVCC的实现。</li><li><strong>无需锁定</strong>：快照读是一种无锁的读取，即读取数据时不需要对行记录进行锁定，因此它不会阻塞事务的读写，同时也不会被其他事务的读写操作阻塞。</li><li><strong>隔离级别影响</strong>：快照读的行为受事务隔离级别的影响，不同的隔离级别会影响读取到的版本。<ul><li>在读未提交隔离级别下，所有事务都读取最新事务；</li><li>在串行化隔离级别下，使用锁控制数据的访问。</li><li>在读已提交和可重复读隔离级别下，使用MVCC 来控制数据的可见性。</li></ul></li></ul><p>InnoDB存储引擎的MVCC（多版本并发控制）机制是基于ReadView和Undo Log共同实现的，关键是通过<code>TRX_ID</code>和<code>ROLL_PTR</code>两个行记录隐藏列来跟踪和管理每一行的修改版本。</p><h2 id="11-版本链-undo-log"><a href="#1-1-版本链-undo-log" class="headerlink" title="1.1 版本链 -undo log"></a>1.1 版本链 -undo log</h2><p>在基于undo log 实现原子性 一文中，可以看到</p><ol><li>行记录中有roll_pointer,</li><li>update 操作中TRX_UNDO_UPD_EXIST_REC 和 TRX_UNDO_DEL_MARK_REC 类型的undo log 中，也有roll_pointer,<br>通过这些roll_pointer, 可以形成一条行记录的版本链。</li></ol><p>insert 操作中，对应的undo log没有roll_pointer 属性，因为insert 操作就是一个行记录的初始版本，没有比它更早的操作了。</p><p>以下是一个通过roll_pointer 组成的版本链，每个undo log 进行了内容省略以展示链接的重点内容</p><img src="/9faedfe0/1.png" class><h2 id="12-readview"><a href="#1-2-readview" class="headerlink" title="1.2 readview"></a>1.2 readview</h2><p>有了版本链，那么该如何判断哪个版本的数据对当前事务可见呢，这里需要引入readview 概念。</p><p>Read View 主要包含以下几个关键的部分：</p><ol><li><strong>m_ids</strong>：当前系统中活跃的事务ID列表。这些事务在生成 Read View 时已经开始但尚未提交。</li><li><strong>min_trx_id</strong>：生成 Read View 时，活跃事务ID中的最小值。这是因为任何 ID 小于此值的事务在 Read View 生成前已经提交。</li><li><strong>max_trx_id</strong>：生成 Read View 时，已知的下一个事务ID。任何大于或等于此 ID 的事务在生成 Read View 后开始的。</li><li><strong>creator_trx_id</strong>：生成这个 Read View 的事务的事务ID。 一个事务只有进行修改操作时，才会被分配trx_id, 否则一个事务的trx_id 默认都是0， 所以 creator_trx_id 也有可能时0</li></ol><p><strong>运作方式</strong>：</p><ul><li>当事务执行查询操作时，它会根据自己的 Read View 来判断数据行的可见性。具体来说，每行数据都有自己的系统版本号（trx_id，即事务ID）。Read View 通过以下逻辑来确定行的可见性：<ol><li>如果行及记录的trx_id  和creator_trx_id 相等，说明当前事务在访问自己修改的数据，数据可见。</li><li>如果行的版本号小于 min_trx_id，说明行是在 Read View 生成之前被创建或最后修改的，因此对当前事务可见。</li><li>如果行的版本号大于或等于 max_trx_id，说明行是在 Read View 生成之后被创建或修改的，因此对当前事务不可见。</li><li>如果行的版本号在 min_trx_id 和 max_trx_id 之间，还需要检查这个版本号是否属于 m_ids 列表中的某个事务：<ul><li>如果属于，说明该行可能由尚未提交的事务修改，对当前事务不可见。</li><li>如果不属于，说明该行由已提交的事务修改，对当前事务可见。</li></ul></li></ol></li></ul><p>如果某个版本的数据对当前事务不可见，那就顺着版本链找洗一个版本的数据，并按照上面的步骤进行判断。如果一个数据直到最后一个版本都不可见，那就说明该条数据对当前事务完全不可见</p><h3 id="121-readview-和-读已提交read-committed"><a href="#1-2-1-readview-和-读已提交（Read-Committed）" class="headerlink" title="1.2.1  readview 和 读已提交（Read Committed）"></a>1.2.1  readview 和 读已提交（Read Committed）</h3><ul><li><strong>生成时机</strong>：在 RC 隔离级别下，Read View 不是在事务开始时生成，而是在一个事务内每次执行 SQL 查询时都会生成新的readview, 所以该事务内是可以看到其他事务已提交的对数据的修改，这在数据一致性上就表现为<code>不可重复读</code></li><li><strong>行为</strong>：每次查询都创建一个新的 Read View，包含当前时刻所有未完成的事务ID。这确保了查询只能看到那些在执行查询前已经提交的事务所做的更改。</li></ul><h3 id="122-readview-和-可重复读repeatable-read"><a href="#1-2-2-readview-和-可重复读（Repeatable-Read）" class="headerlink" title="1.2.2 readview 和 可重复读（Repeatable Read）"></a>1.2.2 readview 和 可重复读（Repeatable Read）</h3><p>InnoDB 的 默认隔离级别。</p><ul><li><strong>生成时机</strong>：在 RR 隔离级别下，Read View 是在事务的第一次查询操作开始时创建的，且在整个事务期间保持不变。这意味着整个事务中所有的查询都将看到相同的数据快照。</li><li><strong>行为</strong>：一旦生成，这个 Read View 将包含事务开始时刻的所有活跃事务ID。无论这些事务后来如何提交或回滚，当前事务的后续查询都不会感知到这些变化。</li></ul><h3 id="123-两者的对比"><a href="#1-2-3-两者的对比" class="headerlink" title="1.2.3 两者的对比"></a>1.2.3 两者的对比</h3><ul><li><strong>数据可见性</strong>：在读已提交中，事务可能看到其他事务提交的更新（即事务中的查询可能返回不同的结果），而在可重复读中，事务保证了始终对数据的一致视图。</li><li><strong>Read View 的生成频率</strong>：读已提交每次查询都重新生成 Read View，而可重复读只在事务开始时生成一次。</li><li><strong>系统开销</strong>：由于读已提交每次查询都需要生成 Read View，可能会有更高的系统开销，尤其是在查询频繁的场景中。相比之下，可重复读的开销主要集中在事务开始阶段。</li></ul><h1 id="2-当前读的隔离型-锁"><a href="#2-当前读的隔离型-锁" class="headerlink" title="2 当前读的隔离型-锁"></a>2 当前读的隔离型-锁</h1><p>当前读指的是读取数据时总是获取数据的最新版本，并通过加锁（行级别的排他锁，S锁或X锁）以确保一致性，防止其他事务修改或删除这些数据。</p><p>当前读通常用于需要修改数据的查询，如</p><ol><li>select…lock in share mode (共享读锁)</li><li>select…for update</li><li>UPDATE</li><li>DELETE</li></ol><p>关于行锁和间隙锁的具体加锁规则，和隔离级别和索引有关，大家可以参考何登成的加锁分析文章<br><a href="https://github.com/hedengcheng/tech/blob/master/database/MySQL/MySQL%20%E5%8A%A0%E9%94%81%E5%A4%84%E7%90%86%E5%88%86%E6%9E%90.pdf">MySQL 加锁分析</a></p><h1 id="3-幻读bad-case"><a href="#3-幻读bad-case" class="headerlink" title="3. 幻读bad case"></a>3. 幻读bad case</h1><p>在前面介绍隔离级别时，提到 在可重复读隔离级别下 加上 间隙锁， 可以一定程度上解决幻觉。<br>但是如果一个事务中 快照读和当前读混用，就会出现幻读bad case.</p><p><a href="https://xiaolincoding.com/mysql/transaction/phantom.html#%E5%B9%BB%E8%AF%BB%E8%A2%AB%E5%AE%8C%E5%85%A8%E8%A7%A3%E5%86%B3%E4%BA%86%E5%90%97">幻读被完全解决了吗？</a> 这篇文章中例举两个幻读 bad case,讲的比较清晰，可以参考</p><h1 id="4-当前读vs快照读"><a href="#4-当前读vs快照读" class="headerlink" title="4. 当前读vs快照读"></a>4. 当前读vs快照读</h1><ul><li><strong>快照读（Snapshot Read）</strong>：<ul><li>读取数据时使用的是某一时间点的快照，不会加锁。</li><li>使用MVCC机制，根据事务的隔离级别和版本号返回合适的行版本。</li><li>通常用于<code>SELECT</code>查询。</li></ul></li><li><strong>当前读（Current Read）</strong>：<ul><li>始终读取最新版本的行。</li><li>可能会加锁，防止其他事务修改或删除读取的数据。</li><li>通常用于修改数据的查询操作，如<code>SELECT ... FOR UPDATE</code>、<code>SELECT ... LOCK IN SHARE MODE</code>、<code>UPDATE</code>和<code>DELETE</code>。</li></ul></li></ul><h1 id="相关文章"><a href="#相关文章" class="headerlink" title="相关文章"></a>相关文章</h1><p><a href="https://sunyan.xyz/5b064db6/">Intro to 事务</a><br><a href="https://sunyan.xyz/9cd551f5/">Intro to InnoDB 事务</a><br><a href="https://sunyan.xyz/b36b0ce9/">InnoDB事务-原子性的实现,undo log</a><br><a href="https://sunyan.xyz/6cb5dc64/">InnoDB事务-持久性的实现, binglog &amp; redo log&amp;undo log</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;隔离性，还有一个说法就是 数据可见性。&lt;/p&gt;
&lt;p&gt;隔离性、数据可见性是一个在并发事务下才需要考虑的问题，并发事务可以分3种情况考虑&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;读-读， 读操作不会对数据产生影响，所以不需要关注&lt;/li&gt;
&lt;li&gt;读-写 or 写-读， 可能会出现脏读、不</summary>
      
    
    
    
    
    <category term="MySQL" scheme="http://example.com/tags/MySQL/"/>
    
    <category term="事务" scheme="http://example.com/tags/%E4%BA%8B%E5%8A%A1/"/>
    
  </entry>
  
  <entry>
    <title>InnoDB事务-原子性的实现,undo log</title>
    <link href="http://example.com/b36b0ce9/"/>
    <id>http://example.com/b36b0ce9/</id>
    <published>2024-05-02T14:22:30.000Z</published>
    <updated>2024-05-10T15:24:50.194Z</updated>
    
    <content type="html"><![CDATA[<p>原子性指的是事务要么完全成功执行，要么完全失败回滚，不允许部分执行。</p><p>这本质上是在要求具有<code>rollback 回滚能力</code>。</p><p>InnoDB中的事务可能会由用户主动触发Rollback；也可能因为遇到死锁异常Rollback；或者发生Crash，重启后对未提交的事务回滚。</p><p>InnoDB 的 rollback回滚能力 是基于 undo log 实现的。undo log 记录了修改操作前的旧版本数据，以便在回滚时恢复数据。</p><h1 id="1-一条-undo-log-的结构"><a href="#1-一条-undo-log-的结构" class="headerlink" title="1  一条 undo log  的结构"></a>1  一条 undo log  的结构</h1><h2 id="11-undo-log-的分类"><a href="#1-1-undo-log-的分类" class="headerlink" title="1.1  undo log 的分类"></a>1.1  undo log 的分类</h2><p>只有在事务中对数据进行修改（如 INSERT、DELETE、UPDATE）的时候， 才需要记录undo log，快照读 select  不需要记录。</p><p>不同的修改操作产生的 <code>undo log</code> 记录的内容和结构会有所不同，因为每种操作对数据的影响不同, 所以undo log 也会有不同的类型。</p><p>InnoDB 的 undo log 主要分为两大类：</p><ol><li><strong>TRX_UNDO_INSERT</strong>：此类主要包括 <code>TRX_UNDO_INSERT_REC</code> 类型的日志，专门用于记录插入操作的撤销信息。</li><li><strong>TRX_UNDO_UPDATE</strong>：此类包括 <code>TRX_UNDO_UPD_EXIST_REC</code> 和 <code>TRX_UNDO_DEL_MARK_REC</code>，用于记录更新存在的记录和标记删除操作的撤销信息。</li></ol><ul><li><strong>共同点</strong>：所有类型的操作都需要且只需要记录<code>足够的信息</code>来逆转所执行的操作。这些记录都存储在 InnoDB 的 undo 表空间或者系统表空间中</li><li><strong>差异</strong>：不同操作类型的 undo 日志记录的具体内容根据操作的性质而异。INSERT 主要关注标记新增行的删除，DELETE 需要记录完整的行数据以便恢复，而 UPDATE 记录修改前的字段值。</li></ul><p>⚠️：对于 undo log 的记录并不是基于每条修改 SQL 语句，而是基于 修改SQL 语句影响的每一条记录。这意味着每条被修改的记录都会有对应的 undo log 。如果一个 SQL 语句影响修改了多行数据，那么将会有多条 undo log 生成。</p><h2 id="12-insert-操作的-undo-log"><a href="#1-2-INSERT-操作的-Undo-log" class="headerlink" title="1.2  INSERT 操作的 Undo log"></a>1.2  INSERT 操作的 Undo log</h2><p>对于 INSERT 操作，undo 日志通常记录较少的信息，主要是把这条记录的主键信息记上。<br><img src="/b36b0ce9/1.png" class></p><h3 id="121-end-of-record-和-start-of-record"><a href="#1-2-1-end-of-record-和-start-of-record" class="headerlink" title="1.2.1 end of record  和 start of record"></a>1.2.1 end of record  和 start of record</h3><p>在InnoDB的undo日志结构中，<code>end of record</code>和<code>start of record</code> 两个字段共同起到链接undo日志记录的作用，使这些记录形成一个双向链表，并提供顺序遍历和反向遍历的功能</p><p><strong>end of record</strong></p><ul><li><strong>定义：</strong><ul><li><code>end of record</code>字段指示当前undo日志记录的结束位置，并提供下一条undo日志记录的起始地址。</li><li>当最后一条undo日志记录没有后继时，则下一条undo日志记录的起始地址为NULL</li></ul></li><li><strong>目的：</strong><ul><li>指向链表中的下一条记录，方便<code>顺序遍历</code>日志记录，可以用于回放或者重做日志，特别是在恢复阶段</li></ul></li></ul><p><strong>start of record</strong></p><ul><li><strong>定义：</strong><ul><li><code>start of record</code>字段指示当前undo日志记录的起始位置，并提供上一条undo日志记录的结束地址。</li><li>如果当前undo日志记录是链表中的第一条，则上一条undo日志记录的结束地址为<code>NULL</code>。</li></ul></li><li><strong>目的：</strong><ul><li>指向链表中的上一条记录，方便<code>反向遍历</code>日志记录，用于事务回滚</li></ul></li></ul><img src="/b36b0ce9/2.png" class><h3 id="122-undo-type"><a href="#1-2-2-undo-type" class="headerlink" title="1.2.2 undo type"></a>1.2.2 undo type</h3><p>该字段指定undo日志记录的类型, 用于区分不同类型的undo操作，如<code>TRX_UNDO_INSERT_REC</code> 、 <code>TRX_UNDO_UPD_EXIST_REC</code> 和 <code>TRX_UNDO_DEL_MARK_REC</code></p><h3 id="123-undo-no"><a href="#1-2-3-undo-no" class="headerlink" title="1.2.3 undo no"></a>1.2.3 undo no</h3><p>日志编号， 在一个事务内从0开始递增，每生成一条日志，undo no 就加1</p><h3 id="124-table-id"><a href="#1-2-4-table-id" class="headerlink" title="1.2.4 table id"></a>1.2.4 table id</h3><p>原始记录所在表的标识符，使undo日志能够与原始记录所在的表关联。</p><h3 id="125"><a href="#1-2-5" class="headerlink" title="1.2.5 "></a>1.2.5 <len, value></len,></h3><p>此部分以<code>&lt;长度，值&gt;</code>的形式保存每个主键列的信息，以便在回滚插入操作时恢复主键值：</p><ol><li>len：表示对应列的存储空间大小。</li><li>value：存储主键的实际值。</li></ol><h2 id="13-delete-操作的-undo-log-标记删除"><a href="#1-3-DELETE-操作的-Undo-log-标记删除" class="headerlink" title="1.3  DELETE 操作的 Undo log-标记删除"></a>1.3  DELETE 操作的 Undo log-标记删除</h2><img src="/b36b0ce9/3.png" class><ol><li>trx_id<br>记录上一个旧版本数据的trx_id,  该值从行记录的隐藏列trx_id 中获取<ul><li><strong>目的：</strong> 在回滚过程中，这个字段可以帮助恢复被删除的记录的原始事务信息，确保在恢复期间不会出现不一致的问题。</li></ul></li><li>roll_pointer<br>记录上一个旧版本数据的roll_pointer,  该值从行记录的隐藏列roll_pointer 中获取<ul><li><strong>描述：</strong> 指向被删除记录的原始回滚指针 (<code>roll_pointer</code>)。</li></ul></li></ol><p>在InnoDB中，每条行记录都有一个位（bit）标记来指示该记录的状态，包括是否已被删除。这是通过记录头（Record Header）中的<code>info bits</code>字段实现的。</p><h3 id="131-标记删除"><a href="#1-3-1-标记删除" class="headerlink" title="1.3.1 标记删除"></a>1.3.1 标记删除</h3><p>TRX_UNDO_DEL_MARK_REC 日志 指的是对记录的逻辑删除，逻辑删除指的是只被标记为删除状态，并不会立即将其物理删除，因为还要支持事务回滚，以及MVCC。</p><p>被标记删除的数据如果真的需要删除，会在适当的时候由后台线程实际清理</p><h3 id="132-行记录的删除标记"><a href="#1-3-2-行记录的删除标记" class="headerlink" title="1.3.2  行记录的删除标记"></a>1.3.2  行记录的删除标记</h3><p>每条行记录的头部都有一个<code>info bits</code>字段，用来存储记录的状态信息，包括是否已被删除。</p><p>在<code>info bits</code>字段的第5个bit位上，标记记录是否已被删除， 当此bit位为<code>1</code>时，表示该记录已被标记删除；为<code>0</code>时，表示该记录是正常的。</p><h2 id="14-update-操作的-undo-log"><a href="#1-4-UPDATE-操作的-Undo-log" class="headerlink" title="1.4 UPDATE 操作的 Undo log"></a>1.4 UPDATE 操作的 Undo log</h2><p>update 的操作 分为两种</p><ol><li><p>不更新主键的update,  这种操作 一条记录 只会TRX_UNDO_UPD_EXIST_REC 一条undo log</p></li><li><p>更新主键的update ，这种update在实际执行时， 会先删除旧记录，再insert 一条新纪录， 所以会记录两条undo log, 一条TRX_UNDO_DEL_MARK_REC， 一条TRX_UNDO_INSERT_REC</p></li></ol><img src="/b36b0ce9/4.png" class><p>具体字段信息和前面两种类似，不再详述。</p><h1 id="2一个事务中的多条undo-log如何组织在一起"><a href="#2一个事务中的多条undo-log如何组织在一起" class="headerlink" title="2一个事务中的多条undo  log如何组织在一起"></a>2一个事务中的多条undo  log如何组织在一起</h1><h2 id="21-undo-page-分类型存储undo-log"><a href="#2-1-undo-page-分类型存储undo-log" class="headerlink" title="2.1 undo page -分类型存储undo log"></a>2.1 undo page -分类型存储undo log</h2><p>InnoDB 对数据的管理是以 <code>page</code> 为单位进行的，undo log 也遵循这一原则，即存储在专门的 undo pages 中。</p><p>每个 undo page 中的日志记录是专用的，不同类型的undo log  不能混着存储， 即一个 page 中不能同时记录 <code>TRX_UNDO_INSERT</code> 类型和 <code>TRX_UNDO_UPDATE</code> 类型的日志。</p><p>这样设计的理由是为了避免在回滚时需要在同一页面上搜索不同类型的日志记录，从而提高了回滚操作的效率。</p><p>可是 一个事务内可以同时存在insert undo log和update undo log, 如果事务需要回滚则所有操作都需要回滚，那为什么还要分开存储呢？</p><ol><li><p><strong>优化事务回滚的逻辑</strong></p><ul><li><strong>操作依赖性减少</strong>：插入操作的回滚仅涉及到删除之前插入的行，而更新或删除操作的回滚需要恢复原始数据。将这些操作的日志分开，可以在回滚时减少对不同类型日志处理逻辑的依赖，使得回滚过程更加模块化和有序。</li><li><strong>执行效率</strong>：分开存储使得处理各自的回滚逻辑时可以更加高效，因为每种类型的回滚处理只需关注其对应类型的日志页。这减少了在单一日志页中搜索和处理不同类型日志的复杂性和时间。</li></ul></li><li><p><strong>并行处理</strong><br>尽管一个事务中可能存在多种类型的 undo 日志，但在并发环境中，不同的回滚任务可能由不同的系统进程或线程处理。例如，某些情况下系统可能并行地处理 <code>insert undo log</code> 和 <code>update undo log</code>。分开存储可以减少锁的竞争和管理的复杂性，提高并发处理的效率。</p></li><li><strong>空间和性能管理</strong><ul><li><strong>简化空间回收</strong>：在事务提交后，<code>insert undo log</code> 可以立即被丢弃和回收，因为插入操作生成的记录一旦提交即视为有效。而 <code>update undo log</code> 可能需要被保留以支持其他事务的一致性读（由于 MVCC）。分开存储使得空间管理更为高效，因为可以针对性地处理和回收日志空间。</li><li><strong>优化读取性能</strong>：在事务处理过程中，尤其是在一些只涉及到特定类型操作的查询或回滚操作中，分开存储可以优化日志的读取性能，因为系统可以直接定位到相关类型的日志页。</li></ul></li><li><strong>日志维护的简化</strong><ul><li>分开存储有助于简化日志维护和日志生命周期管理。系统可以更容易地追踪和管理不同类型日志的生成、使用和清理周期。</li></ul></li></ol><h2 id="22-undo-page-链表"><a href="#2-2-undo-page-链表" class="headerlink" title="2.2 undo page 链表"></a>2.2 undo page 链表</h2><p>在一个事务中，可能会产生多条 undo log。</p><p>如果一个 undo page 填满了，事务会向系统申请新的undo page,并将其通过链表（通常是使用类似于前驱（previous）和后继（next）指针的机制）连接起来。</p><p>前面提到过，一个 undo page 不能混合存储不能类型的链接， 所以对于一个事务它可以有insert undo page 和update undo page 两个链表。</p><p>每个事务都会分配单独的页面链表。</p><img src="/b36b0ce9/5.png" class><p>下面简单介绍下链表中第一个 undo page</p><ol><li><p>file header 如前面介绍，会有一个字段来标识该page 是undo page, 用来存储undo log。</p></li><li><p>undo page header 会记录该页面存储的undo log 类型， insert or update</p></li><li><p>undo log segment header , 会记录该链表所属的segment</p></li><li><p>undo log header ,理论上，每个事务都会分配自己的页面链表， 但如果一个事务产生的undo log很少，那么这个页面链表就有可能被重用。所以实际上一个页面链表中实际可能存储多个事务的undo log,  undo log header  中记录了不同事务间日志的分隔信息。</p></li></ol><h2 id="23-回滚段"><a href="#2-3-回滚段" class="headerlink" title="2.3 回滚段"></a>2.3 回滚段</h2><p>InnoDB默认创建128个回滚段（Rollback Segments），用于管理undo日志。</p><ul><li><strong>元数据存储：</strong> 每个回滚段的元数据存储在系统表空间第5号页面中。</li><li><strong>Slot结构</strong>：每个回滚段包含1024个<code>slot</code>，每个<code>slot</code>可以映射到一个Undo页。</li><li><strong>事务与回滚段的关联：</strong> 事务会在需要的时候分配一个回滚段。</li><li><strong>轮询策略：</strong> InnoDB使用轮询方式将回滚段分配给新事务，以实现负载均衡。</li></ul><h2 id="24-undo页链表的形成与维护"><a href="#2-4-Undo页链表的形成与维护" class="headerlink" title="2.4  Undo页链表的形成与维护"></a>2.4  Undo页链表的形成与维护</h2><ol><li><strong>事务开始：</strong><ul><li>新的事务开始时，会分配一个插入段和一个更新段。</li><li>在分配的回滚段头页中，初始化undo页链表的头指针和尾指针。</li></ul></li><li><strong>查找可用的Slot：</strong><ul><li>事务在开始写入undo日志时，会首先查找一个可用的<code>slot</code>，并初始化一个新的undo页链表。</li></ul></li><li><strong>分配新的Undo页：</strong><ul><li>分配新的undo页，将其添加到undo页链表的末尾。</li><li>如果这是链表的第一个undo页，回滚段头页的<code>first</code>指针和<code>last</code>指针会同时指向该页。</li></ul></li><li><strong>维护Undo页链表：</strong><ul><li>当undo页链表中的最后一个undo页已满时，分配一个新的undo页并链接到链表的末尾。</li><li>回滚段头页的<code>last</code>指针会指向新分配的undo页。</li><li>新undo页的<code>prev</code>指针指向链表的前一个undo页，形成链表结构。</li></ul></li></ol><h1 id="3-行记录如何与undo-log-关联-roll_pointer"><a href="#3-行记录如何与undo-log-关联-roll-pointer" class="headerlink" title="3 行记录如何与undo log 关联 -roll_pointer"></a>3 行记录如何与undo log 关联 -roll_pointer</h1><img src="/b36b0ce9/6.png" class><p><code>roll_pointer</code> 是存储在每个行记录中的一个指针，指向该行记录相关的最近一次undo log 记录。</p><p>注意这个undo 记录指的是具体的 undo log，而不是整个页面链表。</p><img src="/b36b0ce9/7.png" class><ul><li>当行记录被修改（包括更新、删除或作为多步操作的一部分的插入）时，InnoDB 首先会在 undo 日志中写入一条记录，这条记录包含了行修改前的数据，和行记录中的的roll_pointer,</li><li>InnoDB 更新行记录中的 <code>roll_pointer</code>，使其指向新写入的 undo 日志记录。如果这个行再次被修改，新的 undo 日志将被写入，<code>roll_pointer</code> 会更新为指向这条新的记录。新的undo 日志中会记录之前的roll_pointer</li></ul><h1 id="4-一条记录的版本链如何形成"><a href="#4-一条记录的版本链如何形成" class="headerlink" title="4. 一条记录的版本链如何形成"></a>4. 一条记录的版本链如何形成</h1><p>InnoDB 通过 roll_ptr 把每一行的历史版本串联在一起</p><ol><li>行记录中有roll_pointer,</li><li>update 操作中TRX_UNDO_UPD_EXIST_REC 和 TRX_UNDO_DEL_MARK_REC 类型的undo log 中，也有roll_pointer,<br>通过这些roll_pointer, 可以形成一条行记录的版本链。</li></ol><p>insert 操作中，对应的undo log没有roll_pointer 属性，因为insert 操作就是一个行记录的初始版本，没有比它更早的操作了。</p><p>以下是一个通过roll_pointer 组成的版本链，每个undo log 进行了内容省略以展示链接的重点内容</p><img src="/b36b0ce9/8.png" class><h1 id="5-undo-log-的持久化"><a href="#5-undo-log-的持久化" class="headerlink" title="5. undo log 的持久化"></a>5. undo log 的持久化</h1><p>undo日志刷盘时机的参数，但通过控制Redo日志、脏页刷新和Purge线程的参数，可以间接影响undo日志的刷盘策略。</p><p><strong>WAL技术</strong><br>在数据实际修改前，先将undo日志持久化到磁盘。</p><p><strong>刷盘时机：</strong></p><ul><li><strong>事务提交：</strong> 当事务提交时，相关的undo日志会被写入磁盘。</li><li><strong>脏页刷盘：</strong> 在InnoDB将脏页（dirty page）写入磁盘之前，首先会确保所有相关的undo日志已经被持久化。</li><li><strong>Redo日志同步：</strong> 当一个Redo日志被同步到磁盘时，所有相关的undo日志也必须被同步。</li></ul><h1 id="6-基于undo-log-的回滚操作"><a href="#6-基于undo-log-的回滚操作" class="headerlink" title="6 基于undo log 的回滚操作"></a>6 基于undo log 的回滚操作</h1><p>InnoDB中的事务</p><ol><li>可能会由用户主动触发Rollback；</li><li>也可能因为遇到死锁异常Rollback；</li><li>或者发生Crash，重启后对未提交的事务回滚。<h2 id="61-用户应用程序主动回滚"><a href="#6-1-用户-应用程序主动回滚" class="headerlink" title="6.1. 用户/应用程序主动回滚"></a>6.1. 用户/应用程序主动回滚</h2></li></ol><ul><li>反向遍历（start of record）当前事务的undo日志链表，按逆序恢复每个更改。</li><li><strong>插入操作：</strong> 在数据页中删除已插入的记录。</li><li><strong>删除操作：</strong> 恢复已删除的记录。</li><li><strong>更新操作：</strong> 恢复更新前的记录。</li><li>每个操作恢复完成后，从undo日志链表中移除相应的undo日志记录。</li></ul><h2 id="62-死锁异常回滚"><a href="#6-2-死锁异常回滚" class="headerlink" title="6.2. 死锁异常回滚"></a>6.2. 死锁异常回滚</h2><p>InnoDB通过死锁检测算法发现两个或多个事务之间的锁等待，形成死锁，,选择最小代价，即持有锁资源最少的事务务进行回滚。</p><p>与主动回滚类似，遍历当前事务的undo日志链表，按逆序恢复每个更改。</p><h2 id="63-崩溃恢复"><a href="#6-3-崩溃恢复" class="headerlink" title="6.3  崩溃恢复"></a>6.3  崩溃恢复</h2><p>MySQL服务器或操作系统崩溃后，InnoDB通过Undo日志与Redo日志结合，确保崩溃时数据页的状态恢复到一致的状态， undo日志用来 回滚未提交的事务。</p><h1 id="7-undo-log-的清理"><a href="#7-undo-log-的清理" class="headerlink" title="7 undo log 的清理"></a>7 undo log 的清理</h1><p>事务提交后，相关的Undo日志记录仍需保留一段时间以支持多版本并发控制（MVCC）</p><h2 id="71-purge-线程"><a href="#7-1-Purge-线程" class="headerlink" title="7.1 Purge 线程"></a>7.1 Purge 线程</h2><p>InnoDB 通过一个后台线程称为 Purge，来清理不再需要的 undo log。</p><ul><li><strong>触发条件</strong>：Purge 进程会定期检查那些已提交事务的 undo log。它会确定这些 undo log 是否还被其他活跃事务作为 MVCC 的一部分所需。</li><li><strong>删除操作</strong>：如果一个 undo log 记录不再被任何事务所需要，Purge 进程会将其从 undo 表空间中删除，释放相关资源。</li></ul><p>undo log 的清理机制是区分操作类型的。</p><h2 id="72-insert-undo-log"><a href="#7-2-Insert-Undo-Log" class="headerlink" title="7.2 Insert Undo Log"></a>7.2 Insert Undo Log</h2><p>Insert undo log 主要记录插入操作的信息。因为插入操作仅仅添加新的记录，不涉及已存在数据的修改，所以这种类型的 undo log 主要用于在事务失败时撤销插入操作。</p><ul><li><strong>清理时机</strong>：当一个事务进行插入操作并成功提交后，相应的 insert undo log 立即变得无用，因为插入的数据已经被确认并不需要再被撤销。此时，这些 undo log 可以被安全地清理掉，因为它们不再被任何事务所需。</li><li><strong>清理过程</strong>：Purge 线程会检测到这些 insert undo log 与已提交的事务关联，并将它们标记为可清理。然后，这些 log 会从 undo 表空间中删除，相关的磁盘空间得以回收。</li></ul><h2 id="73-update-undo-log"><a href="#7-3-Update-Undo-Log" class="headerlink" title="7.3 Update Undo Log"></a>7.3 Update Undo Log</h2><p>Update undo log 记录了对现有数据的修改（包括更新和删除操作）。这些记录对于事务回滚和多版本并发控制（MVCC）至关重要。</p><ul><li><strong>清理时机</strong>：与 insert undo log 不同，即使相关事务已经提交，update undo log 也不能立即被清理。这是因为在 InnoDB 中实现 MVCC 时，其他并发事务可能需要访问这些 log 中的旧数据版本来维持一致性读。</li><li><strong>清理过程</strong>：Purge 线程会周期性地检查 update undo log。只有当这些 log 记录不再被任何其他活跃事务所需时（即没有更早的读视图需要这些数据），它们才会被标记为可清理。然后，Purge 操作会逐步从 undo 表空间中删除这些记录。</li></ul><h2 id="74-长事务对undo-log-清理的影响"><a href="#7-4-长事务对undo-log-清理的影响" class="headerlink" title="7.4 长事务对undo log 清理的影响"></a>7.4 长事务对undo log 清理的影响</h2><p>长事务意味着系统里面会存在很老的事务视图。由于这些事务随时可能访问数据库里面的任何数据，所以这个事务提交之前，数据库里面它可能用到的回滚记录都必须保留，这就会导致大量占用存储空间。</p><h1 id="相关文章"><a href="#相关文章" class="headerlink" title="相关文章"></a>相关文章</h1><p><a href="https://sunyan.xyz/5b064db6/">Intro to 事务</a><br><a href="https://sunyan.xyz/9cd551f5/">Intro to InnoDB 事务</a><br><a href="https://sunyan.xyz/9faedfe0/">InnoDB事务-隔离性的实现,MVCC &amp; 锁</a><br><a href="https://sunyan.xyz/6cb5dc64/">InnoDB事务-持久性的实现,binglog &amp; redo log&amp;undo log</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;原子性指的是事务要么完全成功执行，要么完全失败回滚，不允许部分执行。&lt;/p&gt;
&lt;p&gt;这本质上是在要求具有&lt;code&gt;rollback 回滚能力&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;InnoDB中的事务可能会由用户主动触发Rollback；也可能因为遇到死锁异常Rollback；或</summary>
      
    
    
    
    
    <category term="MySQL" scheme="http://example.com/tags/MySQL/"/>
    
    <category term="事务" scheme="http://example.com/tags/%E4%BA%8B%E5%8A%A1/"/>
    
  </entry>
  
  <entry>
    <title>Intro to InnoDB事务</title>
    <link href="http://example.com/9cd551f5/"/>
    <id>http://example.com/9cd551f5/</id>
    <published>2024-05-02T14:15:42.000Z</published>
    <updated>2024-05-10T15:17:42.616Z</updated>
    
    <content type="html"><![CDATA[<p>在<a href="https://sunyan.xyz/5b064db6/">Intro to 事务中</a>介绍过， 一致性是事务的核心特征，或者说最终目的，<code>原子性、隔离性和持久性都是实现一致性的手段</code>。</p><p>所以在介绍InnoDB 事务时，主要介绍AID 特性的实现<br><a href="https://sunyan.xyz/b36b0ce9/">InnoDB事务-原子性的实现， undo log</a><br><a href="https://sunyan.xyz/9faedfe0/">InnoDB事务-隔离性的实现, MVCC &amp; 锁</a><br><a href="https://sunyan.xyz/6cb5dc64/">InnoDB事务-持久性的实现， binglog &amp; redo log&amp;undo log</a></p><p>在具体看InnoDB 事务实现AID 特性之前，可以先看以下这些前置知识</p><h1 id="1-innodb-数据管理"><a href="#1-InnoDB-数据管理" class="headerlink" title="1. InnoDB 数据管理"></a>1. InnoDB 数据管理</h1><h2 id="11-page"><a href="#1-1-Page" class="headerlink" title="1.1 Page"></a>1.1 Page</h2><p>page 是 InnoDB 存储数据的基本单位，也是数据在磁盘和内存之间交换的最小单位。每个页通常的大小为 16KB<br>针对不同的数据有不同的Page类型进行存储，如index page 索引页， undo page  等</p><ol><li>File Header 中 有fil_page_type 来标识该页的类型</li><li>File Trailer 用来校验页面数据是否完成 <img src="/9cd551f5/1.png" class></li></ol><h2 id="12-区extent"><a href="#1-2-区（extent）" class="headerlink" title="1.2 区（extent）"></a>1.2 区（extent）</h2><p>为了更好地管理page, InnoDB引入了区的概念， 连续的64个page  是一个区，大小默认是1MB。 可以认为extent 是一个物理上概念</p><p>一个区（Extent）是由连续的页组成的数据块，每个区包含 64 个连续的页，因此每个区的大小为 1MB （16KB * 64）。使用区的目的是为了优化磁盘空间的分配和管理，通过批量处理连续的页,减少随机IO来提高数据存取效率。</p><h2 id="13-段-segment"><a href="#1-3-段-（segment-）" class="headerlink" title="1.3 段 （segment ）"></a>1.3 段 （segment ）</h2><p>InnoDB 中的段（Segment）作为一个逻辑结构，起着将数据库的高层逻辑结构（如表和索引）与低层物理存储结构（如页和区）连接起来的桥梁作用。</p><p>以下是几个详细的例子，通过这些例子可以更好地理解段是如何在数据库管理系统中发挥作用的。</p><h3 id="131-数据表段"><a href="#1-3-1-数据表段" class="headerlink" title="1.3.1 数据表段"></a>1.3.1 数据表段</h3><p>假设您在数据库中创建了一个新表，这个表将需要存储数据行。InnoDB 会为这个表创建一个数据段：</p><ul><li><strong>逻辑层面</strong>：在逻辑层面，这个数据段代表了表中所有数据行的集合。</li><li><strong>物理层面</strong>：物理上，这个数据段开始时可能只包含几个区，每个区由 64 个连续的页组成。随着表中数据的增加，段可以动态地分配更多的区来存储更多的数据页。</li><li><strong>操作</strong>：当你执行 INSERT 操作向表中添加数据时，InnoDB 将在这个数据段中找到适当的页来存储新的行。如果必要的页不存在或页已满，段管理逻辑将请求分配新的区，并继续数据插入。</li></ul><h3 id="132-索引段"><a href="#1-3-2-索引段" class="headerlink" title="1.3.2 索引段"></a>1.3.2 索引段</h3><p>当你为表创建一个索引时，无论是主键索引还是辅助索引，InnoDB 都会为每个索引创建一个单独的索引段：</p><ul><li><strong>逻辑层面</strong>：索引段逻辑上表示索引的结构，这包括维护键值和指向表中对应行的指针。</li><li><strong>物理层面</strong>：物理上，索引段存储索引树（B-tree）的结构，其中每个节点（或页）包含索引键和指向行的指针。随着索引的增长，可能需要更多的页和区来扩展索引树。</li><li><strong>操作</strong>：进行查询优化时，如执行基于索引的查找，InnoDB 通过索引段快速访问相关页，有效地定位到数据行。</li></ul><h3 id="133-undo-日志段"><a href="#1-3-3-Undo-日志段" class="headerlink" title="1.3.3 Undo 日志段"></a>1.3.3 Undo 日志段</h3><p>Undo 日志也是使用段来管理的，每当数据被修改时，修改前的数据将存储在 undo 日志段中：</p><ul><li><strong>逻辑层面</strong>：逻辑上，undo 日志段保存了数据修改前的状态，支持事务的回滚操作。</li><li><strong>物理层面</strong>：物理上，undo 日志段由一系列的页组成，这些页按需分配，并在事务回滚时提供必要的历史数据。</li><li><strong>操作</strong>：如果事务失败或执行 ROLLBACK 命令，InnoDB 通过访问 undo 日志段中的记录来恢复数据到其原始状态。</li></ul><h2 id="14-表空间tablespace"><a href="#1-4-表空间（Tablespace）" class="headerlink" title="1.4 表空间（Tablespace）"></a>1.4 表空间（Tablespace）</h2><p>表空间是 InnoDB 数据存储的最高层级，它可以包含多个段。表空间是磁盘上的物理文件，可以看作是一个容器，内部组织着数据库的数据和索引。InnoDB 默认有一个主表空间，即 <code>ibdata</code> 文件，它包含了系统数据、数据字典、undo 日志等。此外，InnoDB 还支持每个表使用单独的文件作为独立表空间（file-per-table），这有助于数据库的扩展和管理。</p><h1 id="2行记录格式"><a href="#2-行记录格式" class="headerlink" title="2.行记录格式"></a>2.行记录格式</h1><p>数据表中的行存放在 数据page  中， 以compact 行格式为例， 每一条数据记录的存储格式如下，<br>    <img src="/9cd551f5/2.png" class></p><p>其中真实数据部分，除了数据表中定义的列之外，InnoDB 会默认为每条记录添加隐藏列</p><div class="table-container"><table><thead><tr><th>列名</th><th>是否必须</th><th>占据空间</th><th>描述</th></tr></thead><tbody><tr><td>row_id</td><td>否</td><td>6 字节</td><td>行ID，唯一标识一条记录</td></tr><tr><td>trx_id</td><td>是</td><td>6 字节</td><td>事务ID</td></tr><tr><td>roll_pointer</td><td>是</td><td>7 字节</td><td>回滚指针</td></tr></tbody></table></div><img src="/9cd551f5/3.png" class><p><code>roll_pointer</code> 是存储在每个行记录中的一个指针，指向该行记录相关的最近一次undo log 记录。<br><code>trx_id</code> 是 InnoDB 存储引擎内部用来唯一标识每个事务的标识符，它记录了最近修改该记录的事务。</p><h1 id="3-innodb-事务trx_id"><a href="#3-InnoDB-事务trx-id" class="headerlink" title="3. InnoDB 事务trx_id"></a>3. InnoDB 事务trx_id</h1><p><code>trx_id</code> 是 InnoDB 存储引擎内部用来唯一标识每个事务的标识符。这个事务ID是一个递增的数字，由 InnoDB 内部自动生成和管理。</p><p>trx_id 存储在行记录的隐藏列中。</p><p>MySQLserver 层也有一个事务唯一标识叫XID。</p><p>InnoDB 内部维护了一个 max_trx_id 全局变量，每次需要申请一个新的 trx_id 时，就获得 max_trx_id 的当前值，然后并将 max_trx_id 加 1。</p><p>功能和作用</p><ol><li><strong>事务的唯一标识</strong>：<code>trx_id</code> 为 InnoDB 提供了一种方式来唯一地识别和跟踪每个活动的或已完成的事务。</li><li><strong>多版本并发控制（MVCC）</strong>：在 InnoDB 的 MVCC 实现中，<code>trx_id</code> 被用来标记每条记录的版本，以此来支持事务的隔离级别。不同事务看到的数据视图依赖于记录的 <code>trx_id</code> 与事务的 <code>trx_id</code> 比较。</li><li><strong>回滚和恢复</strong>：在事务处理过程中，如果需要回滚，InnoDB 通过 <code>trx_id</code> 来确定哪些更改需要被撤销。此外，在系统崩溃后的恢复过程中，<code>trx_id</code> 也被用来重建活跃事务的状态。</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;在&lt;a href=&quot;https://sunyan.xyz/5b064db6/&quot;&gt;Intro to 事务中&lt;/a&gt;介绍过， 一致性是事务的核心特征，或者说最终目的，&lt;code&gt;原子性、隔离性和持久性都是实现一致性的手段&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;所以在介绍InnoDB 事</summary>
      
    
    
    
    
    <category term="MySQL" scheme="http://example.com/tags/MySQL/"/>
    
    <category term="事务" scheme="http://example.com/tags/%E4%BA%8B%E5%8A%A1/"/>
    
  </entry>
  
  <entry>
    <title>Intro to 事务</title>
    <link href="http://example.com/5b064db6/"/>
    <id>http://example.com/5b064db6/</id>
    <published>2024-05-01T13:57:16.000Z</published>
    <updated>2024-05-08T14:05:03.875Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-什么是事务"><a href="#1-什么是事务" class="headerlink" title="1. 什么是事务"></a>1. 什么是事务</h1><p>事务（Transaction）的概念起源于数据库领域，最早由美国计算机科学家 E. F. Codd 在其关于关系数据库（Relational Database）的论文中提出。</p><p>他提出了 ACID（原子性、一致性、隔离性和持久性）属性，这些属性成为事务的核心特征。</p><p>在今天的软件开发中，事务的概念已不仅仅应用于数据库领域，还拓展到了业务开发的各个领域，包括但不限于数据库、缓存、消息队列等。</p><h2 id="11-acid-特性"><a href="#1-1-ACID-特性" class="headerlink" title="1.1 ACID 特性"></a>1.1 ACID 特性</h2><ul><li>原子性(Atomicity):  保证事务中的所有操作要么全部完成，要么全部不发生，有助于处理系统错误或故障时的数据恢复，确保事务执行的完整性。</li><li>一致性(Consistency)：系统从一个正确态转移到另一个正确态，由应用通过 AID 来保证，可以说是事务的核心特性</li><li>隔离性(Isolation): 处理并发事务带来的各种问题，确保每个事务看到的是一致的数据视图，防止交叉事务的干扰。</li><li>持久性(Durability): 确保事务一旦提交，其结果就就会被持久化，这保证了数据的稳定性和可靠性。</li></ul><p>定义本身不再赘述，这里重点强调一点：一致性是事务的核心特征，或者说最终目的。<code>原子性、隔离性和持久性都是实现一致性的手段</code>，因此这 4 个特性并不是并列关系。</p><h1 id="2-事务的分类"><a href="#2-事务的分类" class="headerlink" title="2. 事务的分类"></a>2. 事务的分类</h1><p>下面将把事务按照服务和数据源数量进行分类，这种分类有助于理解事务管理的复杂性以及在不同场景下的设计和实现。</p><h2 id="21-本地事务-单服务单数据源事务"><a href="#2-1-本地事务-单服务单数据源事务" class="headerlink" title="2.1  本地事务-单服务单数据源事务"></a>2.1  本地事务-单服务单数据源事务</h2><p>在实际业务开发中，单个服务操作单个数据源的事务被归类为本地事务。这种事务类型是最简单的，因为它直接依赖于数据库本身的事务能力来完成，应用无需进行额外操作</p><p><strong>示例：</strong><br>库存服务：当用户下单时，库存服务负责检查和更新商品库存。这个服务可能只与一个库存数据库交互，进行减库存的操作。如果库存足够，事务提交，否则回滚。这个操作只涉及库存数据库，因此是一个典型的本地事务。</p><h2 id="2-2-分布式事务"><a href="#2-2-分布式事务" class="headerlink" title="2. 2 分布式事务"></a>2. 2 分布式事务</h2><p>分布式事务可以从跨多个数据源的事务和跨多个服务的事务两个角度理解。它既可以是多个数据库实例之间的分布式事务，也可以是跨不同中间件的业务层面分布式事务。</p><h3 id="221-单服务多数据源"><a href="#2-2-1-单服务多数据源" class="headerlink" title="2.2.1 单服务多数据源"></a>2.2.1 单服务多数据源</h3><p>这种情况通常发生在单个应用或服务需要同时操作多个数据库或存储系统。</p><p>例如，一个电子商务应用可能需要在处理订单的同时，在一个数据库中更新库存信息，在另一个数据库中更新用户账户信息。这要求事务管理机制能够跨越这些数据库，确保所有数据库操作要么全部成功，要么全部失败，以保证数据的一致性</p><p>在这种场景下，可以使用如XA协议这样的分布式事务协议，通过2PC等机制来协调和管理跨多个数据源的事务。</p><h3 id="222-多服务多数据源"><a href="#2-2-2-多服务多数据源" class="headerlink" title="2.2.2 多服务多数据源"></a>2.2.2 多服务多数据源</h3><p>随着微服务架构的发展，单个业务操作往往需要多个微服务协作完成，而这些服务可能各自使用独立的数据库。例如，在电商下单过程中，订单服务、库存服务、账务服务、物流服务和优惠服务需要协同处理同一业务请求，并进行交互和数据更新。</p><p>在这种场景下，分布式事务的管理比单个服务场景更为复杂，因为它不仅涉及数据一致性，还涉及网络调用的可靠性和服务间的协调。这类分布式事务通常可以通过可靠消息队列、TCC 和 SAGA 等模式来实现。</p><h2 id="23-共享事务-多服务单数据源"><a href="#2-3-共享事务-多服务单数据源" class="headerlink" title="2.3 共享事务-多服务单数据源"></a>2.3 共享事务-多服务单数据源</h2><p>在微服务架构下，通常不允许多服务共享同一数据源。理想的微服务架构是每个微服务都有其专属数据库（即服务与数据源一一对应），这种设计被称为数据库隔离。</p><p>因此，本文及本系列不会涉及该类型事务。</p><h1 id="3-两种分布式事务的区别"><a href="#3-两种分布式事务的区别" class="headerlink" title="3. 两种分布式事务的区别"></a>3. 两种分布式事务的区别</h1><p>在事务分类中，<code>单服务多数据源</code> 和 <code>多服务多数据源</code> 都被归类为分布式事务，那么这两种分布式事务有什么区别呢？</p><p>首先，单服务多数据源事务是多个数据库实例之间的分布式事务， 也被称为全局事务。当它被称为分布式事务时，这里的“分布式”是相对于数据源而言的，并不涉及服务。</p><p>而多服务多数据源事务是跨不同中间件的业务层面分布式事务。</p><p>这两种分布式事务的一个重要区别在于一致性的实现方式不同：</p><ul><li>单服务多数据源事务通常可以追求 <strong>强一致性</strong>。</li><li>多服务多数据源事务由于其复杂性和分布式特性，通常只能追求 <strong>最终一致性</strong>。</li></ul><p>下面将详细解释这两种情况及其原因。</p><h2 id="31-单服务多数据源-与-强一致性"><a href="#3-1-单服务多数据源-与-强一致性" class="headerlink" title="3.1 单服务多数据源 与 强一致性"></a>3.1 单服务多数据源 与 强一致性</h2><p>在单服务多数据源的场景中，尽管涉及多个数据源，但所有操作都由一个单一服务控制。这种配置允许使用两阶段提交（2PC）等传统的分布式事务协议来确保强一致性，即在任何时刻，所有数据源都能反映出相同的事务状态。</p><p><strong>为什么可以实现强一致性：</strong></p><ul><li><strong>集中式协调</strong>：单个服务可以作为事务的中央协调者，管理所有数据源的事务提交或回滚。</li><li><strong>锁定资源</strong>：事务处理过程中可以在各个数据源上锁定必要的资源，直到事务完成，确保事务的原子性和一致性。</li><li><strong>同步更新</strong>：所有数据源的更新操作可以同步进行，确保在事务提交时，所有的变更都能一次性反映出来。</li></ul><h2 id="32-多服务多数据源-与-最终一致性"><a href="#3-2-多服务多数据源-与-最终一致性" class="headerlink" title="3.2 多服务多数据源 与 最终一致性"></a>3.2 多服务多数据源 与 最终一致性</h2><p>多服务多数据源事务涉及多个独立的服务，每个服务可能管理自己的数据源。在这种架构下，实现强一致性变得非常复杂和成本高昂，因此通常采用最终一致性模型。</p><p><strong>为什么通常只能实现最终一致性：</strong></p><ul><li><strong>服务自治</strong>：每个服务都是自治的，独立管理自己的数据源，它们之间的通信可能是异步的，不能立即反映其他服务的状态变更。</li><li><strong>复杂的协调机制</strong>：需要跨服务协调复杂的事务可能涉及网络延迟和服务间通信失败，使得同步更新所有数据源变得不切实际。</li><li><strong>使用补偿事务</strong>：多服务事务常采用如SAGA等模式，通过一系列的本地事务和补偿事务来处理业务流程，每个事务独立提交，仅通过补偿机制来撤销错误操作，逐步达到数据的一致性。</li></ul><h1 id="4-强一致性-vs-最终一致性"><a href="#4-强一致性-vs-最终一致性" class="headerlink" title="4.  强一致性 vs 最终一致性"></a>4.  强一致性 vs 最终一致性</h1><h2 id="41-一致性的分类"><a href="#4-1-一致性的分类" class="headerlink" title="4.1 一致性的分类"></a>4.1 一致性的分类</h2><h3 id="411-强一致性strong-consistency"><a href="#4-1-1-强一致性（Strong-Consistency）" class="headerlink" title="4.1.1 强一致性（Strong Consistency）"></a>4.1.1 强一致性（Strong Consistency）</h3><p><strong>强一致性</strong>意味着系统在更新数据后，任何随后的访问都将立即看到这一更新。在强一致性模型中，所有节点上的数据在任何时间点都是一致的。这通常要求在数据更新过程中进行严格的协调，确保所有副本在继续操作前都同步更新。</p><p><strong>优点：</strong></p><ul><li>数据一致性和用户体验最为理想。</li><li>易于理解和使用，因为它模拟了单个系统的行为。</li></ul><p><strong>缺点：</strong></p><ul><li>可能严重影响系统的可用性和性能，尤其在网络延迟较高的情况下。</li><li>在 CAP 定理中，通常需要在遇到网络分区时牺牲可用性。<h3 id="412-线性一致性linearizability"><a href="#4-1-2-线性一致性（Linearizability）" class="headerlink" title="4.1.2 线性一致性（Linearizability）"></a>4.1.2 线性一致性（Linearizability）</h3></li></ul><p>线性一致性是强一致性的一个特例，它不仅保证所有节点看到相同的数据，还要求系统表现得就像所有操作都是顺序发生的。这意味着如果操作A在操作B之前完成，那么系统中的所有节点都应该首先看到A的结果，然后是B的结果。<br><strong>优点</strong>：</p><ul><li>提供了强一致性的最高标准，适用于需要严格数据顺序的应用。</li><li>简化了系统的编程模型。<br><strong>缺点</strong>：</li><li>对系统性能和可用性的影响比一般的强一致性还要大。</li></ul><h3 id="413-弱一致性weak-consistency"><a href="#4-1-3-弱一致性（Weak-Consistency）" class="headerlink" title="4.1.3 弱一致性（Weak Consistency）"></a>4.1.3 弱一致性（Weak Consistency）</h3><p>弱一致性不保证在数据更新后立即反映这一变化。在更新操作和其影响被所有用户观察到之间，存在一个不确定的时间窗口。这种模型通常用于对实时一致性要求不高的系统。<br><strong>优点</strong>：</p><ul><li>提高了系统的可用性和性能。</li><li>在处理高并发操作时更加有效。<br><strong>缺点</strong>：</li><li>用户可能会读到旧数据。</li><li>应用逻辑可能需要处理数据不一致的问题。<h3 id="414-最终一致性eventual-consistency"><a href="#4-1-4-最终一致性（Eventual-Consistency）" class="headerlink" title="4.1.4  最终一致性（Eventual Consistency）"></a>4.1.4  最终一致性（Eventual Consistency）</h3></li></ul><p>最终一致性保证，在没有新的更新的情况下，所有的数据副本最终将会是一致的。系统不保证达到一致状态的具体时间。<br><strong>优点</strong>：</p><ul><li>高度可用和可扩展。</li><li>适用于分布广泛的系统，可以容忍数据在短时间内的不一致。<br><strong>缺点</strong>：</li><li>应用需要能够处理数据一段时间内的不一致。</li><li>开发者需要设计有效的数据同步和冲突解决策略。<h2 id="42-cap-与-acid的微妙平衡-分布式系统只能追求最终一致性"><a href="#4-2-CAP-与-ACID的微妙平衡-分布式系统只能追求最终一致性" class="headerlink" title="4.2  CAP 与 ACID的微妙平衡-分布式系统只能追求最终一致性"></a>4.2  CAP 与 ACID的微妙平衡-分布式系统只能追求最终一致性</h2></li></ul><p>根据 CAP 定理，一个分布式系统不可能同时满足一致性（Consistency）、可用性（Availability）和分区容忍性（Partition Tolerance）三个属性，最多只能满足其中两个，必须牺牲一个。</p><ul><li><strong>一致性（Consistency）</strong>：在任何时刻，任何分布式节点中看到的数据都保持一致。</li><li><strong>可用性（Availability）</strong>：系统能够不间断地提供服务的能力。</li><li><strong>分区容忍性（Partition Tolerance）</strong>：在分布式环境中，当部分节点因网络原因失联（即形成“网络分区”）时，系统仍能正确提供服务的能力。</li></ul><h2 id="421-为什么说-分布式系统-必须接受-分区容忍性"><a href="#4-2-1-为什么说-分布式系统-必须接受-分区容忍性" class="headerlink" title="4.2.1 为什么说 分布式系统 必须接受 分区容忍性"></a>4.2.1 为什么说 分布式系统 必须接受 分区容忍性</h2><p>理解为什么分区容忍性在分布式环境下必然存在，需要从分布式系统的基本构成和网络通信的不可靠性两个角度探讨。</p><ol><li><p><strong>分布式系统的基本构成</strong> 分布式系统由多个相互协作的独立组件组成，这些组件可能位于物理上分散的不同位置。该架构的主要优势是提高系统的可扩展性、容错性和资源利用率。然而，这也意味着系统的各个部分必须通过网络通信。</p></li><li><p><strong>网络通信的不可靠性</strong> 网络本身存在不可靠性，可能因多种原因导致通信失败：</p><ol><li><strong>网络故障</strong>：网络设备或连接可能出现故障，如路由器故障、连接断开等。</li><li><strong>网络延迟</strong>：消息在传输过程中可能遭遇不可预测的延迟。</li><li><strong>带宽限制</strong>：网络的带宽限制可能导致数据包延迟到达或丢失。</li><li><strong>网络安全</strong>：网络攻击（如分布式拒绝服务攻击，DDoS）可能导致网络部分或完全不可用。</li></ol></li></ol><p>如果一个系统设计选择不接受网络分区，那么一旦网络分区发生，系统将无法正常工作，这在大多数业务场景中是不可接受的。</p><p>因此，在分布式系统中，分区容忍性（Partition Tolerance）是必然存在的特性。</p><p>基于分区容忍性必须满足的现状以及 CAP 理论，系统只能在一致性和可用性之间做出选择。通常，系统会选择高可用性，强一致性因此被牺牲，系统只能追求最终一致性。</p><h1 id="5-理解分布式事务中的各种协议"><a href="#5-理解分布式事务中的各种协议" class="headerlink" title="5. 理解分布式事务中的各种协议"></a>5. 理解分布式事务中的各种协议</h1><h2 id="51-dtp-模型和-xa-规范"><a href="#5-1-DTP-模型和-XA-规范" class="headerlink" title="5.1 DTP 模型和 XA 规范"></a>5.1 DTP 模型和 XA 规范</h2><h3 id="511-dtp-模型"><a href="#5-1-1-DTP-模型" class="headerlink" title="5.1.1  DTP 模型"></a>5.1.1  DTP 模型</h3><p>DTP（Distributed Transaction Processing，分布式事务处理）模型是由 X/Open（后来的 Open Group）提出的一种分布式事务处理架构模型。它定义了一套标准，使得不同厂商的分布式事务处理系统能够互操作。</p><p>在标准的 DTP 模型中，定义了以下四个主要组件：</p><ol><li><p><strong>Application Program（AP，应用程序）：</strong></p><ul><li>发起分布式事务的主体，由最终用户或开发者编写。</li><li>通过调用事务管理器的接口（例如 TX 接口）开始、提交或回滚事务。</li><li>应用程序与事务管理器和资源管理器交互。</li></ul></li><li><strong>Transaction Manager（TM，事务管理器）：</strong><ul><li>负责管理分布式事务的开始、提交和回滚等操作。</li><li>维护事务的状态，并使用两阶段提交协议（2PC）协调所有参与的资源管理器。</li><li>提供对外的 TX 接口供应用程序使用，并通过 XA 接口与资源管理器交互。</li></ul></li><li><strong>Resource Manager（RM，资源管理器）：</strong><ul><li>负责管理和控制对特定资源的访问，例如数据库管理系统（DBMS）、文件系统、消息队列等。</li><li>接收事务管理器的请求以进行资源操作，并确保数据一致性。</li><li>实现 XA 接口与事务管理器通信。</li></ul></li><li><strong>Communication Resource Manager（CRM，通信资源管理器）：</strong><ul><li>可选组件，负责管理与外部系统的通信资源。</li><li>在分布式事务中协调和同步事务状态，确保跨系统的事务一致性。</li><li>管理跨网络的事务传播，确保分布式环境中的事务处理一致性。</li></ul></li></ol><p><strong>主要接口：</strong></p><ol><li><strong>TX 接口：</strong><ul><li>应用程序 AP 与事务管理器 TM 之间的桥梁，负责事务的开始、提交和回滚等操作。</li><li>例如，在 Java EE 中，TX 接口通常对应 <code>javax.transaction.UserTransaction</code>。</li></ul></li><li><strong>XA 接口：</strong><ul><li>事务管理器 TM 与资源管理器 RM 之间的接口，协调资源管理器在两阶段提交协议中的操作。</li><li>常见的 XA 接口方法包括 <code>xa_open</code>、<code>xa_start</code>、<code>xa_end</code>、<code>xa_prepare</code>、<code>xa_commit</code>、<code>xa_rollback</code> 等。</li></ul></li><li><strong>CRM 接口：</strong><ul><li>事务管理器与通信资源管理器之间的接口，确保分布式事务在网络通信中保持一致性。</li><li>没有明确的标准接口，由各系统厂商自行实现。</li></ul></li></ol><h3 id="512-xa规范"><a href="#5-1-2-XA规范" class="headerlink" title="5.1.2 XA规范"></a>5.1.2 XA规范</h3><p>XA 规范是 X/Open 组织在 DTP（Distributed Transaction Processing）模型中定义的，用于描述事务管理器（TM）和资源管理器（RM）之间交互的接口标准。</p><ol><li><p><strong>接口标准：</strong><br>XA 规范定义了一套标准接口，包括 <code>xa_start</code>、<code>xa_end</code>、<code>xa_prepare</code>、<code>xa_commit</code>、<code>xa_rollback</code> 等。</p></li><li><p><strong>2PC 协议：</strong><br>XA 接口实现了两阶段提交协议（2PC），以确保分布式事务的一致性和完整性。</p></li></ol><h3 id="513-xa-事务"><a href="#5-1-3-XA-事务" class="headerlink" title="5.1.3 XA 事务"></a>5.1.3 XA 事务</h3><p><strong>XA事务</strong>是一种分布式事务。通过两阶段提交协议和XA接口标准，事务管理器和资源管理器能够可靠地协同工作，实现跨系统的事务处理，确保多个独立资源的一致性。</p><p><strong>实际应用</strong></p><ol><li>数据库系统：<ul><li>大多数主流数据库系统都支持XA事务，如Oracle、MySQL、DB2、SQL Server等。</li><li>通过实现XA接口，数据库可以参与分布式事务并与事务管理器协同工作。</li></ul></li><li>消息中间件：<ul><li>一些消息队列和消息中间件也支持XA事务，如IBM MQ、ActiveMQ等。</li><li>能够确保消息发送与其他资源操作的一致性。</li></ul></li><li>Java EE环境：<ul><li>在Java EE应用程序中，<code>javax.transaction.UserTransaction</code>和<code>javax.transaction.TransactionManager</code>接口提供了对XA事务的支持。<h2 id="52-两阶段提交2pc"><a href="#5-2-两阶段提交（2PC）" class="headerlink" title="5.2 两阶段提交（2PC）"></a>5.2 两阶段提交（2PC）</h2></li></ul></li></ol><p>两阶段提交是一种具体的事务协议，用于在分布式系统中协调多个事务参与者的行为，以确保事务的原子性。它包含以下两个阶段：</p><ul><li><strong>准备阶段</strong>：协调者询问所有参与者，是否准备好提交事务。</li><li><strong>提交/回滚阶段</strong>：基于各参与者的答复和超时情况，协调者决定是否全局提交或回滚，<ul><li>只有全部参与者回答了prepared 才会commit;</li><li>若有一个参与者回答和non-prepared 或者超时未回答，则rollback</li></ul></li></ul><img src="/5b064db6/1.png" class><h3 id="521-协调者宕机单点问题参与者阻塞"><a href="#5-2-1-协调者宕机：单点问题，参与者阻塞" class="headerlink" title="5.2.1 协调者宕机：单点问题，参与者阻塞"></a>5.2.1 协调者宕机：单点问题，参与者阻塞</h3><p>在2PC中，一个重要特点是参与者缺乏超时机制。因此，在第一阶段结束后，他们必须原地等待协调者的第二阶段指令。一旦协调者宕机，所有参与者都会受到影响。如果协调者长时间未恢复或未发送正常的提交或回滚指令，所有参与者都将被阻塞。</p><p>为何参与者缺乏超时处理机制呢？因为这可能引发数据一致性问题。当参与者迟迟未收到提交或回滚指令时，无论其默认为提交还是回滚，都可能导致全局数据不一致。</p><p>这也给了我们业务开发一些启示：在任何不确定情况下，都不应随意指定默认操作，最佳做法是启动警报，让人工介入处理。</p><h3 id="522-回滚性能差"><a href="#5-2-2-回滚性能差" class="headerlink" title="5.2.2  回滚性能差"></a>5.2.2  回滚性能差</h3><p>所有的操作都已经完成，回滚需要全部推翻。</p><h3 id="523-一致性问题"><a href="#5-2-3-一致性问题" class="headerlink" title="5.2.3 一致性问题"></a>5.2.3 一致性问题</h3><h4 id="5241-协调者宕机"><a href="#5-2-4-1-协调者宕机" class="headerlink" title="5.2.4.1 协调者宕机"></a>5.2.4.1 协调者宕机</h4><p>如上面单点问题中描述，协调者宕机后，由于参与者没有超时处理机制，会一直阻塞等待，直到协调者宕机恢复后， 根据持久化的数据判断该事务状态，进而发送commit 或者 rollback ， 所以在协调者宕机恢复前 协调者和参与者的数据是不一致的</p><h4 id="5232-参与者宕机"><a href="#5-2-3-2-参与者宕机" class="headerlink" title="5.2.3.2 参与者宕机"></a>5.2.3.2 参与者宕机</h4><p>如果参与者收到commit后，宕机了。此时数据也是不一致的<br>参与者宕机恢复后，可以检查自己的持久化信息，来判断事务的状态。</p><h4 id="5233-网络问题"><a href="#5-2-3-3-网络问题" class="headerlink" title="5.2.3.3 网络问题"></a>5.2.3.3 网络问题</h4><p>有的参与者收到了commit,有的参与者收不到；<br>参与者的ack 消息，协调者有的收到了，有的没收到。<br>其中参与者收不到第二阶段的消息，自然不会有ack, 表现上也是协调者收不到ack。<br>这里的解决方案就是 协调者超时处理机制-重试，在重试成功之前，数据是不一致的。</p><h3 id="524-梳理下-dtp-xa-2pc-之间的关系"><a href="#5-2-4-梳理下-DTP、XA、2PC-之间的关系" class="headerlink" title="5.2.4 梳理下 DTP、XA、2PC 之间的关系"></a>5.2.4 梳理下 DTP、XA、2PC 之间的关系</h3><p>DTP（Distributed Transaction Processing，分布式事务处理）模型是由X/Open（后来的Open Group）提出的一种分布式事务处理的体系结构模型。它定义了一套标准，使得不同厂商的分布式事务处理系统能够互操作。</p><p>XA规范是X/Open组织 在DTP（Distributed Transaction Processing）模型中定义的，用于描述事务管理器（TM）和资源管理器（RM）之间的交互的接口标准。</p><p>XA 规范基于2PC 实现。<br><img src="/5b064db6/2.png" class></p><p>但是 2PC协议是一种通用的事务提交协议，可以在任何实现中使用。除了XA规范，2PC协议还可以用于其他事务管理协议和框架，如：</p><ol><li><strong>Seata</strong>：阿里巴巴开源的分布式事务框架，提供全局事务管理服务，支持2PC但不直接使用XA接口。</li><li><strong>Atomikos</strong>：支持两阶段提交协议的独立事务管理器。</li><li><strong>Bitronix</strong>：另一个独立事务管理器，也支持2PC协议。</li><li>在某些场景下，可以直接在应用程序代码中实现简化版的2PC协议，而无需遵循XA规范。</li></ol><h2 id="53-三阶段提交3pc"><a href="#5-3-三阶段提交（3PC）" class="headerlink" title="5.3  三阶段提交（3PC）"></a>5.3  三阶段提交（3PC）</h2><p>3PC 的3个阶段，</p><ol><li>CanCommit</li><li>PreCommit</li><li>DoCommit</li></ol><p>3PC 相比2PC 的变化</p><ol><li>3PC提交把2PC的prepare 阶段细分为两个阶段，分别称为 CanCommit、PreCommit</li><li>参与者增加了超时处理机制，超时默认会提交事务</li></ol><p>3PC 的提出是为了改进2PC 存在的问题</p><h3 id="531-cancommit-优化回滚操作性能"><a href="#5-3-1-CanCommit-优化回滚操作性能" class="headerlink" title="5.3.1 CanCommit 优化回滚操作性能"></a>5.3.1 CanCommit 优化回滚操作性能</h3><p>新增的 CanCommit 是一个询问阶段，协调者让每个参与的数据库根据自身状态，评估该事务是否有可能顺利完成。这可以解决提高precommit 阶段的成功率，万一失败了，回滚操作也比较轻，因为还没开始做实质性的操作</p><p>但是这里要注意一个性能问题，在事务需要回滚的场景中，三段式的性能通常要比两段式好很多，但在事务能够正常提交的场景中，两段式和三段式提交的性能都很差，三段式因为多了一次询问，性能还要更差一些。</p><h3 id="532-解决协调者单点问题"><a href="#5-3-2-解决协调者单点问题" class="headerlink" title="5.3.2 解决协调者单点问题"></a>5.3.2 解决协调者单点问题</h3><p>通过增加参与者超时处理机制，默认会提交事务，相当于解决了协调者宕机参与者阻塞等待的单点问题</p><h3 id="533-加重数据一致性问题"><a href="#5-3-3-加重数据一致性问题" class="headerlink" title="5.3.3 加重数据一致性问题"></a>5.3.3 加重数据一致性问题</h3><p>在2PC中已经讨论过,为什么2PC参与者没有超时处理机制？<br>因为超时处理机制可能引发数据一致性问题，当 参与者迟迟收不到commit or rollback 指令时， 参与者不论是 默认提交 还是默认回滚，都有可能导致全局数据不一致。</p><p>3PC 增加了超时机制， 会默认提交事务，这会加重数据一致性的问题</p><h2 id="54-tcctry-confirmcancel"><a href="#5-4-TCC（Try-Confirm-Cancel）" class="headerlink" title="5.4 TCC（Try-Confirm/Cancel）"></a>5.4 TCC（Try-Confirm/Cancel）</h2><p>TCC是一种应用层事务协议，它分为三个阶段：Try（尝试）、Confirm（确认）、Cancel（取消）。在Try阶段，每个参与者尝试执行事务并锁定必要资源；在Confirm阶段，如果所有参与者的Try操作都成功，那么执行Confirm操作提交事务；如果任何Try失败，则执行Cancel操作回滚事务。TCC适用于业务逻辑复杂，需要长时间运行的事务。</p><p>个人认为，TCC可以被理解为是2PC的一种变体，具有两阶段的结构，但它在实施和操作上更适合处理复杂的业务逻辑和提高系统的灵活性与效率。</p><h2 id="55-可靠消息队列"><a href="#5-5-可靠消息队列" class="headerlink" title="5.5  可靠消息队列"></a>5.5  可靠消息队列</h2><p>使用可靠消息队列来解决分布式事务问题是一种被称为“最终一致性”的策略，它通过异步消息传递的方式，确保在分布式系统中多个服务之间的数据一致性。</p><p>使用可靠消息队列解决分布式事务的核心思想在于：</p><ol><li><strong>异步与最终一致性</strong>：通过异步的方式处理分布式事务，并确保最终一致性。</li><li><strong>可靠消息传递</strong>：确保消息传递的可靠性，包括重试机制、幂等处理等。</li></ol><h2 id="56-saga"><a href="#5-6-SAGA" class="headerlink" title="5.6 SAGA"></a>5.6 SAGA</h2><p>SAGA是一种将长期事务分解为一系列较小的、独立的子事务的方法。每个子事务都可以单独提交或回滚。如果某个子事务失败，SAGA通过执行补偿事务（即逆操作）来恢复之前的状态。SAGA降低了资源锁定的时间，适用于微服务架构中的事务管理。</p><p>参考文章<br>《周志明的软件架构课》<br><a href="https://www.51cto.com/article/648668.html">https://www.51cto.com/article/648668.html</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;1-什么是事务&quot;&gt;&lt;a href=&quot;#1-什么是事务&quot; class=&quot;headerlink&quot; title=&quot;1. 什么是事务&quot;&gt;&lt;/a&gt;1. 什么是事务&lt;/h1&gt;&lt;p&gt;事务（Transaction）的概念起源于数据库领域，最早由美国计算机科学家 E. F. Cod</summary>
      
    
    
    
    
    <category term="事务" scheme="http://example.com/tags/%E4%BA%8B%E5%8A%A1/"/>
    
  </entry>
  
  <entry>
    <title>使用github page+hexo 创建个人网站</title>
    <link href="http://example.com/5fe6baa/"/>
    <id>http://example.com/5fe6baa/</id>
    <published>2024-04-29T01:29:29.000Z</published>
    <updated>2024-06-16T09:50:05.895Z</updated>
    
    <content type="html"><![CDATA[<p>关于使用 github page + hexo 创建个人网站， <a href="https://hexo.io/zh-cn/">hexo官网</a>上的步骤已经非常详细，网上也有非常多相关的文章， 所以基础步骤就不写了。</p><p>这里记录一些个性化过程中遇到的问题。</p><h1 id="1-toc-锚点失效"><a href="#1-TOC-锚点失效" class="headerlink" title="1. TOC 锚点失效"></a>1. TOC 锚点失效</h1><p>文章目录正常生成了，但是点击目录无法跳转到文章对应位置。<br><a href="https://convivae.top/posts/hexo-bo-ke-cai-keng/#%E6%96%B9%E6%B3%95-2">解决办法点这里查看</a></p><h1 id="2-文章的短链接生成"><a href="#2-文章的短链接生成" class="headerlink" title="2. 文章的短链接生成"></a>2. 文章的短链接生成</h1><p>hexo 文章标题默认的格式是:year/:month/:day/:title/,这个格式的标题在我看来有2个主要问题</p><ol><li>太长，可以考虑只取默认格式中的一部分，如:title/</li><li>易变化， 写文章时有可能会修改标题,所以文章的url就会发生变化。 url 一旦发生变化就会对网站排名产生负面影响</li></ol><p>综上，我希望每篇文章都有一个 固定且短的 url。</p><p><a href="https://github.com/rozbo/hexo-abbrlink">hexo-abbrlink:create one and only link for every post for hexo</a> 插件可以实现该功能，</p><p>它根据文章标题和创建时间为文章生成一个abbrlink， 如果文章已经有该属性则不会重复生成。</p><h1 id="3-custom-domain-消失记"><a href="#3-custom-domain-消失记" class="headerlink" title="3. custom domain 消失记"></a>3. custom domain 消失记</h1><p>在github pages 配置了custom domain ，但是我发现每次deploy新内容后，配置好的custom domain 都会消失，经过排查发现是缺失来 CNAME文件。</p><h2 id="31-cname是什么"><a href="#3-1-CNAME是什么" class="headerlink" title="3.1 CNAME是什么"></a>3.1 CNAME是什么</h2><p>CNAME（Canonical Name）记录是一种DNS（Domain Name System）记录类型，用于将一个域名别名映射到另一个真正的域名。<br>它的作用是简化域名管理、实现负载均衡、支持CDN集成等。CNAME记录的工作流程包括DNS查询、递归查询、权威DNS服务器响应和IP地址返回等步骤。通过正确配置CNAME记录，可以有效管理和优化网站的域名解析。</p><h2 id="32-cname记录的作用"><a href="#3-2-CNAME记录的作用" class="headerlink" title="3.2 CNAME记录的作用"></a>3.2 CNAME记录的作用</h2><ol><li>域名重定向：允许多个域名指向同一个目标域名，简化了域名管理。例如，将www.example.com和blog.example.com都指向example.com。</li><li>负载均衡：通过CNAME记录可以将流量分布到不同的服务器，实现负载均衡。</li><li>内容分发网络（CDN）集成：CDN提供商通常要求将用户的子域名（如cdn.example.com）CNAME到他们的CDN域名（如cdn.provider.com），以便进行流量管理和内容分发。</li></ol><h2 id="33-cname记录的工作流程"><a href="#3-3-CNAME记录的工作流程" class="headerlink" title="3.3 CNAME记录的工作流程"></a>3.3 CNAME记录的工作流程</h2><p>CNAME记录的工作流程可以分为两个主要阶段：解析CNAME记录本身和解析CNAME记录指向的目标域名，直到最终得到一个IP地址。</p><ol><li>DNS查询开始：<br>用户在浏览器中输入域名，如www.example.com，并发送DNS查询请求。</li><li>递归DNS服务器处理请求：<br>用户的计算机向递归DNS服务器（通常由ISP提供）发送请求。<br>递归DNS服务器查询根DNS服务器，获取顶级域名服务器（如.com的服务器）的信息。</li><li>递归查询过程：<br>递归DNS服务器查询顶级域名服务器，获取该域的权威DNS服务器信息（如example.com的DNS服务器）。<br>递归DNS服务器接着查询权威DNS服务器。</li><li>权威DNS服务器响应：<br>权威DNS服务器查找www.example.com的DNS记录。<br>如果www.example.com有一个CNAME记录指向example.com，权威DNS服务器返回这个CNAME记录。</li><li>CNAME解析：<br>递归DNS服务器接收到CNAME记录后，再次进行DNS查询，以解析CNAME记录指向的目标域名example.com。<br>递归DNS服务器最终获取目标域名example.com的A记录（IP地址）。</li><li>返回IP地址：<br>递归DNS服务器将目标域名example.com的A记录返回给用户的计算机。<br>用户的计算机使用该IP地址与目标服务器建立连接，加载网站内容。<h2 id="34-hexo-中cname-文件的作用"><a href="#3-4-Hexo-中CNAME-文件的作用" class="headerlink" title="3.4 Hexo 中CNAME 文件的作用"></a>3.4 Hexo 中CNAME 文件的作用</h2></li></ol><p>当你使用自定义域名而不是<code>username.github.io</code>来访问网站时，需要在在<code>source</code>目录中创建一个文件<code>CNAME</code> 文件，并写入自定义域名</p><ul><li><code>CNAME</code>文件的存在和内容会告知GitHub Pages你希望通过哪个自定义域名访问你的网站。</li><li>例如，如果你的自定义域名是<code>www.example.com</code>，你需要在<code>CNAME</code>文件中写入<code>www.example.com</code>。</li></ul><h1 id="4-数学公式的支持"><a href="#4-数学公式的支持" class="headerlink" title="4. 数学公式的支持"></a>4. 数学公式的支持</h1><p>可参考 <a href="https://myblackboxrecorder.com/use-math-in-hexo/">在任意的hexo主题支持数学公式</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;关于使用 github page + hexo 创建个人网站， &lt;a href=&quot;https://hexo.io/zh-cn/&quot;&gt;hexo官网&lt;/a&gt;上的步骤已经非常详细，网上也有非常多相关的文章， 所以基础步骤就不写了。&lt;/p&gt;
&lt;p&gt;这里记录一些个性化过程中遇到的问题。</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>chatGPT是如何被训练出来的</title>
    <link href="http://example.com/8cb014c5/"/>
    <id>http://example.com/8cb014c5/</id>
    <published>2024-04-19T07:23:32.000Z</published>
    <updated>2024-05-17T09:14:34.168Z</updated>
    
    <content type="html"><![CDATA[<p>本文内容基于 Andrej Karpathy 的视频 <a href="https://www.youtube.com/watch?v=bZQun8Y4L2A">State of GPT</a>，并加入了个人理解，进行总结。</p><p>该部分的主题是how to train your GPT assistants，  在chatGPT 的语境中，Assistant 特指能回答问题，像助手一样可以帮我们做很多事。</p><h1 id="0-gpt训练的四个阶段"><a href="#0-GPT训练的四个阶段" class="headerlink" title="0. GPT训练的四个阶段"></a>0. GPT训练的四个阶段</h1><img src="/8cb014c5/1.png" class><p>目前我们能够使用到的chatGPT 都是RLFH 模型，该模型的训练可以分为3个阶段</p><ol><li>pretraining  预训练</li><li>Supervised finetuning 监督微调</li><li>Reinforcement Learning from Human Feedback， 包括reward modeling 和Reinforcement Learning， 因为reward modeling 不能独立起作用，也不能独立部署，必须Reinforcement Learning结合使用，所以把它们归类为一个阶段。</li></ol><p>在以上每个阶段中，都有各自训练需要的数据集、算法、和训练输出结果</p><h1 id="1-pre-training-预训练"><a href="#1-pre-training-预训练" class="headerlink" title="1. pre-training   预训练"></a>1. pre-training   预训练</h1><p>pre-trainin 预训练阶段是一切的起点，它需要最多的数据，最多的计算资源GPU，最长的训练时间。</p><p>总的特点如下</p><ol><li><strong>大规模数据</strong>：预训练使用的大规模数据集包含了来自互联网的各种文本，这些数据集规模庞大，通常包含数十亿甚至数百亿个token。</li><li><strong>无监督学习</strong>：预训练通常采用无监督学习方法，即不需要人为标注的数据。模型通过预测文本中的下一词（或下一个token）来学习语言结构和模式。</li><li><strong>通用性</strong>：预训练模型具有通用性，因为它并没有针对特定任务进行优化，而是广泛地学习各种语言模式。这种通用性使得模型可以适应多种下游任务。<h2 id="11-dataset"><a href="#1-1-Dataset" class="headerlink" title="1.1 Dataset"></a>1.1 Dataset</h2></li></ol><img src="/8cb014c5/2.png" class><p>预训练使用的大规模数据集包含了来自互联网的各种文本，这些数据集规模庞大，不过目前chatGPT 没有公开具体的数据， 上图是meta 开源的LLaMA的训练数据集</p><ul><li>67.0%的Common Crawl，也就是常规网络爬取的数据集，这部分数据集的特点是内容涉及的类型很全面，但是因为内容可能是任何人写的，质量一般偏低，也会包含大量不相关，例如广告、导航条、版权声明等。</li><li>15.0%是C4数据集 (Colossal Clean Crawled Corpus, “庞大的清洁语料库”)，这个数据集包含了大量的网页文本，这些文本已经过清理，移除了广告、重复内容、非英语文本、和其他不适合训练的元素。这个数据集的目标是提供一个大规模、高质量、多样性强的英语文本数据集，以支持自然语言处理任务。尽管C4经过清理，但仍然包含了来自互联网的各种文本，因此可能包含一些质量不高的信息的信息。</li><li>剩余18%的训练数据来源主要是来自Github、维基百科、书籍、Arxiv论文、交易所等， 可以认为是高质量的数据。</li></ul><p>这些训练数据有以下特点</p><ol><li><strong>规模庞大</strong>：训练数据的数量非常大，包含数十亿条文本记录。这种大规模的数据量帮助模型学会语言的复杂性和细微差别。</li><li><strong>多样性</strong>：训练数据涵盖了广泛的主题和领域，包括科学、艺术、历史、文学、技术、日常生活等。这种多样性使得模型能够应对各种类型的问题和对话。</li><li><strong>广泛覆盖</strong>：涵盖了从基础知识到专业知识的广泛范围，使得模型在回答问题时既能处理简单的日常问题，也能应对复杂的专业问题。</li><li>. <strong>开放获取</strong>：所有数据都来自公开可获取的资源，没有使用私人或受保护的数据，确保了数据的合法性和道德性。</li></ol><p>根据以上特点，可以总结认为像chatGPT 这样的LLM, 学习了人类在互联网上发表过所有知识。不过由于高质量的数据只占18%， 如果你想获取高质量的回答，所以你需要一些chatGPT 沟通的技巧， 即prompt 技巧，才能获取高质量的回答。</p><h2 id="12-preprocess-dataset-tokenization"><a href="#1-2-preprocess-dataset-Tokenization" class="headerlink" title="1.2 preprocess dataset-Tokenization"></a>1.2 preprocess dataset-Tokenization</h2><p>针对从互联网上获取到的大量数据，chatGPT 并不是直接拿来训练，而是要经过tokenization 标记化，将文本转换成模型可以理解的整数序列。</p><h3 id="121-什么是tokenization"><a href="#1-2-1-什么是Tokenization？" class="headerlink" title="1.2.1 什么是Tokenization？"></a>1.2.1 什么是Tokenization？</h3><p>Tokenization是将文本拆分成较小的单元（称为tokens）的过程。这些tokens可以是单词、子词、字符或其他文本片段，，并将其映射到特定的标记或整数的过程。</p><img src="/8cb014c5/3.png" class><p>从图中可以看出，GPT 并不是直接使用从互联网上获取的原始数据进行训练， 而是先讲数据分解成token,再将token 转化成整数数列。最终进入到神经网络/Transformer 进行训练的是这些整数数列。</p><p>以下Tokenization的步骤：</p><ol><li><p><strong>文本清理</strong>：在进行Tokenization之前，文本可能需要清理，比如去掉多余的空格、标点符号的处理等。这一步骤视具体应用而定。</p></li><li><p><strong>拆分文本</strong>：将文本拆分成tokens。例如，对于句子“ChatGPT is great.”，可以拆分为“ChatGPT”、“is”和“great”。</p></li><li><p><strong>子词级别的Tokenization</strong>：现代语言模型（如GPT-3）通常使用子词（subword）级别的Tokenization，如Byte Pair Encoding（BPE）或WordPiece。这些方法可以将罕见词分解为更常见的子词片段。例如，单词“unhappiness”可以分解为“un”、“happiness”或进一步分解为“un”、“happi”、“ness”。</p></li><li><p><strong>分配唯一ID</strong>：每个token被分配一个唯一的整数ID，模型内部使用这些ID而不是直接使用文本。例如，“ChatGPT”可能被分配ID 12345，“is”被分配ID 6789，依此类推。</p></li></ol><h3 id="122什么是token"><a href="#1-2-2什么是token" class="headerlink" title="1.2.2什么是token"></a>1.2.2什么是token</h3><p>token 在自然语言处理（NLP）中扮演着核心角色，尤其是在训练像ChatGPT这样的大型语言模型时，token是<code>模型训练</code>和<code>生成文本</code>的基本单位。 在英语预料中，它可以是是一个词、也可以是一部分词、或者一个字符（虽然演讲中示例对token 的举例基本是都是完整的单词）</p><p>那么这里就要思考，为什么不用完整的单词训练呢，既简单又直接。</p><ol><li>词汇表大小和稀疏性问题<br>使用完整单词进行训练的一个主要问题是词汇表的大小。英语和其他语言中的词汇量非常庞大，尤其是当考虑到新词、专有名词、不同的词形变化（如复数形式、时态等）时，词汇表的规模可能会变得非常大。<ul><li><strong>大词汇表问题</strong>：一个庞大的词汇表意味着模型需要处理更多的单词，这不仅增加了模型的复杂性，还会显著增加训练和运行模型所需的资源（如内存和计算时间）。</li><li><strong>稀疏性问题</strong>：当词汇表很大时，很多单词在训练数据中出现的频率可能非常低，导致数据稀疏。稀疏性问题会降低模型对这些单词的学习效率，影响模型的性能和泛化能力。</li></ul></li><li>处理未知词汇的能力<br>直接使用完整单词进行训练面临的另一个挑战是如何处理训练数据中未出现过的单词（即未知词汇或OOV问题）。<ul><li><strong>未知词汇（OOV）问题</strong>：在新的文本中经常会出现训练数据中未见过的单词。如果模型只学习到了完整单词，那么它很难处理这些未知词汇。</li><li><strong>泛化能力</strong>：通过使用子词（如词干、前缀、后缀等）或更小的单元，模型可以更好地泛化到未见过的单词。例如，通过识别“un-”和“-able”这样的前后缀，模型可以推断出“unbelievable”等单词的含义，即使这个词在训练数据中没有直接出现过。</li></ul></li><li>训练效率和计算资源<br>分词还可以帮助提高训练效率和减少对计算资源的需求。<ul><li><strong>减少参数数量</strong>：较小的词汇表可以减少模型的参数数量，降低过拟合的风险，同时提高模型的训练速度和推理速度。</li><li><strong>内存和存储优化</strong>：使用更小的词汇表意味着可以更高效地使用内存和存储资源，尤其是在嵌入层中。</li></ul></li><li>适应多样化的语言现象<br>语言中存在大量的变体和创新，直接使用完整单词可能难以适应这些变化。<ul><li><strong>词形变化</strong>：在许多语言中，单词可以有多种形式。使用基于规则或统计的分词方法可以帮助模型理解不同词形之间的关联，提高模型对语言变化的适应能力。</li><li><strong>新词创造和网络语言</strong>：新词和网络流行语的出现是常态。分词系统可以通过更新词库或调整分词算法来适应这些变化。</li></ul></li></ol><h2 id="13-unsupervised-learning-无监督学习"><a href="#1-3-Unsupervised-Learning-无监督学习" class="headerlink" title="1.3 Unsupervised Learning-无监督学习"></a>1.3 Unsupervised Learning-无监督学习</h2><p>预训练采用无监督学习方法，即不需要人为标注的数据。模型通过预测文本中的下一词（或下一个token）来学习语言结构和模式。</p><img src="/8cb014c5/4.png" class><p>数据会以批次为单位， 输入到Transformer中进行训练，其训练过程就是不断让模型根据前面已经的内容（黄色部分），去猜测当前token(绿色部分)的下一个词是什么（红色部分）， 如果猜测的结果和实际情况不一致，则要调整模型，直至结果一致。</p><p>猜测的过程是基于概率进行的。</p><p>如果猜错了，那距离正确答案又多远，这就是损失函数的概念。低损失意味着更高的预测正确概率</p><img src="/8cb014c5/5.png" class><h2 id="13-base-model"><a href="#1-3-base-model" class="headerlink" title="1.3 base model"></a>1.3 base model</h2><p>预训练完成后会得到一个base model ，因为它并没有针对特定任务进行优化，而是广泛地学习各种语言模式。这种通用性使得模型可以适应多种下游任务。</p><h3 id="131-base-model-are-not-assistant"><a href="#1-3-1-base-model-are-not-assistant" class="headerlink" title="1.3.1 base model are not assistant"></a>1.3.1 base model are not assistant</h3><p>这句话的含义是base model 并不能回答问题。<br>预训练阶段得到的base model  只是一个文档生成器， 只会根据你输入的内容去预测下一个单词，并不会回答你的问题，所以还起不到assistant 助手 的作用。 如图中，base model 并不会按照你的要求写诗，知识生成相似的内容。<br><img src="/8cb014c5/6.png" class></p><p>what is the capital of France?</p><p>再例如针对以上问题，base model 并不会回答问题，给出“Paris”, 而是会续写内容， 给出以下可能的答案<br>what is France’s largest city?<br>what is France’s population?<br>what is the currency of France?</p><h2 id="14-如何让-base-model-回答问题"><a href="#1-4-如何让-base-model-回答问题" class="headerlink" title="1.4  如何让 base model 回答问题"></a>1.4  如何让 base model 回答问题</h2><h3 id="141-few-shot-learning"><a href="#1-4-1-few-shot-Learning" class="headerlink" title="1.4.1 few-shot Learning"></a>1.4.1 few-shot Learning</h3><p><strong>Few-shot Learning</strong>：指的一种在给模型提供少量示例的情况下让其学习新任务的方法。Few-shot 可以分为零样本学习（zero-shot learning）、一样本学习（one-shot learning）和小样本学习（few-shot learning）。<br><strong>原理</strong>：模型在大规模语料上进行预训练，学到了广泛的语言知识和基本任务能力。在新任务上，通过提供少量的示例（输入-输出对），模型可以从中推断出该任务的模式和要求。</p><p>few-shot 之所以起作用，是把问题伪装成了一个文档中缺失的内容，让base model 通过完成文档的能力把它补全。但是这一过程非常不稳定， 在实践中总体效果一般。<br><img src="/8cb014c5/7.png" class></p><h3 id="142-supervised-finetuning"><a href="#1-4-2-Supervised-finetuning" class="headerlink" title="1.4.2 Supervised finetuning"></a>1.4.2 Supervised finetuning</h3><p>指在base model 的基础上，使用带有标签的数据集对模型进行进一步训练，以提高其在特定任务上的表现。</p><h3 id="143-few-shot-vs-supervised-finetuning"><a href="#1-4-3-few-shot-vs-Supervised-finetuning" class="headerlink" title="1.4.3 few-shot  vs Supervised finetuning"></a>1.4.3 few-shot  vs Supervised finetuning</h3><ul><li><strong>Few-shot Learning</strong>：<ul><li><strong>优点</strong>：无需大量标注数据，适应新任务快速。</li><li><strong>缺点</strong>：在任务复杂或示例较少时，效果可能不如监督微调。</li></ul></li><li><strong>监督微调</strong>：<ul><li><strong>优点</strong>：在有足够标注数据时，能够显著提升模型性能。</li><li><strong>缺点</strong>：需要大量标注数据，成本较高。</li></ul></li></ul><h1 id="2-supervised-finetuning-stage-微调"><a href="#2-Supervised-finetuning-stage-微调" class="headerlink" title="2. Supervised finetuning stage 微调"></a>2. Supervised finetuning stage 微调</h1><p>指在base model 的基础上，使用带有标签的数据集对模型进行进一步训练，以提高其在特定任务上的表现。</p><h2 id="21-dataset"><a href="#2-1-Dataset" class="headerlink" title="2.1 Dataset"></a>2.1 Dataset</h2><p>相比pre-training 预训练阶段，Supervised finetuning 监督微调阶段使用少量但高质量的数据集。<br>这些数据集包含了成对的输入和期望输出，例如问题与答案（Q&amp;A）、命令与响应、或者其他相关任务的配对。<br>这些数据通常通过以下方式获得：</p><ol><li><strong>人工标注</strong>：雇佣标注人员根据预设的指导文档（labeling documentation）来创建数据。标注人员可能会根据给定的指令编写问题的答案，或者评估和选择模型生成的候选回答。</li><li><strong>众包平台</strong>：使用众包服务（如Amazon Mechanical Turk）来收集和标注数据。这种方式可以快速且成本相对较低地获得大量标注数据</li><li><strong>合作伙伴</strong>：与学术机构、研究组织或其他公司合作，共享或共同创建数据集。</li></ol><img src="/8cb014c5/8.png" class><h2 id="22-sft-model"><a href="#2-2-SFT-model" class="headerlink" title="2.2 SFT model"></a>2.2 SFT model</h2><p>SFT model 已经是一个可以回答问题的assistant</p><h2 id="24-pre-training-vs-fine-tuning"><a href="#2-4-Pre-training-vs-fine-tuning" class="headerlink" title="2.4  Pre-training vs fine tuning"></a>2.4  Pre-training vs fine tuning</h2><img src="/8cb014c5/9.png" class><p>以下是预训练（Pre-training）和微调（Fine-tuning）两个阶段的对比表格，从目标、数据集、过程、成本和迭代与改进五个方面进行总结：</p><div class="table-container"><table><thead><tr><th>特征</th><th>预训练（Pre-training）</th><th>微调（Fine-tuning）</th></tr></thead><tbody><tr><td>目标</td><td>学习广泛的语言知识和互联网信息，不专注于特定任务。</td><td>调整预训练模型以适应特定任务或应用场景。</td></tr><tr><td>数据集</td><td>来自互联网的大规模、多样化文本数据，可能包含多种语言和主题。</td><td>较小但高质量的特定任务相关数据集，如问答对、标注文本等。</td></tr><tr><td>过程</td><td>通过预测文本序列中的下一个词进行训练，使用大规模数据集进行广泛学习。</td><td>在预训练模型的基础上，使用特定任务的数据集进行额外训练。</td></tr><tr><td>成本</td><td>高，需要大量计算资源（如GPU集群）和时间，成本可能高达数百万至数十亿美元。</td><td>相对较低，主要涉及数据标注和模型调整，计算资源需求较小。</td></tr><tr><td>迭代和改进</td><td>通常在大型公司或研究机构中进行，可能每年或几个月进行一次，取决于资源和需求。</td><td>可以频繁进行，根据模型表现和用户反馈不断优化模型。</td></tr></tbody></table></div><h1 id="3-rlhf"><a href="#3-RLHF" class="headerlink" title="3. RLHF"></a>3. RLHF</h1><p>Reinforcement Learning from Human Feedback<br>根据人类反馈进行强化学习<br>这个阶段包含两个步骤，</p><ol><li>Reward Modeling</li><li>Reinforcement Learning<h2 id="3-1-reward-modeling"><a href="#3-1-Reward-Modeling" class="headerlink" title="3. 1 Reward Modeling"></a>3. 1 Reward Modeling</h2></li></ol><h3 id="311-dataset-comparisons"><a href="#3-1-1-Dataset-comparisons" class="headerlink" title="3.1.1  Dataset- comparisons"></a>3.1.1  Dataset- comparisons</h3><img src="/8cb014c5/10.png" class><ol><li>相同的prompt, SFT model  会给出不同版本的回答，即多个候选答案。目前我们在使用</li><li>人类评估者会对这些候选回答进行比较打分排名，并选择他们认为最合适或最准确的回答。</li><li>模型会根据这些比较结果进行学习，以便对其他问题的多个候选答案的好坏进行预测</li></ol><h3 id="312-rm-training"><a href="#3-1-2-RM-Training" class="headerlink" title="3.1.2 RM Training"></a>3.1.2 RM Training</h3><img src="/8cb014c5/11.png" class><p>从图中可以看出， 提示词+ 结果+ 人类评估者反馈作为关联在一起的数据又被打包到batch中进行训练， 如图，可以理解成一个相同的提示词prompt 有3个不同版本的回答，对应的3个不同的reward(可以理解成是分数)， 训练后模型就可以针对一个prompt 的回答预测出一个人类评估者的打分。</p><p>所以这个阶段的目的就是构建一个reward model 学习人类打分的规律，以来预测一个回答可能会获得的人类评分。</p><p>由于reward model只是学习了人类打分的规律，所以如果单独使用Reward Model 进行打分并不会促进模型生成更好的答案 ， 它需要和Reinforcement Learning 强化学习结合使用才能起作用</p><h2 id="32-reinforcement-learning"><a href="#3-2-Reinforcement-Learning" class="headerlink" title="3.2 Reinforcement Learning"></a>3.2 Reinforcement Learning</h2><p>有了reward model 后，就可以对模型的回答进行打分。对于分数高的回答，要提高其出现的概率，如此不断迭代，尽可能生成获得更高分的回答。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;本文内容基于 Andrej Karpathy 的视频 &lt;a href=&quot;https://www.youtube.com/watch?v=bZQun8Y4L2A&quot;&gt;State of GPT&lt;/a&gt;，并加入了个人理解，进行总结。&lt;/p&gt;
&lt;p&gt;该部分的主题是how to tra</summary>
      
    
    
    
    
    <category term="AI" scheme="http://example.com/tags/AI/"/>
    
    <category term="chatGPT" scheme="http://example.com/tags/chatGPT/"/>
    
  </entry>
  
  <entry>
    <title>Intro to chatGPT,从G、P、T的含义解释chatGPT</title>
    <link href="http://example.com/60dfe914/"/>
    <id>http://example.com/60dfe914/</id>
    <published>2024-04-19T07:19:46.000Z</published>
    <updated>2024-05-17T09:14:34.174Z</updated>
    
    <content type="html"><![CDATA[<p>ChatGPT是由OpenAI开发的，一个能够理解和生成自然语言的人工智能（AI）模型，可以和用户进行互动并生成类似人类的对话。</p><img src="/60dfe914/1.png" class><h1 id="1-chatgpt-的发展历程"><a href="#1-chatGPT-的发展历程" class="headerlink" title="1. chatGPT 的发展历程"></a>1. chatGPT 的发展历程</h1><p>ChatGPT模型的发展历程是一个不断演进和改进的过程。以下是关键的时间节点和发展阶段：</p><ol><li><strong>GPT-1（2018年6月）</strong>：OpenAI发布了首个生成预训练变换器模型（Generative Pre-trained Transformer，GPT-1）。该模型基于Transformer架构，使用无监督学习方法在大规模文本语料上进行预训练，然后在特定任务上进行微调（Fine-tuning）。<ul><li><strong>架构</strong>：12层Transformer解码器，参数量约为1.17亿。</li><li><strong>创新点</strong>：引入无监督预训练和有监督微调相结合的训练方法，在多个NLP任务上表现优异。</li></ul></li><li><strong>GPT-2（2019年2月）</strong>：OpenAI发布了GPT-2，模型规模和能力大幅提升。最初由于担心模型被滥用，OpenAI仅发布了部分参数的模型，后于2019年11月发布了完整模型。<ul><li><strong>架构</strong>：最大版本有48层，参数量达15亿。</li><li><strong>创新点</strong>：显著提高了模型的生成质量和连贯性，在文本生成、翻译、问答等任务上表现出色。</li></ul></li><li><strong>GPT-3（2020年6月）</strong>：是当时最大和最强大的语言模型，包含1750亿参数。<ul><li><strong>架构</strong>：1750亿参数，96层，采用更大规模的数据进行训练。</li><li><strong>创新点</strong>：通过超大规模预训练和少量示例（Few-shot Learning），在无需微调的情况下，也能在多个任务上取得惊人的效果。</li></ul></li><li><strong>GPT-3.5（2021年11月）</strong>：<ul><li>是GPT-3的改进版本，模型参数达到2000亿，利用人类反馈进行强化学习，进一步提升模型的交互能力。</li><li>增强了处理复杂对话和多轮对话的能力。</li></ul></li><li><strong>GPT-4（2023年2月）</strong>：<ul><li>尚未公开具体参数，但推测远超GPT-3.5，支持多模态输入和输出</li><li>提供了更高水平的自然语言理解和生成能力，支持多模态输入与输出</li></ul></li></ol><ul><li><strong>GPT-4o(2024年5月)</strong><ul><li>工程能力大幅提升， 体现在相应速度更快，且开始能理解语音语调</li></ul></li></ul><p>ChatGPT模型的发展不仅仅是参数的增加，更是算法优化、数据多样化和对用户反馈的持续改进。</p><p>目前我们使用的chatGPT 后GPT-3.5、GPT-4、GPT-4o 三个版本可选。</p><h1 id="2-g-p-t分别是什么意思"><a href="#2-G、P、T分别是什么意思" class="headerlink" title="2. G、P、T分别是什么意思"></a>2. G、P、T分别是什么意思</h1><p>GPT 是 Generative Pre-trained Transformer 的缩写。以下内容将重点介绍G、P、T 三个字母各自的含义，以此来更好得理解chatGPT 是什么。</p><h2 id="21-generative-生成式"><a href="#2-1-Generative-生成式" class="headerlink" title="2.1 Generative  生成式"></a>2.1 Generative  生成式</h2><p>ChatGPT是一个Generative AI, 即生成式AI。</p><p>AI 作为一个总称，其实包含非常多具体的类型， ChatGPT 所属的Generative  AI 是其中一个子类。</p><p>日常生活中会用到的siri、小度、小爱、识别图片中动物是小猫、医院的专家诊断系统、和你国际象棋对战的机器人，这些都是AI 。 但是这些AI 都是规则驱动的系统，只能根据预设规则进行回答，一旦超出预设范围，就会表现的人工智障。</p><p>chatGPT作为一个Generative AI, 其最大的特点就是根据输入生成新内容。它不仅能回答问题，还能进行创意写作、故事生成、诗歌创作等多种任务。</p><p>Generative AI 也有自己具体的分类</p><ol><li>Natural language generation<br>chatGPT 就是 NLG, 可以说是目前最著名的AI 应用</li><li>text to image<br>根据文字生成图片<ol><li>Midjourney</li><li>DALL-E</li><li>Stable Diffusion</li></ol></li><li>Generative Adversarial Networks (GANs)<br>生成对抗网络，其核心是两个相互对抗的网络：生成器（Generator）和判别器（Discriminator）。这两个网络在训练过程中相互竞争，从而不断提升自身的性能。<ol><li><strong>生成器（Generator）</strong> - 这个网络的任务是捕捉训练数据的分布，并生成尽可能接近真实数据的新数据。生成器接受一个随机噪声向量作为输入，通过这个噪声向量构造出新的数据实例。</li><li><strong>判别器（Discriminator）</strong> - 判别器的任务是区分输入给它的数据是来自训练集（即真实数据）还是生成器生成的假数据。基本上，判别器是一个二分类模型，输出一个标量表示输入数据是真实数据的概率。<br>这两个模型在训练过程中进行对抗。生成器试图产生越来越真实的数据以“欺骗”判别器，而判别器则试图变得更好地区分真假数据。通过这种对抗过程，生成器学会生成高质量的数据。</li></ol></li><li>VAEs<br>可用于异常检测，方法是在正常数据的数据集上训练模型，然后使用经过训练的模型来识别偏离正常数据的实例。这可用于检测各种情况下的异常情况，例如发现金融交易中的欺诈行为、发现制造中的缺陷或发现网络中的安全漏洞。例如，Uber 在其金融交易中使用 VAE 进行异常检测，以检测欺诈行为。</li></ol><h2 id="22-pre-trained-预训练"><a href="#2-2-Pre-trained-预训练" class="headerlink" title="2.2 Pre-trained 预训练"></a>2.2 Pre-trained 预训练</h2><p>预训练 是chatGPT训练 的第一个阶段。</p><p>chatGPT 的 训练可以分为以下几个阶段</p><ol><li>pre-training</li><li>supervised fine-tuning</li><li>reward modeling</li><li>reinforcement learning</li></ol><p>其中pre-training是第一个阶段, 在预训练阶段使用的数据集通常是从互联网上收集的大量文本，这些文本可能包含多种语言、主题和格式。</p><p>预训练的目标是让模型在大量的文本数据上进行了广泛的训练学习了语言的结构、语法以及大量的知识。学习结束后， 预训练 阶段会产出一个base model, base model 可以根据 用户的输入， 去续写文本， 注意， 这个阶段的chatGPT 还不能回答你的问题。</p><p>针对以下问题输入，，比如， 它只能给出这样的回答。<br>what is the capital of China?</p><p>base model 并不会回答问题，给出“beijing”, 而是会续写内容， 给出以下可能的答案</p><p>what is China’s largest city?<br>what is China’s population?<br>what is the currency of China?</p><p>如果想让模型“理解”人类的问题并进行回答， 还需要进行第二阶段的训练 supervised fine-tuning。</p><h2 id="23-transformer"><a href="#2-3-Transformer" class="headerlink" title="2.3 Transformer"></a>2.3 Transformer</h2><p>ChatGPT的核心技术是Transformer架构，这是一种深度学习模型，擅长处理序列数据。Transformer通过自注意力机制（self-attention mechanism）来捕捉输入文本中的重要特征和上下文关系。这种架构使得模型在处理长文本和复杂上下文时，能够保持较高的准确性和连贯性。</p><p>要理解Transformer在ChatGPT中的应用，我们可以用一个简单的类比来说明。想象一下，你在读一本书，并试图理解每一段的意思。你的大脑不仅仅是逐字逐句地阅读，还会结合前后的内容，理解整段的意义。Transformer在ChatGPT中的作用就像你大脑的这种理解机制。</p><h1 id="3-chatgpt-使用场景"><a href="#3-chatGPT-使用场景" class="headerlink" title="3. chatGPT 使用场景"></a>3. chatGPT 使用场景</h1><p>发挥你的想象，能用的具体场景是在太多了。后面这里会陆续补充我使用的场景</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;ChatGPT是由OpenAI开发的，一个能够理解和生成自然语言的人工智能（AI）模型，可以和用户进行互动并生成类似人类的对话。&lt;/p&gt;
&lt;img src=&quot;/60dfe914/1.png&quot; class&gt;
&lt;h1 id=&quot;1-chatgpt-的发展历程&quot;&gt;&lt;a href=&quot;</summary>
      
    
    
    
    
    <category term="AI" scheme="http://example.com/tags/AI/"/>
    
    <category term="chatGPT" scheme="http://example.com/tags/chatGPT/"/>
    
  </entry>
  
  <entry>
    <title>GPTs开发-Best English Name，你找到最合适、最满意的英文名</title>
    <link href="http://example.com/209fa7d3/"/>
    <id>http://example.com/209fa7d3/</id>
    <published>2024-04-16T08:44:20.000Z</published>
    <updated>2024-05-17T09:14:34.164Z</updated>
    
    <content type="html"><![CDATA[<p>最近开发了一个用来取英文名的GPTs <code>Best English Name</code>。<br>作为GPTs 开发者，虽然离openai 给我发钱还远着呢，但是没关系，我可以自己先用GPTs 变现，虽然变现的钱还只够cover 一个月plus  的费用。</p><h1 id="best-english-name-是什么"><a href="#Best-English-Name-是什么" class="headerlink" title="Best English Name 是什么"></a>Best English Name 是什么</h1><img src="/209fa7d3/1.png" class><p>Best English Name 是一个起名助手，它可以帮助你找到气质相符且满足各种要求的英文名，并且会对英文名有详细且深入的解释，让你充分了解自己的英文名。<br><img src="/209fa7d3/2.png" class><br>这个气质可以是你通过图片上传告诉chatGPT, 也可以自己描述给chatGPT,同时你也可以有其他定制化要求，包括但不限于</p><ol><li>和中文读音/拼写类似</li><li>特定长度</li><li>特定开头字母</li><li>女性化/中性化/男性化</li><li>MBTI</li></ol><h1 id="我为什么要开发best-english-name"><a href="#我为什么要开发Best-English-Name" class="headerlink" title="我为什么要开发Best English Name"></a>我为什么要开发Best English Name</h1><h2 id="我的痛点"><a href="#我的痛点" class="headerlink" title="我的痛点"></a>我的痛点</h2><p>本人在工作、生活中均有需要用到英文名的场景， 记得第一次需要取英文名是入职新公司，我花了一下午的时间在豆瓣上翻找曾经看过的英文影视剧，试图找到一个喜欢的英文名， 但最后选定的名字总让我觉得“这不是我”。</p><p>我自己在确定英文名的过程中，有这样的担忧</p><ol><li>这个名字听起来非常“不像我”</li><li>这个名字适合女生吗</li><li>这个名字在英语国家奇怪吗，会不会根本就没人起这个名字</li><li>这个名字会不会太大众化了，会不会有点过时了</li><li>这个名字会不会有什么不好的寓意，到时候引起不必要的误会</li></ol><p>同时我对那个我将要喜欢且适合的英文名也有一定的要求</p><ol><li>我希望这个英文名中性化一点</li><li>我希望我的名字以A 开头</li><li>我希望这个名字有美好的寓意且我能够了解这个美好的寓意，这样当别人问我为什么取这个名字的时候，我就可以清楚地向ta 介绍我的名字</li></ol><p>开始使用chatGPT 后， 我发现它可以很好的提供一些满足我要求的英文名，同时规避掉我的担忧。</p><h2 id="痛点的大众化"><a href="#痛点的大众化" class="headerlink" title="痛点的大众化"></a>痛点的大众化</h2><p>在小红书闲逛时，我发现有很多人都在为选择一个英文名困扰， ta 们通常会发出一张照片让网友根据第一眼印象帮忙起名，有时候会有一些额外的需求。</p><p>同时我还发现有很多人经常向网友询问自己的英文名是什么意思。</p><p>我认为这两种情况和我起名字时遇到的问题是一样的，chatGPT 能帮助我找到满意的英文名，那么它也可以帮助其他人找到满意的英文名， 所以我把自己取英文的过程做成了一个可复用的GPTs, 希望可以帮助更多人找到满意适合的英文名</p><h1 id="gpts-的工程开发"><a href="#GPTs-的工程开发" class="headerlink" title="GPTs 的工程开发"></a>GPTs 的工程开发</h1><h2 id="一个不懒的gpts"><a href="#一个不懒的GPTs" class="headerlink" title="一个不懒的GPTs"></a>一个不懒的GPTs</h2><p>回答详细深入。</p><h2 id="一个抗攻击的gpts"><a href="#一个抗攻击的GPTs" class="headerlink" title="一个抗攻击的GPTs"></a>一个抗攻击的GPTs</h2><p>不会泄漏该GPTs 的instruction</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;最近开发了一个用来取英文名的GPTs &lt;code&gt;Best English Name&lt;/code&gt;。&lt;br&gt;作为GPTs 开发者，虽然离openai 给我发钱还远着呢，但是没关系，我可以自己先用GPTs 变现，虽然变现的钱还只够cover 一个月plus  的费用。&lt;/p&gt;</summary>
      
    
    
    
    
    <category term="AI" scheme="http://example.com/tags/AI/"/>
    
    <category term="chatGPT" scheme="http://example.com/tags/chatGPT/"/>
    
  </entry>
  
  <entry>
    <title>Intro to AI</title>
    <link href="http://example.com/81fcff81/"/>
    <id>http://example.com/81fcff81/</id>
    <published>2024-04-16T07:13:57.000Z</published>
    <updated>2024-05-17T09:14:34.171Z</updated>
    
    <content type="html"><![CDATA[<p>在学习AI 过程中，发现专业名词相当多，初学者可能会感到而混乱，所以本篇内容是对该领域内的一些“大词”进行简单介绍， 做一些概念扫盲， 以保证在接下来的学习中心中有框架。 正式内容会按照下图框架介绍</p><img src="/81fcff81/1.png" class><h2 id="1-人工智能-artificial-intelligence"><a href="#1-人工智能-Artificial-Intelligence" class="headerlink" title="1. 人工智能 (Artificial Intelligence)"></a>1. 人工智能 (Artificial Intelligence)</h2><p>人工智能（Artificial Intelligence，简称AI）是一门研究和开发用于模拟、扩展和扩展人类智能的理论、方法、技术及应用系统的科学技术。</p><p>简单来说，AI指的是使计算机系统能够执行通常需要人类智能才能完成的任务。这些任务包括学习、推理、解决问题、感知、语言理解和生成等。</p><p>人工智能（AI）的发展历史可以追溯到20世纪中期，它的发展经历了多个重要阶段，每个阶段都有其独特的特点和里程碑事件。下面来简单介绍其发展历程</p><h3 id="11-早期阶段基础理论和初步探索1950s-1960s"><a href="#1-1-早期阶段：基础理论和初步探索（1950s-1960s）" class="headerlink" title="1.1 早期阶段：基础理论和初步探索（1950s-1960s）"></a>1.1 早期阶段：基础理论和初步探索（1950s-1960s）</h3><p><strong>1956年达特茅斯会议</strong>：AI正式诞生。1956 年，John McCarthy、Marvin Minsky、Allen Newell 和 Herbert A. Simon 等研究者在达特茅斯会议上首次提出了“人工智能”这一术语，标志着AI研究正式成为一个独立的学科。</p><p><strong>1950年代末</strong>：艾伦·图灵提出了“图灵测试”，成为判断机器是否具备智能的基准。</p><p>这一时期，由于技术和硬件的限制，早期的AI系统主要关注于简单任务的自动化，如象棋和跳棋游戏。例如，IBM 的 Deep Blue 和 MIT AI Lab 的 MacHack VI 等系统在象棋游戏中取得了显著的成就。这些系统虽然在处理复杂模式方面受限，但它们的成功展示了AI在规则清晰、限定环境中的潜力。</p><h3 id="12-专家系统与机器学习1970s-1990s"><a href="#1-2-专家系统与机器学习（1970s-1990s）" class="headerlink" title="1.2 专家系统与机器学习（1970s-1990s）"></a>1.2 专家系统与机器学习（1970s-1990s）</h3><p><strong>1970年代</strong>：专家系统（Expert Systems）开始兴起，如MYCIN用于医疗诊断，DENDRAL用于化学分析。这些系统能够储存、解释和推理知识,可以用来解决特定领域的问题。</p><p><strong>1980s</strong>：反向传播算法（Backpropagation）的提出使神经网络重新受到关注。尽管计算能力仍有限，但理论和方法上的突破为未来的发展奠定了基础。</p><p><strong>1990s</strong>: 随着更多的数据可用和计算能力的增加，机器学习方法，特别是基于统计的方法开始主导AI研究。支持向量机和随机森林等技术的发展，为AI在图像识别、自然语言处理和其他复杂模式识别任务中的应用打开了新的可能性。</p><h3 id="13-现代ai大数据与深度学习时代2000s-至今"><a href="#1-3-现代AI：大数据与深度学习时代（2000s-至今）" class="headerlink" title="1.3 现代AI：大数据与深度学习时代（2000s-至今）"></a>1.3 现代AI：大数据与深度学习时代（2000s-至今）</h3><ol><li><strong>计算能力和数据的提升</strong>：<ul><li><strong>2000s</strong>：互联网和大数据的发展，为AI提供了大量训练数据。计算能力的提升，特别是GPU的使用，使得复杂的模型训练成为可能。</li></ul></li><li><strong>深度学习的崛起</strong>：<ul><li><strong>2010s</strong>：深度学习（Deep Learning）在图像识别、语音识别、自然语言处理等领域取得突破。AlexNet在2012年ImageNet竞赛中的成功，标志着深度学习的重大胜利。</li></ul></li><li><strong>广泛应用</strong>：<ul><li><strong>2010s-至今</strong>：AI在自动驾驶、医疗诊断、金融分析、智能客服等领域得到广泛应用。2016年，AlphaGo击败围棋冠军李世石，展示了AI在复杂博弈中的强大能力。</li><li><strong>自然语言处理</strong>：GPT-3等大型语言模型的推出，使得AI能够生成逼真的自然语言文本，应用于翻译、对话系统、内容创作等多个领域。</li></ul></li></ol><h2 id="2-机器学习-与-深度学习"><a href="#2-机器学习-与-深度学习" class="headerlink" title="2. 机器学习 与 深度学习"></a>2. 机器学习 与 深度学习</h2><p>机器学习（Machine Learning）和深度学习（Deep Learning）是现代人工智能（Artificial Intelligence）领域的核心技术。它们的发展极大地推动了从图像识别到自然语言处理的各种应用。</p><h3 id="21-机器学习machine-learning"><a href="#2-1-机器学习（Machine-Learning）" class="headerlink" title="2.1 机器学习（Machine Learning）"></a>2.1 机器学习（Machine Learning）</h3><p>机器学习是一种使计算机能够通过经验自动改进的技术。它依赖于算法和统计模型，使得计算机系统可以识别数据中的模式并做出决策，无需明确编程。</p><p><strong>主要类别</strong>：</p><ol><li><strong>监督学习（Supervised Learning）</strong>：模型在带有标签的数据集上进行训练，学习输入与输出之间的映射关系。应用包括分类（Classification）和回归（Regression）。</li><li><strong>无监督学习（Unsupervised Learning）</strong>：模型在没有标签的数据集上工作，目标是发现数据的内在结构。常见的任务有聚类（Clustering）和降维（Dimensionality Reduction）。</li><li><strong>半监督学习（Semi-supervised Learning）</strong>：使用部分标记的数据进行训练，结合监督学习和无监督学习的特点。</li><li><strong>强化学习（Reinforcement Learning）</strong>：模型通过与环境的交互学习策略，目标是最大化某种数值奖励（Reward）。</li></ol><h3 id="22-深度学习deep-learning"><a href="#2-2-深度学习（Deep-Learning）" class="headerlink" title="2.2 深度学习（Deep Learning）"></a>2.2 深度学习（Deep Learning）</h3><p>深度学习是机器学习中的一个子集，它使用称为人工神经网络（Artificial Neural Networks）的模型，特别是具有多个层（Layers）的深层网络，以学习数据的高级抽象特征。</p><p><strong>核心概念</strong>：</p><ul><li><strong>神经网络（Neural Networks）</strong>：一个由节点（或称为神经元，Neurons）组成的网络，节点在层中组织并通过激活函数（Activation Functions）处理信息。</li><li><strong>卷积神经网络（Convolutional Neural Networks, CNNs）</strong>：特别适合处理图像数据。</li><li><strong>循环神经网络（Recurrent Neural Networks, RNNs）</strong>：优秀的处理序列数据如时间序列或自然语言的工具。</li><li><strong>长短期记忆网络（Long Short-Term Memory, LSTM）</strong>和<strong>门控循环单元（Gated Recurrent Units, GRU）</strong>：是RNN的变体，解决了传统RNN长期依赖问题。</li><li><strong>Transformer</strong>：一种基于自注意力机制（Self-attention Mechanism）的架构，广泛用于自然语言处理领域。</li></ul><h3 id="23-区别与联系"><a href="#2-3-区别与联系" class="headerlink" title="2.3  区别与联系"></a>2.3  区别与联系</h3><p><strong>联系</strong>：</p><ul><li>深度学习是机器学习的一种特殊形式，利用复杂的神经网络结构来解决广泛的问题。</li><li>两者都依赖数据来学习，并通过迭代过程改进模型性能。</li></ul><p><strong>区别</strong>：</p><ul><li>机器学习包括一系列不仅限于神经网络的技术和方法。</li><li>深度学习通常需要更大量的数据和更强的计算能力。</li></ul><h2 id="3-神经网络-neural-networks"><a href="#3-神经网络-Neural-Networks" class="headerlink" title="3. 神经网络 (Neural Networks)"></a>3. 神经网络 (Neural Networks)</h2><p>神经网络是实现AI 的一种技术手段，一种广泛用于机器学习（Machine Learning）和深度学习（Deep Learning）领域的计算模型/算法架构。</p><p>它受到人类大脑神经元（Neurons）和它们的互动方式的启发，它由多个层（Layers）组成，每层包含多个神经元，这些神经元通过权重（Weights）连接传递信息。</p><p>神经网络的训练过程基于机器学习的基本前提，即能够从数据中学习。通过向网络提供大量的数据样本（包括输入和期望的输出），神经网络可以学习到如何映射输入到输出，这种能力是通过调整内部结构（即权重）来实现的。</p><p>这一学习过程使用了机器学习中的核心概念，如损失函数（Loss Functions）、梯度下降（Gradient Descent）和反向传播算法（Backpropagation Algorithms）。这些都是机器学习领域的基本工具，用于训练模型以改进其性能。</p><h2 id="4-transformer"><a href="#4-Transformer" class="headerlink" title="4. Transformer"></a>4. Transformer</h2><p>Transformer（变换器）是一种革命性的神经网络架构，它在自然语言处理（Natural Language Processing, NLP）和其他序列建模任务中取得了显著的成就。Transformer 最初由 Vaswani 等人在 2017 年的论文 “Attention Is All You Need” 中提出，其核心思想是完全依靠注意力机制（attention mechanisms），摒弃了之前常用的循环神经网络（Recurrent Neural Networks, RNNs）和卷积神经网络（Convolutional Neural Networks, CNNs）中的结构。</p><h2 id="5-nlp"><a href="#5-NLP" class="headerlink" title="5. NLP"></a>5. NLP</h2><p>自然语言处理（Natural Language Processing，简称NLP）是人工智能的一个分支，致力于让计算机理解、解释和生成人类语言。它结合了计算机科学、人工智能和语言学的知识与技术，用于处理和分析大量的自然语言数据。</p><p>NLP使用各种技术来处理和理解语言。这些方法从规则基础的方法到基于机器学习的方法，特别是深度学习，都有涵盖。随着时间的推移，深度学习在NLP中变得越来越重要，因为它能够在处理自然语言的复杂性方面提供显著的改进。</p><p><strong>传统方法：</strong></p><ul><li><strong>基于规则的系统</strong>：使用预定义的语言规则来解释文本。</li><li><strong>统计模型</strong>：基于大量语料库数据，使用统计方法推断和预测。</li></ul><p><strong>现代方法</strong></p><ul><li><strong>神经网络</strong>：使用多层神经网络模型处理语言任务，如循环神经网络（RNN）、长短期记忆网络（LSTM）和最近的变换器模型（如BERT、GPT）。</li></ul><p>NLP技术被广泛应用于各种现实世界的场景和产品中，例如：</p><ul><li><strong>聊天机器人</strong>和<strong>虚拟助手</strong>（如Apple的Siri、Google Assistant、Amazon Alexa）。</li><li><strong>文本分析工具</strong>，帮助企业监测和分析社交媒体上的消费者情绪。</li><li><strong>电子邮件过滤</strong>和<strong>反垃圾邮件技术</strong>。</li><li><strong>语音到文本服务</strong>，如在法庭记录或医疗记录系统中自动转录口述内容。</li></ul><h2 id="5-llm"><a href="#5-LLM" class="headerlink" title="5. LLM"></a>5. LLM</h2><p>LLM（Large Language Models，大型语言模型）是自然语言处理（Natural Language Processing, NLP）领域的一种先进技术，主要依赖于深度学习（Deep Learning）技术，特别是基于 Transformer 架构的神经网络模型。这些模型因其规模庞大和在多种语言任务上的出色表现而得名。</p><p>LM 是设计用来理解、生成、翻译、摘要等处理文本的大规模神经网络模型。它们通常包含数十亿至数万亿个参数，并在大量多样化的文本数据上进行训练，以学习语言的深层次结构和语义。<br><strong>核心组件</strong></p><ol><li><strong>Transformer 架构（Transformer Architecture）</strong>：<ul><li>LLM 多使用基于 Transformer 的模型，这种模型依靠自注意力机制（Self-Attention Mechanism）来处理文本数据，优于传统的循环神经网络（Recurrent Neural Networks, RNNs）或卷积神经网络（Convolutional Neural Networks, CNNs）。</li></ul></li><li><strong>预训练与微调（Pre-training and Fine-tuning）</strong>：<ul><li><strong>预训练（Pre-training）</strong>：在大规模未标记数据上进行，模型学习语言的通用特征。</li><li><strong>微调（Fine-tuning）</strong>：在特定任务的较小标记数据集上进行，调整模型以适应具体应用。</li></ul></li><li><strong>自监督学习（Self-supervised Learning）</strong>：<ul><li>LLM 通常通过自监督学习预训练，这意味着它们使用输入数据的不同部分作为自己的监督信号，例如，预测文本中被遮蔽（Masked）的单词。</li></ul></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;在学习AI 过程中，发现专业名词相当多，初学者可能会感到而混乱，所以本篇内容是对该领域内的一些“大词”进行简单介绍， 做一些概念扫盲， 以保证在接下来的学习中心中有框架。 正式内容会按照下图框架介绍&lt;/p&gt;
&lt;img src=&quot;/81fcff81/1.png&quot; class&gt;</summary>
      
    
    
    
    
    <category term="AI" scheme="http://example.com/tags/AI/"/>
    
    <category term="chatGPT" scheme="http://example.com/tags/chatGPT/"/>
    
  </entry>
  
  <entry>
    <title>TrustMessage-基于2PC+MySQL+泛化调用实现的可靠消息中心</title>
    <link href="http://example.com/99d433fa/"/>
    <id>http://example.com/99d433fa/</id>
    <published>2024-04-09T08:41:05.000Z</published>
    <updated>2024-05-08T14:06:28.633Z</updated>
    
    <content type="html"><![CDATA[<h1 id="0-项目结构介绍"><a href="#0-项目结构介绍" class="headerlink" title="0. 项目结构介绍"></a>0. 项目结构介绍</h1><div class="table-container"><table><thead><tr><th>Module</th><th>Description</th></tr></thead><tbody><tr><td>trustmessage-mysql</td><td>基于2PC+MySQL表实现的可靠消息中心，业务操作+消息表操作均存在于同一个项目中</td></tr><tr><td>turstmessage-middleware</td><td>可靠消息中心中间件，基于RPC接口提交消息+2PC+MySQL 表实现</td></tr><tr><td>turstmessage-middlewareapi</td><td>可靠消息中心中间件， 回查接口定义</td></tr><tr><td>Turstmessage-middlewareclient</td><td>可靠消息中心中间件， 消息生产者，提供了HTTP回查接口、Dubbo泛化回查接口的示例</td></tr></tbody></table></div><p>以下是项目正式介绍。</p><p>在业务处理中，经常会有重要但没那么紧急的数据需要同步给下游，比如</p><ol><li>订单侧完成消息后给优惠侧发一个消息，优惠侧做一个单向对账的功能，确保券被正确核销</li></ol><p>在这种场景中，需要把本地业务操作 + 消息发送当成一个事务处理，即满足原子性， 一般常见的解决方案会有两种</p><ol><li>本地事务+本地消息表</li><li>RocketMQ</li></ol><p>本项目将从本地事务+本地消息表 出发， 一步步探讨如何用 MySQL  实现一个支持分布式事务的可靠消息中心，即TrustMessage。</p><p><a href="https://github.com/sysunyan1699/TrustMessage">项目github链接，点击可查看代码</a></p><h1 id="1-本地事务-本地消息表"><a href="#1-本地事务-本地消息表" class="headerlink" title="1. 本地事务+ 本地消息表"></a>1. 本地事务+ 本地消息表</h1><p>由于Spring 的事务机制只保证数据库操作的原子性，所以当涉及到 数据库的业务操作 和 其他中间件如kafka操作 具有原子性的时候，就要用其他的方案来保证。</p><p>本地事务+ 本地消息表 这种方案是把 需要发送的消息作为数据库操作的一部分，保存到数据库中的一个表里，然后通过另外的逻辑，将消息的真正发送 稍后异步进行，比如用一个定时任务将消息异步发送到Kafka。</p><p>这种方法确保了数据库操作和消息发送在<code>逻辑语义上的原子性</code>，因为它们都在同一个数据库事务中处理。</p><p>这里需要注意，这种方案的实时性是比较差的，所以你需要判断的业务场景场景是否能够容忍这样的异步操作。</p><h2 id="11-业务流程"><a href="#1-1-业务流程" class="headerlink" title="1.1 业务流程"></a>1.1 业务流程</h2><img src="/99d433fa/1.png" class><p>以上流程中，在本地事务提交后，有一个定时任务轮询消息表将需要发送的消息消息发送出去。有4个点需要注意一下</p><ol><li>事务提交后了，消息发送失败， 定时任务的重试机制，会找出这条消息进行异步补发 </li><li>事务提交后了，消息发送成功，但是消息状态修改状态， 定时任务会找出这条再次发送</li><li>重试异步补发过程中，如果消息依然发送失败，那么会继续重试补发</li><li>重试异步补发过程中，消息发送成功，但是数据库消息已发送状态修改失败，那么定时任务又会再次找到这条消息再发一遍</li></ol><p>以上 2和4 均会面临消息重复的情况， 个人认为在业务常见中消息重复是一种可接受的情况，有时候业务自己甚至会消息重放， 所以消息消费者做好幂等逻辑就可以了。</p><h2 id="12-消息发送重试次数"><a href="#1-2-消息发送重试次数" class="headerlink" title="1.2 消息发送重试次数"></a>1.2 消息发送重试次数</h2><p>消息发送不能无限次重试</p><ol><li><p>浪费资源，重试了那么多次都未成功，可能是逻辑出现问题了或者宕机了，赶紧去查问题吧</p></li><li><p>上下游业务数据迟迟无法达到最终一致性 ， 本身我们使用消息其终极目的就是为了让系统数据达到最终一致性， 如果一直无限制发送，这个目的是无法达到的, 所以赶紧停下去查问题吧</p></li></ol><p>基于以上两个考虑，系统对于重试都应该有个次数限制，达到次数限制后就应该告警让人工介入处理。</p><h2 id="13-消息表设计"><a href="#1-3-消息表设计" class="headerlink" title="1.3 消息表设计"></a>1.3 消息表设计</h2><p>在本地事务+ 本地消息表 方案中，其消息表的设计一般如下，</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> message (</span><br><span class="line">id <span class="type">bigint</span> unsigned <span class="keyword">NOT</span> <span class="keyword">NULL</span> AUTO_INCREMENT,</span><br><span class="line">message text COMMENT <span class="string">&#x27;消息内容&#x27;</span>,</span><br><span class="line">send_status <span class="type">INT</span> <span class="keyword">DEFAULT</span> <span class="number">0</span> COMMENT <span class="string">&#x27;0-投递中 1-投递成功 2-投递失败&#x27;</span>,</span><br><span class="line">send_try_count <span class="type">INT</span> <span class="keyword">DEFAULT</span> <span class="number">0</span> COMMENT <span class="string">&#x27;commit 消息发送 当前重试次数&#x27;</span>,</span><br><span class="line">send_next_retry_time DATETIME <span class="keyword">NOT</span> <span class="keyword">NULL</span> <span class="keyword">DEFAULT</span> <span class="built_in">CURRENT_TIMESTAMP</span> COMMENT <span class="string">&#x27;消息发送 下次重试时间&#x27;</span>,</span><br><span class="line">create_time DATETIME <span class="keyword">DEFAULT</span> <span class="built_in">CURRENT_TIMESTAMP</span>,</span><br><span class="line">update_time DATETIME <span class="keyword">DEFAULT</span> <span class="built_in">CURRENT_TIMESTAMP</span> <span class="keyword">ON</span> <span class="keyword">UPDATE</span> <span class="built_in">CURRENT_TIMESTAMP</span>,</span><br><span class="line"><span class="keyword">PRIMARY</span> KEY (id),</span><br><span class="line"><span class="keyword">UNIQUE</span> INDEX idx_messageKey(message_key)</span><br><span class="line">) ENGINE<span class="operator">=</span>InnoDB;</span><br></pre></td></tr></table></figure><h1 id="2-如果消息表和业务表操作是分布式事务"><a href="#2-如果消息表和业务表操作是分布式事务" class="headerlink" title="2. 如果消息表和业务表操作是分布式事务"></a>2. 如果消息表和业务表操作是分布式事务</h1><p>但是如果保证不了这两个表不在同一个库 /数据库实例中，那就会在业务操作和消息表写入两个操作中遇到分布式事务。这在分库分表的业务中是很容易出现的情况。</p><p>针对对分布式事务，常见的解决方案就是 2PC、3PC、TCC、SAGA。</p><p>接下来将讲解以 2PC+MySQL消息表 实现的可靠消息中心</p><h2 id="21-业务流程"><a href="#2-1-业务流程" class="headerlink" title="2.1 业务流程"></a>2.1 业务流程</h2><p>以MySQL消息表+ 2PC 来实现可靠消息中心， 其整体实现流程如下<br><img src="/99d433fa/2.png" class></p><h2 id="22-消息可见性"><a href="#2-2-消息可见性" class="headerlink" title="2.2 消息可见性"></a>2.2 消息可见性</h2><p>消息可见性， 在涉及分布式事务的场景中，消息增加了一个<code>可见性</code>概念， 这是因为在引入2PC 后，写入消息表的消息不再像本地事务+本地消息表一样<code>写入即可见</code>，必须是commit后才对消费者可见， 所以在数据表的设计中需要增加一个状态字段来维护消息可见性。<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">message_status INT COMMENT &#x27;消息状态 1-prepare 2-commit 3-rollback 4-unknown&#x27;,</span><br></pre></td></tr></table></figure></p><p>其状态流转如图所示<br><img src="/99d433fa/3.png" class></p><h2 id="23-如果业务执行消息commit-or-rollback-失败怎么办-消息回查"><a href="#2-3-如果业务执行消息commit-or-rollback-失败怎么办-消息回查" class="headerlink" title="2.3 如果业务执行消息commit or rollback 失败怎么办-消息回查"></a>2.3 如果业务执行消息commit or rollback 失败怎么办-消息回查</h2><p>如流程图中所示，在2PC 阶段，拿到业务执行结果修改消息状态失败有可能是失败。</p><p>一个操作执行失败后，一种常见的解决方案方案就是重试，尽最大努力交付。</p><p>但是对于业务处理来讲，一般有超时时间的限制，因为这种同步重试可能并不适用，即使可以，一般重试次数都会限定在3次。</p><p>除了同步重试，还有一种方案就是 消息回查，我个人理解这相当于一种异步重试。</p><p>在本项目中，消息回查指的就是开启一个定时任务去全表扫描，找出insert一定时间后，其状态仍然是 prepare的消息 ，通过业务逻辑判断该条消息是否已经执行完成 or 失败，对应地把消息状态更改为 commit or rollback。</p><p>为了进行消息回查，肯定要有一个业务唯一标识来识别该条消息需要对应业务数据，从而判断对应业务是否执行完成。<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">message_key VARCHAR(255) COMMENT &#x27;消息唯一键，用于做回查的标识&#x27;,</span><br></pre></td></tr></table></figure></p><h2 id="24-消息回查不能无限次"><a href="#2-4-消息回查不能无限次" class="headerlink" title="2.4 消息回查不能无限次"></a>2.4 消息回查不能无限次</h2><ol><li>浪费资源，回查了这么多次的都没拿到结果，一种可能就是业务逻辑出现问题了，适可而止赶紧去查问题吧</li><li>系统数据迟迟无法达到最终一致性 ， 本身我们使用消息其终极目的就是为了让系统数据达到最终一致性， 如果一直无限制查询，这个目的是无法达到的, 所以赶紧停下去查问题吧</li></ol><p>所有消息回查应该有个次数限制， 这就是表中以下两个字段的作用<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">verify_try_count <span class="type">INT</span> COMMENT <span class="string">&#x27;消息状态回查 下次重试次数&#x27;</span>,  </span><br><span class="line">verify_next_retry_time <span class="type">TIMESTAMP</span> <span class="keyword">DEFAULT</span> <span class="built_in">CURRENT_TIMESTAMP</span> COMMENT <span class="string">&#x27;消息状态回查 下次重试时间 1-未发送 2-已发送&#x27;</span>,</span><br></pre></td></tr></table></figure></p><h2 id="25-消息回查次数达到上限怎么办"><a href="#2-5-消息回查次数达到上限怎么办" class="headerlink" title="2.5 消息回查次数达到上限怎么办"></a>2.5 消息回查次数达到上限怎么办</h2><p>有两种参考方案</p><ol><li>默认修改消息状态为commit  或者 rollback，</li><li>将消息状态置为回查失败状态 ， 告警人工介入处理</li></ol><p>默认修改消息状态为commit  或者 rollback 这个方案，一个最大的问题就是针对状态不确定的消息，不论将其默认修改为那种状态， 都是有可能引起业务上下游数据不一致问题。</p><p>一旦上下游数据产生了数据不一致性，必然导致很长的排查链路和大量的数据修复工作。</p><p>所以本项目中我选择第二种方案，消息回查达到上限后直接告警，让消息生产者这一方人工介入处理。</p><p>此处说明一下，这种方案当然也会有数据不一致的问题，因为下游业务始终还未拿到消息修改自己的状态，但是相比拿到了随机确定的的状态 导致的数据不一致性，此时问题还被控制在消息生产者这一环，问题排查会相对简单。</p><h2 id="26-消息发送重试"><a href="#2-6-消息发送重试" class="headerlink" title="2.6 消息发送重试"></a>2.6 消息发送重试</h2><p>与本地事务+本地消息表方案一致</p><h2 id="27-消息表设计"><a href="#2-7-消息表设计" class="headerlink" title="2.7 消息表设计"></a>2.7 消息表设计</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> message (</span><br><span class="line">id <span class="type">bigint</span> unsigned <span class="keyword">NOT</span> <span class="keyword">NULL</span> AUTO_INCREMENT,</span><br><span class="line">message_key <span class="type">VARCHAR</span>(<span class="number">255</span>) COMMENT <span class="string">&#x27;消息唯一键，用于做回查的标识&#x27;</span>,</span><br><span class="line">message text COMMENT <span class="string">&#x27;消息内容&#x27;</span>,</span><br><span class="line">message_status <span class="type">INT</span> <span class="keyword">DEFAULT</span> <span class="number">1</span> COMMENT <span class="string">&#x27;消息状态 1-prepare 2-commit 3-rollback 4-unknown&#x27;</span>,</span><br><span class="line">verify_try_count <span class="type">INT</span> <span class="keyword">DEFAULT</span> <span class="number">0</span> COMMENT <span class="string">&#x27;消息状态回查 当前重试次数&#x27;</span>,</span><br><span class="line">verify_next_retry_time DATETIME <span class="keyword">NOT</span> <span class="keyword">NULL</span> <span class="keyword">DEFAULT</span> <span class="built_in">CURRENT_TIMESTAMP</span> COMMENT <span class="string">&#x27;消息状态回查 下次重试时间&#x27;</span>,</span><br><span class="line">send_status <span class="type">INT</span> <span class="keyword">DEFAULT</span> <span class="number">0</span> COMMENT <span class="string">&#x27;0-投递中 1-投递成功 2-投递失败&#x27;</span>,</span><br><span class="line">send_try_count <span class="type">INT</span> <span class="keyword">DEFAULT</span> <span class="number">0</span> COMMENT <span class="string">&#x27;commit 消息发送 当前重试次数&#x27;</span>,</span><br><span class="line">send_next_retry_time DATETIME <span class="keyword">NOT</span> <span class="keyword">NULL</span> <span class="keyword">DEFAULT</span> <span class="built_in">CURRENT_TIMESTAMP</span> COMMENT <span class="string">&#x27;消息发送 下次重试时间&#x27;</span>,</span><br><span class="line">create_time DATETIME <span class="keyword">DEFAULT</span> <span class="built_in">CURRENT_TIMESTAMP</span>,</span><br><span class="line">update_time DATETIME <span class="keyword">DEFAULT</span> <span class="built_in">CURRENT_TIMESTAMP</span> <span class="keyword">ON</span> <span class="keyword">UPDATE</span> <span class="built_in">CURRENT_TIMESTAMP</span>,</span><br><span class="line"><span class="keyword">PRIMARY</span> KEY (id),</span><br><span class="line"><span class="keyword">UNIQUE</span> INDEX idx_messageKey(message_key)</span><br><span class="line">) ENGINE<span class="operator">=</span>InnoDB;</span><br></pre></td></tr></table></figure><h2 id="28-消费者消费消息"><a href="#2-8-消费者消费消息" class="headerlink" title="2.8 消费者消费消息"></a>2.8 消费者消费消息</h2><p>针对可见， 即已经commit 的消息，消费者该如何获取到消息消费呢，有两种方案</p><ol><li>消息者直接查询消息表</li><li>消费者从消息队列队列消费</li></ol><h3 id="281-消息者轮训消息表"><a href="#2-8-1-消息者轮训消息表" class="headerlink" title="2.8.1 消息者轮训消息表"></a>2.8.1 消息者轮训消息表</h3><p>这种方案最大的问题就是，在微服务架构下，上下游两个不同的服务 操作 同一个数据表 是一个不合理且不推荐的做法。</p><h3 id="282-消息队列消费"><a href="#2-8-2-消息队列消费" class="headerlink" title="2.8.2 消息队列消费"></a>2.8.2 消息队列消费</h3><p>和本地事务+本地消息表一样，已经commit 的消息可以由一个定时任务轮训发送到业务创建的消息队列中供订阅的消费者消费<br>发送过程也可以有一个重试的过程。</p><h1 id="3-如果这是一个公共中间件-基于rpc-接口实现的可靠消息中心"><a href="#3-如果这是一个公共中间件-基于RPC-接口实现的可靠消息中心" class="headerlink" title="3. 如果这是一个公共中间件-基于RPC 接口实现的可靠消息中心"></a>3. 如果这是一个公共中间件-基于RPC 接口实现的可靠消息中心</h1><p>以上讨论的方案， 都是基于消息表逻辑和业务逻辑同一个服务中， 如果把该功能做成一个公共中间件，那么在技术方案上会略有变化。</p><p>中间件需要提供的功能</p><ol><li>两阶段提交功能</li><li>回查功能</li><li>消息转发</li></ol><p>以上3个功能和上一种方案没有本质上的区别， 只是基于一个中间件的定位，支持这3种功能需要更多的封装与数据信息。</p><h2 id="31-业务流程"><a href="#3-1-业务流程" class="headerlink" title="3.1 业务流程"></a>3.1 业务流程</h2><img src="/99d433fa/4.png" class><h2 id="32-两阶段提交功能"><a href="#3-2-两阶段提交功能" class="headerlink" title="3.2 两阶段提交功能"></a>3.2 两阶段提交功能</h2><p>提供3个RPC 接口， prepare， commit, rollback, 接口底层封装对数据表的操作</p><h2 id="33-消息唯一性"><a href="#3-3-消息唯一性" class="headerlink" title="3.3 消息唯一性"></a>3.3 消息唯一性</h2><p>当作为一个公共中间件，接受多个业务数据的时候，消息的唯一性应该有业务标识 + 消息标识共同确定，即bizId + messageKey</p><h2 id="34-回查功能"><a href="#3-4-回查功能" class="headerlink" title="3.4 回查功能"></a>3.4 回查功能</h2><p>相比于直接在业务服务里集成可靠消息的功能时，可以简单直接的在服务内部查询，当作为公共中间件时，  只能通过服务间调用完成，服务间调用有两种形式</p><ol><li>HTTP</li><li>RPC</li></ol><p>为了增加可维护性和拓展型， 无论是哪种形式，中间件都应该定义好调用的格式，让消息生产者按照统一格式提供回查接口。</p><p>这个格式包括</p><ol><li>接口定义</li><li>接口入参</li><li>接口返回值</li></ol><p>其中接口定义信息需要生产消息时提供</p><p>在实现消息生产者按照统一格式提供回查接口 这一点是，HTTP接口的回查相对简单， 如果RPC 接口， 要注意使用泛化调用。</p><p>本项目实现了HTTP 接口的回查和 Dubbo 协议的泛化调用回查</p><p>HTTP接口格式为<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://127.0.0.1:8082/verifyMessage?bizID=1&amp;messageKey=key1</span><br></pre></td></tr></table></figure></p><p>Dubbo RPC 接口定义为<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">public interface VerifyMessageService &#123;  </span><br><span class="line">  </span><br><span class="line">// 消息回查接口  </span><br><span class="line">int verifyMessage(Integer bizID,String messageKey);  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h2 id="35-消息转发"><a href="#3-5-消息转发" class="headerlink" title="3.5 消息转发"></a>3.5 消息转发</h2><p>在一个公共中间件里实现消息转发，必然也需要生产消息时提供这部分信息<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">forward_topic VARCHAR(255) COMMENT &#x27;业务转发topic&#x27;,  </span><br><span class="line">forward_key VARCHAR(255) COMMENT &#x27;业务转发指定key&#x27;,  </span><br></pre></td></tr></table></figure></p><h2 id="36-消息表设计"><a href="#3-6-消息表设计" class="headerlink" title="3.6 消息表设计"></a>3.6 消息表设计</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> message (</span><br><span class="line">id <span class="type">bigint</span> unsigned <span class="keyword">NOT</span> <span class="keyword">NULL</span> AUTO_INCREMENT,</span><br><span class="line">    biz_id <span class="type">INT</span> <span class="keyword">NOT</span> <span class="keyword">NULL</span> COMMENT <span class="string">&#x27;业务ID&#x27;</span>,</span><br><span class="line">    message_key <span class="type">VARCHAR</span>(<span class="number">255</span>) COMMENT <span class="string">&#x27;消息唯一键，用于做回查的标识&#x27;</span>,</span><br><span class="line">message text COMMENT <span class="string">&#x27;消息内容&#x27;</span>,</span><br><span class="line">message_status <span class="type">INT</span> <span class="keyword">DEFAULT</span> <span class="number">1</span> COMMENT <span class="string">&#x27;消息状态 1-prepare 2-commit 3-rollback 4-verify fail&#x27;</span>,</span><br><span class="line">    forward_topic <span class="type">VARCHAR</span>(<span class="number">255</span>) <span class="keyword">NOT</span> <span class="keyword">NULL</span> COMMENT <span class="string">&#x27;业务转发topic&#x27;</span>,</span><br><span class="line">forward_key <span class="type">VARCHAR</span>(<span class="number">255</span>) COMMENT <span class="string">&#x27;业务转发指定key&#x27;</span>,</span><br><span class="line">    verify_info <span class="type">VARCHAR</span>(<span class="number">2000</span>) COMMENT <span class="string">&#x27;回查信息&#x27;</span>,</span><br><span class="line">verify_try_count <span class="type">INT</span> <span class="keyword">DEFAULT</span> <span class="number">0</span> COMMENT <span class="string">&#x27;消息状态回查 当前重试次数&#x27;</span>,</span><br><span class="line">verify_next_retry_time DATETIME <span class="keyword">NOT</span> <span class="keyword">NULL</span> <span class="keyword">DEFAULT</span> <span class="built_in">CURRENT_TIMESTAMP</span> COMMENT <span class="string">&#x27;消息状态回查 下次重试时间&#x27;</span>,</span><br><span class="line">send_status <span class="type">INT</span> <span class="keyword">DEFAULT</span> <span class="number">0</span> COMMENT <span class="string">&#x27;0-投递中 1-投递成功 2-投递失败&#x27;</span>,</span><br><span class="line">send_try_count <span class="type">INT</span> <span class="keyword">DEFAULT</span> <span class="number">0</span> COMMENT <span class="string">&#x27;commit消息发送 当前重试次数&#x27;</span>,</span><br><span class="line">send_next_retry_time DATETIME  <span class="keyword">NOT</span> <span class="keyword">NULL</span> <span class="keyword">DEFAULT</span> <span class="built_in">CURRENT_TIMESTAMP</span> COMMENT <span class="string">&#x27;消息发送 下次重试时间&#x27;</span>,</span><br><span class="line">create_time DATETIME <span class="keyword">DEFAULT</span> <span class="built_in">CURRENT_TIMESTAMP</span>,</span><br><span class="line">update_time DATETIME <span class="keyword">DEFAULT</span> <span class="built_in">CURRENT_TIMESTAMP</span> <span class="keyword">ON</span> <span class="keyword">UPDATE</span> <span class="built_in">CURRENT_TIMESTAMP</span>,</span><br><span class="line"><span class="keyword">PRIMARY</span> KEY (id),</span><br><span class="line"><span class="keyword">UNIQUE</span> INDEX idx_message_key_biz_id (message_key, biz_id)</span><br><span class="line">) ENGINE<span class="operator">=</span>InnoDB;</span><br></pre></td></tr></table></figure><h1 id="4-基于kafka-提交消息实现的可靠事件中心"><a href="#4-基于kafka-提交消息实现的可靠事件中心" class="headerlink" title="4. 基于kafka 提交消息实现的可靠事件中心"></a>4. 基于kafka 提交消息实现的可靠事件中心</h1><p>在实现消息回查的可靠消息中心方案中，另外一种常见的方案是 业务代码直接把消息提交给kafka, 然后中间件消费消息并持久化道数据库中，等待消息提交commit 或者rollback , 没有的话就进行回查。如下图，图片源自极客时间专栏</p><img src="/99d433fa/5.png" class><p>我认为两种技术方案没有本质的区别， 其差异只是消息的prepare 、commit、rollback 的提交是由RPC 接口完成还是由消息生产消费完成， 其他回查的逻辑、发送逻辑、以及需要的信息基本无差异。</p><p>不过在使用Kafka 提交时，有以下两种需要考虑</p><h2 id="41-中间件如何识别一条消息是事务消息"><a href="#4-1-中间件如何识别一条消息是事务消息" class="headerlink" title="4.1 中间件如何识别一条消息是事务消息"></a>4.1 中间件如何识别一条消息是事务消息</h2><ol><li>Topic命名约定</li></ol><p>一种简单的方法是通过Topic命名来区分。例如，所有需要支持回查的Topic可以遵循一个特定的命名模式，如添加前缀或后缀（例如，<code>replayable-myTopic</code>）。这种方法的优点是简单易实施，但缺点是灵活性较低，且对现有系统可能需要更多的改动。</p><ol><li>特定主题或分区</li></ol><p>将需要回查的消息发送到Kafka的特定主题或分区中。这样，中间件只需监听这个特定的主题或分区来处理需要回查的消息。这种方法要求生产者在发送消息时知道哪些消息需要回查，并据此发送到正确的主题或分区。</p><ol><li>Topic配置属性</li></ol><p>Kafka允许为每个Topic设置自定义配置属性。可以引入一个自定义属性（如<code>replayable=true</code>）来标识一个Topic需要支持消息回查。这种方式比命名约定更为灵活和隐蔽，但要求应用层和消息生产者遵循这一约定，并且需要在应用层实现逻辑来处理这些属性。</p><ol><li>消息元数据标记</li></ol><p>在消息发送时，可以在消息的元数据（Metadata）中添加特定的标记或字段来指示这条消息需要进行回查。</p><p>设计考虑：</p><ul><li><strong>性能</strong>：确定这些方法中哪一种对生产和消费的性能影响最小。</li><li><strong>易用性</strong>：选择易于实施和维护的方法。</li><li><strong>灵活性</strong>：评估是否需要对单个消息进行标记，还是以Topic为单位进行区分。</li></ul><h2 id="42-如何识别消息类型-转发信息-回查信息"><a href="#4-2-如何识别消息类型、转发信息、回查信息" class="headerlink" title="4.2 如何识别消息类型、转发信息、回查信息"></a>4.2 如何识别消息类型、转发信息、回查信息</h2><p>消息类型包括 prepare、commit、rollback<br>转发信息,需要转发至的真正业务tpoic、 如果需要指定分区的话还包括key信息<br>回查信息，包括回查方式如HTTP、RPC, 回查地址，回查接口等</p><ol><li>使用Kafka消息头</li></ol><p><strong>优点</strong>：</p><ul><li>保持了消息体的纯净和独立性。</li><li>灵活性高，易于添加或修改额外的控制信息和元数据。</li><li>性能考虑，对于小到中等大小的消息，使用消息头的性能开销相对较小<br><strong>缺点</strong>：</li><li>新版本依赖：较旧版本的Kafka客户端可能不支持消息头功能，这要求生产者和消费者使用支持消息头的Kafka版本。</li><li><strong>额外处理</strong>：消费者需要额外的步骤来读取和解析消息头。</li></ul><ol><li>预先定义消息格式</li></ol><p><strong>优点</strong>：</p><ul><li>直接且简单，易于实现。</li><li>不依赖Kafka特定的功能，具有较好的兼容性。<br><strong>缺点</strong>：</li><li>增加了消息体的大小。</li><li>需要在消费端进行消息解析，略微增加了处理的复杂性。</li></ul><p>本项目以指定topic+预定义消息格式的方式简单实现了消息的提交，消息格式如下， 大家可以参考。<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">package com.example.trustmessage.middlewareapi.common;</span><br><span class="line">public class MiddlewareMessage &#123;  </span><br><span class="line">  </span><br><span class="line">// 要给到业务方的真正消息  </span><br><span class="line">private String message;  </span><br><span class="line"></span><br><span class="line">private int bizID; </span><br><span class="line">  </span><br><span class="line">// 用于消息回查的业务唯一标识  </span><br><span class="line">private String messageKey;  </span><br><span class="line"> </span><br><span class="line">private int messageStatus;  </span><br><span class="line">  </span><br><span class="line">private String forwardTopic;  </span><br><span class="line">  </span><br><span class="line">// 向业务方转发时需要指定的key，没有则说明按照kafka 默认分区策略进行分区  </span><br><span class="line">private String forwardKey;  </span><br><span class="line">  </span><br><span class="line">private VerifyInfo verifyInfo;  </span><br><span class="line">   </span><br><span class="line">public static class VerifyInfo &#123;  </span><br><span class="line">private int protocolType; // 1-http, 2-rpc-dubbo  </span><br><span class="line">private String registryProtocol;  </span><br><span class="line">private String registryAddress;  </span><br><span class="line">private String url;  </span><br><span class="line">private String version;   </span><br><span class="line">&#125;   </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h1 id="5-基于rpc接口-vs-基于kafka提交"><a href="#5-基于RPC接口-vs-基于Kafka提交" class="headerlink" title="5. 基于RPC接口 vs 基于Kafka提交"></a>5. 基于RPC接口 vs 基于Kafka提交</h1><p>基于两种不同消息提交方式实现的中间件， 将从以下两方面进行比较</p><ol><li>消息的顺序性</li><li>流量增加后扩容</li></ol><h2 id="51-消息的顺序性"><a href="#5-1-消息的顺序性" class="headerlink" title="5.1 消息的顺序性"></a>5.1 消息的顺序性</h2><p>使用中间件回查机制，由于网络原因，有可能出现 某条业务的commit or rollback 消息比prepare 先到达中间件，面对这种情况,commit or rollback的处理逻辑是需要报错的，client 只能重试或者等待回查机制更新消息状态</p><p>但是由于kafka 可以在一个分区内的保证消息的有序性，所以基于Kafka提交的方案可以有一种优雅的方式保证prepare消息和commit/rollback 消息的有序性。</p><p>解决方案很简单，生产者在发送消息按照业务 唯一标识指定key ,即指定目标分区即可。</p><h2 id="52-流量增加后扩容"><a href="#5-2-流量增加后扩容" class="headerlink" title="5.2 流量增加后扩容"></a>5.2 流量增加后扩容</h2><p>以下比较基于在代码层面已经做好分库分表、异步处理、批量处理、cache 等性能优化的基础上</p><p>假设已经分库分表，数据库处理不是瓶颈<br>万一流量激增，基于Kafka提交的方案 可能会产生产生必须要处理的消息积压，针对消息积压常见的解决方案中</p><ol><li>增加消费者数量，不过一般来讲，线上生产环境都会已经把消费者数量和分区数量设置成一样的，所以这个方案无法发挥功能</li><li>增加分区数量，假设公司的工作流程里允许增加，如果使用场景对消息顺序性有要求，你又要考虑新增分区后对消息顺序性的影响</li><li>新建一个更多分区的topic, 涉及到生产者、消费者的代码变更</li><li>消费者性能优化， 比如异步处理、批量处理， 但是如果项目已经做好这些措施，面对消息积压，只能回到下面3种方式</li></ol><p>综合以上，我个人认为基于RPC接口的方案可以用<code>自动扩容策略</code>直接应对， 简单直接优雅。</p><h1 id="6-作为中间件的技术设计"><a href="#6-作为中间件的技术设计" class="headerlink" title="6. 作为中间件的技术设计"></a>6. 作为中间件的技术设计</h1><h2 id="61-性能提升"><a href="#6-1-性能提升" class="headerlink" title="6.1 性能提升"></a>6.1 性能提升</h2><ol><li>线程池异步处理</li><li>cache 存储回查接口</li><li>基于bizID + messageKey 的分库分表</li></ol><h2 id="62-幂等性"><a href="#6-2-幂等性" class="headerlink" title="6.2 幂等性"></a>6.2 幂等性</h2><ol><li>prepare 消息的幂等性， 唯一索引</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;0-项目结构介绍&quot;&gt;&lt;a href=&quot;#0-项目结构介绍&quot; class=&quot;headerlink&quot; title=&quot;0. 项目结构介绍&quot;&gt;&lt;/a&gt;0. 项目结构介绍&lt;/h1&gt;&lt;div class=&quot;table-container&quot;&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;</summary>
      
    
    
    
    
    <category term="事务" scheme="http://example.com/tags/%E4%BA%8B%E5%8A%A1/"/>
    
  </entry>
  
  <entry>
    <title>深入解析bloom filter的原理与实现</title>
    <link href="http://example.com/b4fa673f/"/>
    <id>http://example.com/b4fa673f/</id>
    <published>2024-03-31T08:16:17.000Z</published>
    <updated>2024-05-02T01:35:18.938Z</updated>
    
    <content type="html"><![CDATA[<h1 id="0什么场景下会用到bloom-filter"><a href="#0-什么场景下会用到bloom-filter" class="headerlink" title="0.什么场景下会用到bloom filter"></a>0.什么场景下会用到bloom filter</h1><ol><li>缓存穿透</li><li>爬虫重复 URL 检测， 避免爬虫过程形成环</li><li>假设有 10 亿条手机号，然后判断某条手机号是否在列表内</li><li><a href="https://redis.com/blog/redis-redisbloom-bloom-filters/?_ga=2.218233113.1633975579.1711521515-926213951.1711521515&amp;_gl=1*538r6b*_ga*OTI2MjEzOTUxLjE3MTE1MjE1MTU.*_ga_8BKGRQKRPV*MTcxMTU4OTkzMi4zLjEuMTcxMTU5MzQyMy41NS4wLjA.*_gcl_au*OTkyMjAyNDAyLjE3MTE1MjE1MTQ">唯一昵称判断 </a></li></ol><p>这些场景可以用什么方式解决</p><ol><li>hashmap, hashset</li><li>MySQL：正常情况下，如果数据量不大，我们可以考虑使用 mysql 存储。将所有数据存储到数据库，然后每次去库里查询判断是否存在。但是如果数据量太大，超过千万，mysql 查询效率是很低的，特别消耗性能。</li><li>bitmap</li><li>bloom filter</li></ol><h1 id="1bloom-filter-是什么"><a href="#1-bloom-filter-是什么？" class="headerlink" title="1.bloom filter 是什么？"></a>1.bloom filter 是什么？</h1><p>布隆过滤器是一种概率性数据结构，它提供了一种空间效率极高的方法来测试一个元素是否属于一个集合。</p><p>其基本原理是使用多个不同的哈希函数对元素进行哈希，然后将得到的哈希值对应到位数组上。一个元素被加入到集合中，那么所有哈希函数计算出的位置都会被置为1。检查元素是否存在于集合中时，使用这些哈希函数计算哈希值，并检查对应的位是否都是1。如果都是1，那么元素可能存在于集合中；如果任何一个位不是1，那么元素肯定不在集合中。</p><p>其主要特点是：</p><ul><li><strong>高空间效率</strong>：相比于传统的集合数据结构，布隆过滤器使用极少的空间来处理大量数据。</li><li><strong>误报率/假阳</strong>：布隆过滤器有一定的误报概率，这意味着它可能会错误地认为某个不在集合中的元素存在于集合中。</li><li><strong>零漏报率</strong>：不会遗漏集合中真正存在的元素</li><li><strong>不可删除</strong>：标准的布隆过滤器不支持从集合中删除元素，尽管存在变种（如计数布隆过滤器）支持这一操作。</li><li><strong>多哈希函数</strong>：布隆过滤器通过多个哈希函数来减少误报率，每个元素被多个哈希函数映射到位数组的多个位置。</li></ul><h2 id="11-为什么空间效率高"><a href="#1-1-为什么空间效率高" class="headerlink" title="1.1 为什么空间效率高"></a>1.1 为什么空间效率高</h2><p>bloom filter 采用 位数组（bit array）作为核心的数据结构。</p><p>位数组是一个非常紧凑的数据结构，它可以有效地表示大量的布尔值（true或false），每个值只占用一个位（bit），而不是使用更传统的数据类型会占用更多的空间。</p><p>比如在爬虫场景中，假设有1亿个URL，每个URL算4字节, 如果用hashmap 实现，一个URL所占空间至少4bytes;如果用位数组实现，每个URL 所占的空间仅1bit，空间效率提升了32倍（存储空间不考虑误判率的前提下）。不可谓不高效。</p><h2 id="12-为什么会有误报率假阳"><a href="#1-2-为什么会有误报率-假阳" class="headerlink" title="1.2 为什么会有误报率/假阳"></a>1.2 为什么会有误报率/假阳</h2><p>既然用到了哈希函数，肯定会遇到哈希冲突。所以一个元素对应 的n  个位置， 可能因为其他元素的哈希冲突 而导致判断时发现等于1， 从而产生假阳现象。</p><h2 id="13-误报如何解决"><a href="#1-3-误报如何解决" class="headerlink" title="1.3 误报如何解决"></a>1.3 误报如何解决</h2><p>假阳问题无法被避免，只能尽可能减少。 减少的途径是选择合适的哈希函数以及指定合适的空间大小</p><h2 id="14-为什么需要多个哈希函数"><a href="#1-4-为什么需要多个哈希函数" class="headerlink" title="1.4 为什么需要多个哈希函数"></a>1.4 为什么需要多个哈希函数</h2><p>布隆过滤器的设计使用多个哈希函数来解决单个哈希函数可能带来的局限性，提高其效率和准确性。具体来说，使用多个哈希函数的原因包括：</p><ul><li><p>降低误报率</p><p>通过使用多个哈希函数独立地映射每个元素到位数组中的多个位置，并在所有这些位置上标记为1，可以显著降低不同元素映射到相同位置（即产生冲突）的概率，从而降低误报率</p></li><li><p>均匀分布</p><p>多个哈希函数可以将元素更均匀地分布在位数组上，减少了集中冲突的可能性。如果只使用一个哈希函数，即使其分布性质良好，也难以保证对于所有可能的输入集合都能保持良好的均匀性。多个哈希函数的组合，如果设计得当，可以相互补偿，实现更为均匀的分布。</p></li></ul><h2 id="15-为什么数据不可删除"><a href="#1-5-为什么数据不可删除" class="headerlink" title="1.5 为什么数据不可删除"></a>1.5 为什么数据不可删除</h2><p>在布隆过滤器中，元素的存在是通过多个位的“1”状态来表示的，而将这些位重置为“0”可能会错误地影响其他元素的存在检测。</p><p>如果需要支持删除，可以考虑使用 变体Bloom Filter， 如cuckoo filter</p><h1 id="2-bitmap-和-bloom-filter-的区别"><a href="#2-bitmap-和-bloom-filter-的区别" class="headerlink" title="2. bitmap 和 bloom filter 的区别"></a>2. bitmap 和 bloom filter 的区别</h1><p>Bitmap（位图）和布隆过滤器都是使用空间效率高的数据结构，它们通过利用位操作来实现存储和查询，但它们的设计目的和应用场景有所不同。</p><h2 id="21-bitmap位图"><a href="#2-1-Bitmap（位图）" class="headerlink" title="2.1 Bitmap（位图）"></a>2.1 Bitmap（位图）</h2><p>位图是一种数据结构，用于高效地存储和查询状态信息。在位图中，每个元素的存在或状态是由单独的位来表示的，即使用1位二进制数（0或1）来表示每个元素是否存在或某种特定状态。</p><p><strong>主要特点和用途</strong>：</p><ul><li><strong>简单直接</strong>：适用于需要追踪大量元素（如整数）存在与否的场景。</li><li><strong>空间效率</strong>：对于大规模数据集，位图使用的空间远小于传统的数据结构（如数组或列表）。</li><li><strong>随机访问</strong>：可以非常快速地检查任何一个元素的存在与否或状态。</li><li><p><strong>固定大小</strong>：位图的大小在创建时由最大元素值决定，因此其空间效率依赖于数据的分布。</p></li><li><p><strong>BitMap 的实现</strong><br>java BitSet<br>redis setbit、getbit</p></li></ul><h2 id="22-区别"><a href="#2-2-区别" class="headerlink" title="2.2 区别"></a>2.2 区别</h2><ul><li><strong>用途</strong>：位图主要用于精确表示一个大型数据集中元素的存在与否或状态信息，而布隆过滤器用于以极小的空间成本判断元素是否可能存在于集合中。</li><li><strong>错误率</strong>：位图提供了100%准确的结果（假设足够的空间来表示所有元素），而布隆过滤器允许一定的误报率。</li><li><strong>操作</strong>：位图支持添加、查询和删除（通过位反转）操作，而标准布隆过滤器不支持删除操作。</li><li><strong>空间效率与数据规模</strong>：布隆过滤器在表示大型集合成员资格时通常比位图更加空间效率，尤其是当元素范围非常大但实际元素数量相对较少时。</li></ul><p>总之，位图和布隆过滤器各有优势和应用场景，选择哪种数据结构取决于具体需求，包括对空间效率、准确率和操作类型的要求。</p><h1 id="3-单机版本guava-cache-源码解析"><a href="#3-单机版本Guava-Cache-源码解析" class="headerlink" title="3. 单机版本Guava Cache 源码解析"></a>3. 单机版本Guava Cache 源码解析</h1><h2 id="31-create"><a href="#3-1-create" class="headerlink" title="3.1 create"></a>3.1 create</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> &lt;T&gt; BloomFilter&lt;T&gt; <span class="title function_">create</span><span class="params">(Funnel&lt;T&gt; funnel, <span class="type">int</span> expectedInsertions <span class="comment">/* n */</span>,  </span></span><br><span class="line"><span class="params">    <span class="type">double</span> falsePositiveProbability)</span> &#123;  </span><br><span class="line">    <span class="type">int</span> <span class="variable">numBits</span> <span class="operator">=</span> optimalNumOfBits(expectedInsertions, falsePositiveProbability);  </span><br><span class="line">    <span class="type">int</span> <span class="variable">numHashFunctions</span> <span class="operator">=</span> optimalNumOfHashFunctions(expectedInsertions, numBits);  </span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">BloomFilter</span>&lt;T&gt;(<span class="keyword">new</span> <span class="title class_">BitArray</span>(numBits), numHashFunctions, funnel,  </span><br><span class="line">BloomFilterStrategies.MURMUR128_MITZ_32);  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="311-参数解释"><a href="#3-1-1-参数解释" class="headerlink" title="3.1.1 参数解释"></a>3.1.1 参数解释</h3><ul><li><p><strong><code>funnel</code></strong>：<code>Funnel</code>类型的参数，用于将任意类型的数据转化成布隆过滤器内部使用的一种形式。<code>Funnel</code>定义了如何把对象转换成二进制流，然后布隆过滤器使用这个二进制流来计算元素的哈希值。</p></li><li><p><strong><code>expectedInsertions</code></strong>：这个参数指定了预期要插入布隆过滤器的元素数量。这个数值是为了优化布隆过滤器内部数据结构的大小。</p></li><li><p><strong><code>falsePositiveProbability</code></strong>（false positive probability）：误判率。这是指一个不存在集合中的元素被判断为存在的概率。值得注意的是，随着实际插入数量的增加，实际的误判率可能会上升。</p></li></ul><h3 id="312-optimalnumofbits"><a href="#3-1-2-optimalNumOfBits" class="headerlink" title="3.1.2 optimalNumOfBits"></a>3.1.2 optimalNumOfBits</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">static</span> <span class="type">int</span> <span class="title function_">optimalNumOfBits</span><span class="params">(<span class="type">int</span> expectedInsertions, <span class="type">double</span> falsePositiveProbability)</span> &#123;  </span><br><span class="line"><span class="keyword">return</span> (<span class="type">int</span>) (-n * Math.log(p) / LN2_SQUARED);  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>optimalNumOfBits</code>通过计算,可以得到一个在满足特定假阳性率（<code>falsePositiveProbability</code>）要求下，对于给定数量的元素预期插入量（<code>expectedInsertions</code>），所需的最少位数。这使得布隆过滤器能够在保证误判率的前提下，使用最少的空间。这种计算对于设计高效且空间节约的布隆过滤器至关重要</p><h3 id="313-计算逻辑"><a href="#3-1-3-计算逻辑" class="headerlink" title="3.1.3 计算逻辑"></a>3.1.3 计算逻辑</h3><p><code>optimalNumOfBits</code>函数的计算基于以下公式，这个公式可以从布隆过滤器的理论误判率公式推导而来：<br><img src="/b4fa673f/bitnums.png" class></p><ul><li>m 是位数组的长度（即函数的返回值）。</li><li>n 是<code>expectedInsertions</code>，即预期的插入数量。</li><li>p 是<code>falsePositiveProbability</code>，即期望的假阳性概率。</li><li>ln⁡2 表示自然对数。<br>这个公式利用了布隆过滤器的误判率特性，通过指定的假阳性率和预期插入数量来计算出一个最优的位数组长度。这个长度能够在满足误判率要求的同时，尽可能地减小布隆过滤器所需的空间。</li></ul><h3 id="314-实现注意"><a href="#3-1-4-实现注意" class="headerlink" title="3.1.4 实现注意"></a>3.1.4 实现注意</h3><p>实际实现时，可能还需要对计算结果进行取整处理，并确保结果是一个正整数。此外，实现可能还会考虑到性能和存储效率的平衡，比如通过限制位数组的长度为2的幂等。</p><h3 id="315-optimalnumofhashfunctions"><a href="#3-1-5-optimalNumOfHashFunctions" class="headerlink" title="3.1.5 optimalNumOfHashFunctions"></a>3.1.5 optimalNumOfHashFunctions</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">static</span> <span class="type">int</span> <span class="title function_">optimalNumOfHashFunctions</span><span class="params">(<span class="type">int</span> n, <span class="type">int</span> m)</span> &#123;  </span><br><span class="line"><span class="keyword">return</span> Math.max(<span class="number">1</span>, (<span class="type">int</span>) Math.round(m / n * LN2));  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>optimalNumOfHashFunctions(expectedInsertions, numBits)</code>这个函数用于计算给定条件下布隆过滤器的最优哈希函数数量。这个计算基于预期要插入的元素数量（<code>expectedInsertions</code>）和布隆过滤器内部位数组的大小（<code>numBits</code>）。目的是为了平衡空间使用和误判率，确保布隆过滤器在给定条件下工作得最有效率。</p><p><strong>计算逻辑</strong></p><p>布隆过滤器的效率和误判率与使用的哈希函数数量有很大关系。太少的哈希函数会增加碰撞的概率，导致误判率升高；而太多的哈希函数又会导致位数组快速填满，同样增加误判率，同时还会增加计算的开销。</p><p>最优哈希函数数量的计算公式是：<br><img src="/b4fa673f/hashnums.png" class><br>这个公式基于以下原理：给定一个固定大小的位数组，存在一个最优的哈希函数数量，可以最小化给定元素数量条件下的误判率。这个最优数量直接关联于位数组的大小和要处理的元素数量。</p><p>其中：</p><ul><li>k 是最优的哈希函数数量，</li><li>m 是位数组的大小（<code>numBits</code>），</li><li>n 是预期插入的元素数量（<code>expectedInsertions</code>），</li><li>ln⁡(2) 是自然对数2的值，大约等于0.693。</li></ul><h3 id="316-new-bitarraynumbits"><a href="#3-1-6-new-BitArray-numBits" class="headerlink" title="3.1.6 new BitArray(numBits)"></a>3.1.6 new BitArray(numBits)</h3><p>Guava cache  bloom filter 在实现位数组是采用创建long[]  + 位移操作</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">BitArray</span> &#123;  </span><br><span class="line"><span class="keyword">final</span> <span class="type">long</span>[] data;  </span><br><span class="line">  </span><br><span class="line">BitArray(<span class="type">int</span> bits) &#123;  </span><br><span class="line">    <span class="built_in">this</span>(<span class="keyword">new</span> <span class="title class_">long</span>[IntMath.divide(bits, <span class="number">64</span>, RoundingMode.CEILING)]);  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"><span class="comment">// Used by serialization  </span></span><br><span class="line">BitArray(<span class="type">long</span>[] data) &#123;  </span><br><span class="line">    checkArgument(data.length &gt; <span class="number">0</span>, <span class="string">&quot;data length is zero!&quot;</span>);  </span><br><span class="line">    <span class="built_in">this</span>.data = data;  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">void</span> <span class="title function_">set</span><span class="params">(<span class="type">int</span> index)</span> &#123;  </span><br><span class="line">    data[index &gt;&gt; <span class="number">6</span>] |= (<span class="number">1L</span> &lt;&lt; index);  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"><span class="type">boolean</span> <span class="title function_">get</span><span class="params">(<span class="type">int</span> index)</span> &#123;  </span><br><span class="line">    <span class="keyword">return</span> (data[index &gt;&gt; <span class="number">6</span>] &amp; (<span class="number">1L</span> &lt;&lt; index)) != <span class="number">0</span>;  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"><span class="comment">/** Number of bits */</span>  </span><br><span class="line"><span class="type">int</span> <span class="title function_">size</span><span class="params">()</span> &#123;  </span><br><span class="line">    <span class="keyword">return</span> data.length * Long.SIZE;  </span><br><span class="line">&#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="32-put"><a href="#3-2-put" class="headerlink" title="3.2 put"></a>3.2 put</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">MURMUR128_MITZ_32() &#123;  </span><br><span class="line"><span class="meta">@Override</span> <span class="keyword">public</span> &lt;T&gt; <span class="keyword">void</span> <span class="title function_">put</span><span class="params">(T object, Funnel&lt;? <span class="built_in">super</span> T&gt; funnel,  </span></span><br><span class="line"><span class="params"><span class="type">int</span> numHashFunctions, BitArray bits)</span> &#123;  </span><br><span class="line"><span class="comment">// TODO(user): when the murmur&#x27;s shortcuts are implemented, update this code  </span></span><br><span class="line"><span class="type">long</span> <span class="variable">hash64</span> <span class="operator">=</span> Hashing.murmur3_128().newHasher().putObject(object, funnel).hash().asLong();  </span><br><span class="line"><span class="type">int</span> <span class="variable">hash1</span> <span class="operator">=</span> (<span class="type">int</span>) hash64;  </span><br><span class="line"><span class="type">int</span> <span class="variable">hash2</span> <span class="operator">=</span> (<span class="type">int</span>) (hash64 &gt;&gt;&gt; <span class="number">32</span>);  </span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">1</span>; i &lt;= numHashFunctions; i++) &#123;  </span><br><span class="line"><span class="type">int</span> <span class="variable">nextHash</span> <span class="operator">=</span> hash1 + i * hash2;  </span><br><span class="line"><span class="keyword">if</span> (nextHash &lt; <span class="number">0</span>) &#123;  </span><br><span class="line">nextHash = ~nextHash;  </span><br><span class="line">&#125;  </span><br><span class="line"><span class="comment">// up to here, the code is identical with the next method  </span></span><br><span class="line">bits.set(nextHash % bits.size());  </span><br><span class="line">&#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>首先，这行代码使用MurmurHash3算法生成一个128位的哈希值，然后将其转换成一个<code>long</code>类型的数值<code>hash64</code>。</p><ul><li><code>funnel</code>是一个函数式接口，用于将对象转换为字节流，以便哈希函数可以处理。</li><li><code>hash64</code>实际上包含两个32位的哈希值，它们可以从<code>hash64</code>的高32位和低32位分别提取。从<code>hash64</code>中提取两个32位的哈希值<code>hash1</code>和<code>hash2</code>。<code>hash1</code>是低32位，<code>hash2</code>是高32位。</li><li><ul><li>接下来，代码遍历从1到<code>numHashFunctions</code>（布隆过滤器要求的哈希函数数量），每次循环计算一个新的哈希值<code>nextHash</code>。这个新的哈希值是通过<code>hash1 + i * hash2</code>计算得到的，其中<code>i</code>是当前的迭代次数。</li></ul></li><li>如果<code>nextHash</code>为负数，通过位取反操作（<code>~nextHash</code>）将其转换为正数，以保证能够正确地映射到位数组的索引上。</li><li>最后，使用<code>nextHash % bits.size()</code>计算得到的索引值在位数组（<code>BitArray</code>）中对应的位置上设置位。<code>bits.size()</code>返回位数组的大小，这确保了计算得到的索引值不会超出位数组的范围。</li></ul><h3 id="321-哈希函数"><a href="#3-2-1-哈希函数" class="headerlink" title="3.2.1 哈希函数"></a>3.2.1 哈希函数</h3><p>Guava cache 采用了非加密的单向散列函数Murmur3.<br><strong>MurmurHash</strong> 由Austin Appleby设计，因其高性能和良好的分布特性而广泛应用。MurmurHash有多个版本，如MurmurHash2、MurmurHash3等。</p><p>根据最开始对bloom filter 的定义，它需要多个哈希函数对数据进行哈希映射， 但Guava Cache bloom filter 实现中其实没有使用多个不同的哈希函数，而是采用了一种叫做“双哈希技术”的方法。</p><p>“双哈希技术”的基本思想是利用两个哈希函数<code>h1(x)</code>和<code>h2(x)</code>生成任意数量的哈希值，对于第<code>i</code>个哈希位置，使用<code>h1(x) + i*h2(x)</code>的方式来生成。这种方法只需要两次哈希操作，就可以模拟出多个哈希函数的效果，且生成的哈希序列具有很好的均匀分布性，既保证了布隆过滤器的效率，又避免了寻找多个好的哈希函数的复杂性，是一种在实际应用中非常实用的解决方案。</p><p>效率和复杂性具体指</p><ol><li><strong>性能和效率</strong>：使用单个哈希函数后通过算法变换生成多个哈希值，可以大大减少计算的复杂度和时间。多个独立的哈希函数意味着每个元素都需要被多次独立哈希，这会增加计算成本和时间。通过使用单个哈希函数并通过数学方法派生出多个伪随机的哈希值，可以在保持布隆过滤器错误率不变的前提下，显著提高效率。</li><li><strong>简化实现</strong>：多个不同的哈希函数难以选取，而且还需保证它们相互之间的独立性和分布的均匀性，这在实践中是非常挑战性的。</li></ol><h2 id="33-mightcontains"><a href="#3-3-mightContains" class="headerlink" title="3.3 mightContains"></a>3.3 mightContains</h2><p>和put 处理过程保持一致</p><h2 id="34-guava-cache-误报率-位数组长度固定"><a href="#3-4-guava-cache-误报率-位数组长度固定" class="headerlink" title="3.4 guava cache 误报率-位数组长度固定"></a>3.4 guava cache 误报率-位数组长度固定</h2><p>在使用Guava的布隆过滤器时，预先估计将要插入的数据量非常重要。布隆过滤器在创建时会根据这个预估的数据量和指定的误判率来决定位数组的大小和使用的哈希函数数量。这些参数共同决定了布隆过滤器的性能和准确性。</p><p>如果实际插入的元素数量超过了最初的预估，过滤器的实际误判率会高于预期的误判率。这是因为当位数组变得过于饱和时，不同元素的哈希值更有可能映射到已经被设置为1的位上，从而增加了误判的几率。</p><p>Guava的文档明确指出了这一点，强调在创建布隆过滤器时应该准确预估元素数量，并考虑到这一点在其API设计中。<code>BloomFilter.create()</code>方法允许开发者在创建过滤器时指定预期插入的元素数量和可接受的误判率。</p><p><a href="https://guava.dev/releases/20.0/api/docs/com/google/common/hash/BloomFilter.html">https://guava.dev/releases/20.0/api/docs/com/google/common/hash/BloomFilter.html</a></p><img src="/b4fa673f/errorrate.png" class><p>为了保证布隆过滤器的效果，应该根据实际使用场景仔细估算元素数量。如果预计数据量存在不确定性，建议预估一个上限，或者在实际元素数量超过预估时重新创建一个新的布隆过滤器。这当然会带来额外的成本，因此在设计初期做出准确估计非常关键。</p><p>总结来说，正确估计将要处理的数据量对于使用Guava布隆过滤器来说是非常重要的。如果实际数据量超过了预估，将会导致高于预期的误判率，可能影响到应用的准确性和可靠性。</p><h1 id="4-分布式-redis-bloom-filter"><a href="#4-分布式-Redis-bloom-filter" class="headerlink" title="4. 分布式 Redis bloom filter"></a>4. 分布式 Redis bloom filter</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">BF.RESERVE &#123;key&#125; &#123;error_rate&#125; &#123;capacity&#125; [EXPANSION expansion] [NONSCALING]</span><br></pre></td></tr></table></figure><h2 id="41-参数解释"><a href="#4-1-参数解释" class="headerlink" title="4.1 参数解释"></a>4.1 参数解释</h2><h3 id="411-expansion-expansion"><a href="#4-1-1-EXPANSION-expansion" class="headerlink" title="4.1.1 EXPANSION expansion"></a>4.1.1 EXPANSION expansion</h3><p><code>BF.RESERVE</code>命令是RedisBloom模块中用来创建一个新的布隆过滤器的命令。这个命令允许用户预先为布隆过滤器指定参数，以便在插入元素之前就确定其大小和其他重要的行为特性。以下是<code>BF.RESERVE</code>命令各个参数的含义：</p><ul><li><p><strong><code>&#123;key&#125;</code></strong>：这是将要创建的布隆过滤器的名称或键值。在Redis中，每个数据结构都通过一个唯一的键来标识和访问。</p></li><li><p><strong><code>&#123;error_rate&#125;</code></strong>：预期的误报率。这是一个0到1之间的浮点数，表示允许的误判概率的上限。误报率越低，布隆过滤器所需的空间就越大。</p></li><li><p><strong><code>&#123;capacity&#125;</code></strong>：布隆过滤器预期要存储的元素的数量。这个数值用于在保持误报率不变的情况下，预先计算布隆过滤器所需的最小大小。</p></li><li><p><strong><code>[EXPANSION expansion]</code></strong>（可选）：这个可选参数用于指定当布隆过滤器的容量不足以容纳更多元素时，自动扩展的行为。<code>expansion</code>是一个大于1的整数，表示每次扩展增加的比例或容量。如果未指定，布隆过滤器可能会使用默认的扩展策略。</p></li><li><p><strong><code>[NONSCALING]</code></strong>（可选）：这个可选标志用来指示创建的布隆过滤器不应自动扩展。这意味着一旦达到其容量上限，就不会尝试扩大过滤器以容纳更多的元素。这通常用于那些对空间使用有严格限制的应用场景。</p></li><li><p><strong>作用</strong>：这个参数用于设置布隆过滤器在达到容量限制并需要扩展时，新创建的布隆过滤器层的大小。<code>expansion</code>的值决定了新层的容量是前一层容量的多少倍。这是一种自动扩展布隆过滤器容量的机制，以适应不断增长的元素数量，同时控制误报率。</p></li><li><strong>场景</strong>：适用于那些元素数量不确定或可能会超出初始设定容量的场合。通过适当设置<code>expansion</code>参数，可以在维持误报率的同时动态增加布隆过滤器的容量。</li><li><strong>举例</strong>：如果设置<code>[EXPANSION 2]</code>，那么每次扩展时，新的布隆过滤器层的容量将是前一层的两倍。</li></ul><h3 id="412-nonscaling"><a href="#4-1-2-NONSCALING" class="headerlink" title="4.1.2 NONSCALING"></a>4.1.2 NONSCALING</h3><ul><li><strong>作用</strong>：指定创建的布隆过滤器为非扩展型（Non-scaling）。即，一旦创建，布隆过滤器的容量固定，不会根据元素的增加而自动增加新的层。这意味着所有元素都将被添加到这个固定大小的布隆过滤器中，不管其容量是否已满。</li><li><strong>场景</strong>：适用于元素数量预先已知且不会超出初始设定容量的场合。这种方式可以避免因为扩展而可能带来的额外内存使用，但要求用户必须更准确地预估所需的容量和误报率。</li></ul><p><strong>注意</strong>：当使用<code>[NONSCALING]</code>参数时，<code>[EXPANSION expansion]</code>参数将无效，因为非扩展型的布隆过滤器不会进行任何扩展操作。</p><h2 id="42-可拓展特性是如何实现"><a href="#4-2-可拓展特性是如何实现" class="headerlink" title="4.2 可拓展特性是如何实现"></a>4.2 可拓展特性是如何实现</h2><p>redis 可扩展布隆过滤(Scalable Bloom Filters)可以理解成是由多个位数组/子过滤器（布隆过滤器实例）链接在一起形成的链表(SBChain)。</p><p>每个子过滤器都有其自己的容量和误报率设置。当前的子过滤器达到容量限制时，就会动态地添加一个新的子过滤器。</p><p>这种方法的关键优势是，它可以在不断增加元素的情况下，动态地扩展总容量，同时控制整体误报率。</p><p>然而，这种动态扩展的能力也意味着查询操作可能需要遍历链表中的多个布隆过滤器实例，这可能会对性能产生一定影响。</p><p>redis bloom filter 的整体上的基本逻辑，比如位数组的大小、选择的哈希函数、哈希函数的数量、多个哈希函数的模拟与Guava cache  bloom filter基本保持一致， 接下来只重点分析其可拓展性是如何实现的</p><h3 id="421-bloom-结构体"><a href="#4-2-1-bloom-结构体" class="headerlink" title="4.2.1 bloom 结构体"></a>4.2.1 bloom 结构体</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">bloom.h</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">bloom</span> &#123;</span>  </span><br><span class="line"><span class="type">uint32_t</span> hashes;  </span><br><span class="line"><span class="type">uint8_t</span> force64;  </span><br><span class="line"><span class="type">uint8_t</span> n2;  </span><br><span class="line"><span class="type">uint64_t</span> entries;  </span><br><span class="line">  </span><br><span class="line"><span class="type">double</span> error;  </span><br><span class="line"><span class="type">double</span> bpe;  </span><br><span class="line">  </span><br><span class="line"><span class="type">unsigned</span> <span class="type">char</span> *bf;  </span><br><span class="line"><span class="type">uint64_t</span> bytes;  </span><br><span class="line"><span class="type">uint64_t</span> bits;  </span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>这个<code>struct bloom</code>定义了一个布隆过滤器的基本数据结构，用于在RedisBloom模块中表示一个布隆过滤器实例。下面是对各个成员变量的详细解释：</p><ul><li><strong><code>uint32_t hashes;</code></strong>：这表示布隆过滤器使用的哈希函数的数量。在布隆过滤器中，元素的存在是通过多个哈希函数映射到位数组的不同位置来表示的。因此，哈希函数的数量直接影响布隆过滤器的误判率和效率。</li><li><strong><code>uint8_t force64;</code></strong>：这是一个标志位，用于指示是否强制使用64位哈希函数。在某些情况下，为了保证在不同平台上的一致性和性能，可能需要强制使用64位哈希函数。</li><li><strong><code>uint8_t n2;</code></strong>：这个成员可能用于表示与位数组大小相关的一个参数，具体含义取决于实现细节。在一些布隆过滤器的实现中，这个参数可能与位数组大小为2的幂次方有关。</li><li><strong><code>uint64_t entries;</code></strong>：这表示预期存储在布隆过滤器中的元素数量。这个数值对于计算位数组的大小和哈希函数数量非常重要。</li><li><strong><code>double error;</code></strong>：这是预期的误判率（false positive rate），是设计布隆过滤器时的一个关键参数。误判率越低，所需的位数组大小和哈希函数数量就越多。</li><li><strong><code>double bpe;</code></strong>：这表示每个元素平均占用的位数（Bits Per Entry）。这个值是根据预期的误判率和元素数量计算得出的，用于确定位数组的大小。</li><li><strong><code>unsigned char *bf;</code></strong>：这是一个指向位数组的指针。位数组是布隆过滤器的核心，用于存储元素的哈希值映射。<code>unsigned char</code>类型被用来表示位数组，每个字节包含8位。</li><li><strong><code>uint64_t bytes;</code></strong>：这表示位数组占用的字节数。由于位数组是以字节为单位进行分配的，因此这个数值表示整个位数组的大小。</li><li><strong><code>uint64_t bits;</code></strong>：这表示位数组中的位数。这个数值是根据<code>entries</code>、<code>error</code>和<code>bpe</code>计算得出的，决定了布隆过滤器可以有效存储的元素数量和误判率。</li></ul><h3 id="422-sblink结构体"><a href="#4-2-2-SBLink结构体" class="headerlink" title="4.2.2 SBLink结构体"></a>4.2.2 <code>SBLink</code>结构体</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sb.h</span><br><span class="line"><span class="comment">/** Single link inside a scalable bloom filter */</span>  </span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">SBLink</span> &#123;</span>  </span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">bloom</span> <span class="title">inner</span>;</span> <span class="comment">//&lt; Inner structure  </span></span><br><span class="line"><span class="type">size_t</span> size; <span class="comment">// &lt; Number of items in the link  </span></span><br><span class="line">&#125; SBLink;  </span><br></pre></td></tr></table></figure><p><code>SBLink</code>代表了可扩展布隆过滤器中的单个链接，即单个布隆过滤器实例。</p><ul><li><strong><code>struct bloom inner;</code></strong>：这是一个嵌套的结构体，表示单个布隆过滤器的内部结构。这个<code>inner</code>结构体可能包含了实现布隆过滤器所需的所有数据，如位数组、哈希函数数量等。</li><li><strong><code>size_t size;</code></strong>：表示当前链接（即单个布隆过滤器实例）中的元素数量。这是为了快速访问单个过滤器内元素的数量，而无需遍历整个位数组。</li></ul><h3 id="423-sbchain结构体"><a href="#4-2-3-SBChain结构体" class="headerlink" title="4.2.3 SBChain结构体"></a>4.2.3 <code>SBChain</code>结构体</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">sb.h  </span><br><span class="line"><span class="comment">/** A chain of one or more bloom filters */</span>  </span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">SBChain</span> &#123;</span>  </span><br><span class="line">SBLink *filters; <span class="comment">//&lt; Current filter  </span></span><br><span class="line"><span class="type">size_t</span> size; <span class="comment">//&lt; Total number of items in all filters  </span></span><br><span class="line"><span class="type">size_t</span> nfilters; <span class="comment">//&lt; Number of links in chain  </span></span><br><span class="line"><span class="type">unsigned</span> options; <span class="comment">//&lt; Options passed directly to bloom_init  </span></span><br><span class="line"><span class="type">unsigned</span> growth;  </span><br><span class="line">&#125; SBChain;</span><br></pre></td></tr></table></figure><p><code>SBChain</code>代表了一系列（一个或多个）<code>SBLink</code>结构体的链表，构成了一个可扩展的布隆过滤器。</p><ul><li><strong><code>SBLink *filters;</code></strong>：这是一个指向<code>SBLink</code>数组的指针，表示当前所有的过滤器链。每个<code>SBLink</code>代表链中的一个布隆过滤器实例。</li><li><strong><code>size_t size;</code></strong>：表示所有过滤器中元素的总数量。这个数字是所有单个<code>SBLink</code>中<code>size</code>成员的总和。</li><li><strong><code>size_t nfilters;</code></strong>：表示链中<code>SBLink</code>实例的数量，即当前有多少个布隆过滤器被链接在一起。</li><li><strong><code>unsigned options;</code></strong>：这是传递给每个布隆过滤器初始化函数<code>bloom_init</code>的选项。这些选项可能控制如布隆过滤器的误报率、是否自动扩展等行为。</li><li><strong><code>unsigned growth;</code></strong>：这个成员变量控制链的增长行为。它可能指定当当前的布隆过滤器填满时，如何增加新的<code>SBLink</code>实例，例如，增加的大小或比例等。</li></ul><h2 id="43-可扩展bloom-filter的工作流程"><a href="#4-3-可扩展bloom-filter的工作流程" class="headerlink" title="4.3 可扩展bloom filter的工作流程"></a>4.3 可扩展bloom filter的工作流程</h2><ol><li><strong>初始化</strong>：当创建一个新的可扩展布隆过滤器时，会指定初始容量、误报率等参数。基于这些参数，创建第一个子过滤器。</li><li><strong>添加元素</strong>：向布隆过滤器添加元素时，会从当前子过滤器开始尝试添加。如果当前子过滤器已满（即达到了其容量限制），则创建一个新的子过滤器，并在新的子过滤器中添加元素。每个新添加的子过滤器都可以根据配置的规则调整大小和误报率，以适应不断增加的元素。</li><li><strong>检查元素</strong>：检查一个元素是否存在时，需要查询所有的子过滤器。如果任何子过滤器表示元素可能存在（即对应的位都为1），则认为元素可能存在于布隆过滤器中。虽然可扩展布隆过滤器可以动态增加容量，但查询操作的成本随之增加，因为可能需要检查多个布隆过滤器实例。</li><li><strong>参数调整</strong>：随着子过滤器的增加，每个新的子过滤器通常会有更大的容量。这是通过调整如比特数、哈希函数数量等参数来实现的。这种方法旨在平衡误报率和内存使用，即使在不断添加元素的情况下也能维持相对稳定的误报率。</li></ol><h1 id="5-可删除-bloom-filter"><a href="#5-可删除-bloom-filter" class="headerlink" title="5. 可删除 bloom filter"></a>5. 可删除 bloom filter</h1><h2 id="51-哪些场景使用布隆过滤器时需要删除"><a href="#5-1-哪些场景使用布隆过滤器时需要删除" class="headerlink" title="5.1 哪些场景使用布隆过滤器时需要删除"></a>5.1 哪些场景使用布隆过滤器时需要删除</h2><p>以上， guava 和 redis 的bloom filter 都没有实现删除功能，不能删除的原因已经解释过，元素的存在是通过多个位的“1”状态来表示的，而将这些位重置为“0”可能会错误地影响其他元素的存在检测。</p><p>但是在某些场景还是需要删除，比如，<br>查看一张优惠券是否已被使用？创建一个包含所有存在但还未被使用优惠券的filter。每次校验时</p><ul><li>如果否，则优惠券不存在。</li><li>如果是，则优惠券有效。检查主数据库。如果有效，则使用后从 Cuckoo 过滤器中删除。</li></ul><h2 id="52-实现删除布隆过滤器的思路"><a href="#5-2-实现删除布隆过滤器的思路" class="headerlink" title="5.2 实现删除布隆过滤器的思路"></a>5.2 实现删除布隆过滤器的思路</h2><h3 id="521-计数型布隆过滤器counting-bloom-filter"><a href="#5-2-1-计数型布隆过滤器（Counting-Bloom-Filter）" class="headerlink" title="5.2.1 计数型布隆过滤器（Counting Bloom Filter）"></a>5.2.1 计数型布隆过滤器（Counting Bloom Filter）</h3><p>这是最直接的方法之一，它通过为每个位使用一个计数器而不是简单的布尔标记来实现。当插入一个元素时，它经过多个哈希函数映射到多个计数器上，并将这些计数器的值增加。相应地，删除一个元素时，这些计数器的值会被减少。如果任何计数器的值达到零，则表示没有任何元素映射到这个位上。这种方法的缺点是需要更多的空间来存储计数器。</p><h3 id="522-双布隆过滤器"><a href="#5-2-2-双布隆过滤器" class="headerlink" title="5.2.2 双布隆过滤器"></a>5.2.2 双布隆过滤器</h3><p>这种方法涉及到使用两个独立的布隆过滤器：一个用于添加操作，另一个用于删除操作。当添加一个元素时，它被添加到第一个布隆过滤器中；当删除一个元素时，该元素被添加到第二个布隆过滤器中。检查元素是否存在时，如果它在第一个布隆过滤器中并且不在第二个布隆过滤器中，则认为该元素存在。这种方法的问题是误报率会增加，因为删除过滤器中的元素也可能错误地阻止对实际存在于集合中的元素的正确判断。</p><h3 id="523-d-left-计数哈希"><a href="#5-2-3-d-left-计数哈希" class="headerlink" title="5.2.3 d-left 计数哈希"></a>5.2.3 d-left 计数哈希</h3><p>d-left计数哈希是一种高效的数据结构，它将元素映射到固定数量的桶中，并在每个桶内维护一个计数器。这种方法可以实现快速的插入、查询和删除操作，并且相比于计数型布隆过滤器，它可以更有效地利用空间。不过，实现起来比较复杂，且当桶填满时性能会下降。</p><h3 id="524-布谷鸟过滤器cuckoo-filter"><a href="#5-2-4-布谷鸟过滤器（Cuckoo-Filter）" class="headerlink" title="5.2.4 布谷鸟过滤器（Cuckoo Filter）"></a>5.2.4 布谷鸟过滤器（Cuckoo Filter）</h3><p>布谷鸟过滤器是另一种支持删除操作的布隆过滤器变种，它基于布谷鸟哈希和部分键存储。每个元素通过哈希函数映射到一个或多个位置，并存储其“指纹”。插入、查询和删除操作都基于这些指纹。布谷鸟过滤器相比计数型布隆过滤器在空间效率上有所提高，且支持删除操作，但在极端情况下可能需要重建过滤器。</p><p>总体上可以看出可删除bloom filter 的实现都是需要额外的空间去存储额外的信息， 那么其实现方式的好坏的评判标准就是 额外空间的大小、性能 以及对误报率的影响。</p><p>以下是一张表格，总结了几种支持删除操作的布隆过滤器变体的优点和缺点：</p><div class="table-container"><table><thead><tr><th>过滤器类型</th><th>优点</th><th>缺点</th></tr></thead><tbody><tr><td>计数型布隆过滤器</td><td>- 直接支持删除操作<br>- 实现相对简单</td><td>- 更多空间需求<br>- 计数器溢出问题</td></tr><tr><td>双布隆过滤器</td><td>- 实现简单<br>- 通过额外布隆过滤器跟踪删除操作</td><td>- 增加空间需求<br>- 误判率增加</td></tr><tr><td>布谷鸟过滤器</td><td>- 高效的插入、删除和查询<br>- 空间效率高</td><td>- 实现复杂<br>- 负载因子高时性能可能下降</td></tr><tr><td>d-left计数哈希过滤器</td><td>- 高空间效率<br>- 性能优于传统计数型布隆过滤器</td><td>- 实现复杂，需要精心设计</td></tr></tbody></table></div><ul><li>如果应用对空间效率要求不是特别高，且需要频繁进行删除操作，计数型布隆过滤器是一个简单有效的选择。</li><li>对于需要最小化误报率而且对空间有一定要求的应用，布谷鸟过滤器提供了一个较好的平衡点。</li><li>在需要极致空间效率且能够接受实现复杂度的高性能应用场景中，d-left 计数哈希过滤器可能是最佳选择。</li></ul><p>关于空间的使用，有如下比对效果<br><img src="/b4fa673f/spacecompare.png" class></p><p>redis 实现了cuckoo 变体bloom filter， 下面来讲一下 cuckoo  filter 的原理</p><h1 id="6-cuckoo-filter"><a href="#6-Cuckoo-Filter" class="headerlink" title="6. Cuckoo Filter"></a>6. Cuckoo Filter</h1><p><a href="https://www.cs.cmu.edu/~binfan/papers/conext14_cuckoofilter.pdf">Cuckoo Filter 论文</a></p><p>the basic unit of the cuckoo hash tables used for our cuckoo filters is called an entry. Each entry stores one fingerprint. The hash table consists of an array of buckets, where a bucket can have multiple entries.</p><p>在Cuckoo Filter 中，最基本的存储单元是entry,  每个单元存储一个 fingerprint。cuckoo hashing 哈希表由一组桶（buckets）组成，每个桶可以包含多个条目（entries）。</p><p>Cuckoo Filter的关键特性</p><ol><li><strong>双哈希函数</strong>：对于任何一个给定的键，Cuckoo Hashing使用两个独立的哈希函数 <code>h1</code> 和 <code>h2</code> 来计算两个候选桶的位置。这两个位置是键可能被存储的地方。</li><li><strong>指纹存储</strong>：与传统的哈希表不同，Cuckoo Hashing不存储完整的键，而是存储键的指纹。这允许在不牺牲太多空间效率的情况下进行高效的查找和删除操作。</li><li><strong>动态插入</strong>：当插入一个新键时，如果候选桶中没有足够的空间，Cuckoo Hashing会通过一系列置换操作来为新键腾出空间。这涉及到将现有的指纹移动到它们的替代位置，从而为新键腾出空间。已占用位置的元素会被移动到其它哈希函数确定的位置，可能导致一个连锁的重新放置过程。这个算法的名字来源于布谷鸟，因为布谷鸟会将自己的蛋放入其他鸟类的巢中，迫使其他鸟类的蛋被移位或丢弃。</li><li><strong>查找操作</strong>：给定一个键，Cuckoo Hashing通过检查两个候选桶中的指纹来确定键是否存在于表中。如果任一桶中存在匹配的指纹，则认为键存在于表中。</li><li><strong>删除操作</strong>：删除操作相对简单，Cuckoo Hashing检查两个候选桶，如果找到匹配的指纹，则移除该指纹。这种删除方法不会影响其他键的存储位置。</li><li><strong>高空间效率</strong>：Cuckoo Hashing通过存储指纹而不是完整的键来优化空间使用，同时保持较高的表占用率，从而实现高空间效率。</li></ol><h2 id="61-fingerprint"><a href="#6-1-fingerprint" class="headerlink" title="6.1 fingerprint"></a>6.1 fingerprint</h2><p>“fingerprint”是存储在Cuckoo Filter中的数据。，一个哈希函数处理数据后得到的哈希串。</p><h2 id="62-partial-key-cuckoo-hashing"><a href="#6-2-Partial-Key-Cuckoo-Hashing" class="headerlink" title="6.2 Partial-Key Cuckoo Hashing"></a>6.2 Partial-Key Cuckoo Hashing</h2><p>A cuckoo filter is a compact variant of a cuckoo hash table that stores only fingerprints-a bit string derived from the item using a hash function。</p><p>在cuckoo filter 中，哈希表中存储的数据发生了变化，由原始数据变成了fingerprint。<br><code>注意这里的（basic/standard） cuckoo hash tables指的是存储原始数据的做法。</code></p><p>partial-key cuckoo hashing 是一种仅使用指纹，而不需要原始数据就可以在置换操作中来确定元素存储位置的哈希技术。由于仅存储fingerprint 比存储原始数据所占内存空间小，所以 Partial-Key Cuckoo Hashing为优化Cuckoo Filter的空间效率和操作性能而设计。</p><img src="/b4fa673f/PartialKey.png" class><p>如上代码所示，Partial-Key Cuckoo Hashing使用两个哈希函数 <code>h1</code> 和 <code>h2</code> 来计算两个候选桶位置。第一个哈希函数 <code>h1</code> 直接作用于元素的指纹，而第二个哈希函数 <code>h2</code> 则是 <code>h1</code> 与指纹的哈希值的异或（XOR）结果。<br>这种XOR的处理结果 可以根据其中任意一个已知的候选桶位置（i）计算出另外一个候选桶位置（j）<br>j = i XOR fingerprint</p><h2 id="63-insert"><a href="#6-3-Insert" class="headerlink" title="6.3 Insert"></a>6.3 Insert</h2><img src="/b4fa673f/cuckooinsert.png" class><p><strong>cuckoo 哈希的插入过程：</strong></p><ol><li>计算元素的指纹。</li><li>确定两个候选桶位置 <code>i1</code> 和 <code>i2</code>。</li><li>如果 <code>i1</code> 或 <code>i2</code> 有空闲条目，则将指纹插入到该条目中。</li><li>如果两个桶都满，则选择一个桶，随机选择一个条目并将其指纹与新元素的指纹交换。</li><li>Partial-Key Cuckoo Hashing ，更新候选桶位置 <code>i</code> 为 <code>i</code> XOR 指纹的哈希值。</li><li>如果找到空闲条目，则插入指纹；否则继续置换过程，直到找到空闲条目或达到最大置换次数。</li></ol><h2 id="64-lookup"><a href="#6-4-Lookup" class="headerlink" title="6.4 Lookup"></a>6.4 Lookup</h2><img src="/b4fa673f/Lookup.png" class><h2 id="65-delete"><a href="#6-5-Delete" class="headerlink" title="6.5 Delete"></a>6.5 Delete</h2><img src="/b4fa673f/delete.png" class><p>为什么cuckoo filter 可以删除元素但是又不影响其他元素的判断， 其实就是每个entry 只存储了一个元素的信息，删除后自然不会影响其他元素的判断。</p><h2 id="66-空间优化"><a href="#6-6-空间优化" class="headerlink" title="6.6 空间优化"></a>6.6 空间优化</h2><p>Cuckoo Filter进行空间优化（SPACE OPTIMIZATIONS）的方法主要涉及对哈希表参数的选择和配置，以及对桶（buckets）的编码策略。以下是论文中提到的一些关键的空间优化策略：</p><ol><li><strong>选择合适的桶大小（Bucket Size）</strong>：<ul><li>桶大小（b）对Cuckoo Filter的空间效率有显著影响。较大的桶可以提高哈希表的占用率（α），但同时也需要更长的指纹来维持相同的误报率。</li><li>论文中通过实验确定了对于不同的目标误报率（ϵ），最优的桶大小是不同的。例如，当误报率大于0.002时，每个桶有2个条目可能比4个条目更有效；而当误报率降低到0.00001至0.002时，每个桶有4个条目可以最小化空间使用。</li></ul></li><li><strong>半排序桶编码（Semi-Sorting Buckets）</strong>：<ul><li>对于每个桶中的指纹进行排序，然后使用一个预先计算好的表来编码这些排序后的指纹序列。由于桶内指纹的顺序不影响查询结果，这种方法可以通过索引来节省空间。</li><li>例如，如果每个桶有4个指纹，每个指纹是4比特长，那么未压缩的桶将占用16比特。通过排序和编码，可以使用一个12比特的索引来代替原来的16比特桶，因为可以预先计算出所有可能的桶值（例如，3876种），并将每个桶表示为一个索引，从而节省1比特每个指纹。</li></ul></li><li><strong>平衡桶的负载（Balancing Bucket Loads）</strong>：<ul><li>通过合理配置哈希函数和桶大小，可以减少哈希表中的冲突和空桶，从而提高空间利用率。</li><li>论文中提到，通过适当的配置，Cuckoo Filter可以以高概率达到95%的表空间占用率。</li></ul></li><li><strong>指纹长度的优化</strong>：<ul><li>指纹长度（f）与桶大小和目标误报率有关。通过调整指纹长度，可以在保持目标误报率的同时，优化空间使用。</li><li>论文中的分析表明，对于实际应用中的集合大小，较短的指纹（例如6比特或更长）通常足以确保哈希表的高利用率。</li></ul></li></ol><p>通过这些空间优化策略，Cuckoo Filter能够在保持高效动态操作的同时，实现紧凑的数据存储。这些优化使得Cuckoo Filter在很多实际应用中比传统的Bloom Filter和Counting Bloom Filter更加空间高效</p><h2 id="67-redis-cuckoo-filter"><a href="#6-7-Redis-cuckoo-filter" class="headerlink" title="6.7 Redis cuckoo filter"></a>6.7 Redis cuckoo filter</h2><p>redis 实现了 cuckoo filter ，基本实现逻辑和论文中表现一致，有兴趣大家可以自己去看一下</p><h1 id="7-自己如何实现一个布隆过滤器"><a href="#7-自己如何实现一个布隆过滤器" class="headerlink" title="7. 自己如何实现一个布隆过滤器"></a>7. 自己如何实现一个布隆过滤器</h1><p>基于以上对3种bloom filter的分析，可以总结出自己实现bloom filter时需要考虑的因素<br><a href="https://github.com/sysunyan1699/BloomFilterPractice">本项目代码可点击这里查看</a></p><h2 id="71-位数组的实现"><a href="#7-1-位数组的实现" class="headerlink" title="7.1 位数组的实现"></a>7.1 位数组的实现</h2><p>首先参考BitSet。<br>BitSet类是Java标准库中提供的一个用于处理位数组的类，基本可以认为是bitmap 在Java 的中的实现，其原理是是long[] 数组+ 位操作。<br>在自己实现布隆过滤器时，可以直接使用BitSet ， 也可以像guava cache 一样，自己用long[] + 位操作自己封装一个bitarray，而不是直接使用的BitSet。</p><h2 id="72-位数组的大小"><a href="#7-2-位数组的大小" class="headerlink" title="7.2 位数组的大小"></a>7.2 位数组的大小</h2><p>在简单的自己实现的版本中，可以直接指定bitarray 大小。</p><p>但在实际线上生产环境中可用的bloom filter 实现，一般都是根据预期插入的数据量 和 可接受的误报率两个数字通过 一个数学公式算出的，而不是直接用预期插入的数据量。</p><p>同时我们在线上生产环境使用布隆过滤器时，根据业务特性和流量去估算预期插入的数据量 和衡量  可接受的误报率 也是非常重要的步骤。 如果估算数据量比实际值大很多，就会浪费内存空间。如果估算数据量比实际值小很多，那么误报率很可能就无法控制在可接受的范围内。</p><ol><li>guava cache bloom filter 位数组大小一旦确定时无法修改的，所以实际数据量如果过大，那么误报率肯定会上升</li><li>redis bloom filter 位数组大小可以scale, 但是判断元素是否存在的这个步骤性能会受到影响</li></ol><h2 id="73-用哪个哈希函数"><a href="#7-3-用哪个哈希函数" class="headerlink" title="7.3 用哪个哈希函数"></a>7.3 用哪个哈希函数</h2><p>这个哈希函数在密码学中叫做单向散列函数。单向散列函数有两类<br>加密与非加密散列，其主要区别如下。</p><h3 id="731-加密散列函数"><a href="#7-3-1-加密散列函数" class="headerlink" title="7.3.1 加密散列函数"></a>7.3.1 加密散列函数</h3><p>设计用于加密应用，强调安全性。它们需要具备一定的性质，如抗碰撞（两个不同的输入不应该产生同一个输出）、隐藏性（无法从输出推断任何信息关于输入）和抗篡改（对输入的微小变化会在输出中产生不<br>可预测的、大的变化）。</p><ul><li><strong>SHA家族（SHA-1、SHA-256、SHA-512等）</strong>：安全哈希算法（Secure Hash Algorithm）家族，广泛用于加密、数据完整性校验和数字签名等安全相关的应用。</li><li><strong>MD5</strong>：消息摘要算法5（Message Digest Algorithm 5），尽管因为安全性问题不再推荐用于加密安全领域，但在一些非安全性要求的场合仍然可以见到其身影。</li><li><strong>RIPEMD</strong>：一系列的加密哈希函数，包括RIPEMD-160、RIPEMD-256和RIPEMD-320，其中RIPEMD-160设计用于替代MD5和SHA-1。<br>-<h3 id="732-非加密散列函数"><a href="#7-3-2-非加密散列函数" class="headerlink" title="7.3.2 非加密散列函数"></a>7.3.2 非加密散列函数</h3>设计重点是高效率和均匀分布的输出，以支持快速数据检索、数据分布平衡等，而不是安全性。在某些情况下，允许存在碰撞，但碰撞的概率要尽可能低。</li></ul><p>常见的非加密单向散列函数：</p><ol><li><strong>MurmurHash</strong>：由Austin Appleby设计，因其高性能和良好的分布特性而广泛应用。MurmurHash有多个版本，如MurmurHash2、MurmurHash3等。</li><li><strong>CityHash</strong>：由Google开发，专为哈希字符串数据设计，适用于构建哈希表等数据结构。后续Google又推出了FarmHash，作为CityHash的改进版，提供更好的性能和更广的适用范围。</li><li><strong>xxHash</strong>：是一种非常快的哈希算法，提供了极高的数据处理速度，同时保持了良好的散列分布特性，适用于需要快速散列大量数据的场景。</li><li><strong>Jenkins哈希函数（如一致性哈希）</strong>：Bob Jenkins所设计的一系列哈希函数，包括lookup3、SpookyHash等，它们广泛用于软件开发中，特别是在需要快速且分布均匀的哈希算法的场合。</li><li><strong>FNV（Fowler-Noll-Vo）</strong>：是一系列设计简单、性能良好的哈希函数，特别适合于散列单个文本字符串。FNV-1和FNV-1a是两个最著名的变种。</li></ol><p>在以上分析原理分析中，guava cache和Redis  都是用了MurmurHash<br>如果我们自己也想要用MurmurHash，目前Java并没有提供实现，可以引入guava cache 包使用MurmurHash。</p><h2 id="74-用多少个哈希函数"><a href="#7-4-用多少个哈希函数" class="headerlink" title="7.4 用多少个哈希函数"></a>7.4 用多少个哈希函数</h2><p>前面说过用多个哈希函数可以减少误报率，那么到底要用多少个哈希函数呢， 这也是可以通过 预期插入的数据量+ 可接受的误报率 通过固定的数学公式计算得出。<br>同时多个哈希结果可以通过像Guava cache一样，用双哈希技术模拟得到。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;0什么场景下会用到bloom-filter&quot;&gt;&lt;a href=&quot;#0-什么场景下会用到bloom-filter&quot; class=&quot;headerlink&quot; title=&quot;0.什么场景下会用到bloom filter&quot;&gt;&lt;/a&gt;0.什么场景下会用到bloom filte</summary>
      
    
    
    
    
    <category term="Programming" scheme="http://example.com/tags/Programming/"/>
    
  </entry>
  
</feed>
